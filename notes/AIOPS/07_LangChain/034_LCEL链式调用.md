# LCELé“¾å¼è°ƒç”¨
# é“¾å¼è°ƒç”¨
## ä»€ä¹ˆæ˜¯é“¾å¼è°ƒç”¨
é¡¾åæ€ä¹‰ï¼Œ`LangChain`å…¶æ ¸å¿ƒæ¦‚å¿µå°±æ˜¯`Chain`ã€‚ `Chain`ç¿»è¯‘æˆä¸­æ–‡å°±æ˜¯â€œé“¾â€ã€‚ç”¨äºå°†å¤šä¸ªç»„ä»¶ï¼ˆæç¤ºæ¨¡æ¿ã€modelæ¨¡å‹ã€è®°å¿†ã€å·¥å…·ç­‰ï¼‰è¿æ¥èµ·æ¥ï¼Œå½¢æˆå¯å¤ç”¨çš„å·¥ä½œæµï¼Œå®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚æ¯”å¦‚æˆ‘ä»¬åˆšåˆšå®ç°çš„é—®ç­”æµç¨‹ï¼š ç”¨æˆ·è¾“å…¥ä¸€ä¸ªé—®é¢˜ --> å‘é€ç»™å¤§æ¨¡å‹ --> å¤§æ¨¡å‹è¿›è¡Œæ¨ç† --> å°†æ¨ç†ç»“æœè¿”å›ç»™ç”¨æˆ·ã€‚è¿™ä¸ªæµç¨‹å°±æ˜¯ä¸€ä¸ªé“¾ã€‚

Chain çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç»„åˆä¸åŒçš„æ¨¡å—åŒ–å•å…ƒï¼Œå®ç°æ¯”å•ä¸€ç»„ä»¶æ›´å¼ºå¤§çš„åŠŸèƒ½ã€‚æ¯”å¦‚ï¼š

+ å°†model ä¸Prompt Template ï¼ˆæç¤ºæ¨¡æ¿ï¼‰ç»“åˆ
+ å°†model ä¸è¾“å‡ºè§£æå™¨ç»“åˆ
+ å°†model ä¸å¤–éƒ¨æ•°æ®ç»“åˆï¼Œä¾‹å¦‚ç”¨äºé—®ç­”
+ å°†model ä¸é•¿æœŸè®°å¿†ç»“åˆï¼Œä¾‹å¦‚ç”¨äºèŠå¤©å†å²è®°å½•
+ é€šè¿‡å°†ç¬¬ä¸€ä¸ªmodel çš„è¾“å‡ºä½œä¸ºç¬¬äºŒä¸ªmodel çš„è¾“å…¥ï¼Œ...ï¼Œå°†å¤šä¸ªmodelæŒ‰é¡ºåºç»“åˆåœ¨ä¸€èµ·

LangChain é“¾å¼è°ƒç”¨å¯å‚è€ƒæ–‡æ¡£ï¼š[https://reference.langchain.com/python/langchain_core/runnables/](https://reference.langchain.com/python/langchain_core/runnables/)

## åŸºæœ¬ç»“æ„
åœ¨LangChainä¸­ï¼Œä¸€ä¸ªåŸºæœ¬çš„`Chain`ç»“æ„ä¸»è¦ç”±ä¸‰éƒ¨åˆ†æ„æˆï¼Œåˆ†åˆ«æ˜¯æç¤ºè¯æ¨¡æ¿ã€å¤§æ¨¡å‹å’Œç»“æœè§£æå™¨ï¼ˆç»“æ„åŒ–è§£æå™¨ï¼‰ï¼Œå…¶æ•°æ®æµå‘æ­£å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://via.placeholder.com/800x600?text=Image+7896b806b0bcfd27)

+ Promptï¼šPrompt æ˜¯ä¸€ä¸ª BasePromptTemplateï¼Œè¿™æ„å‘³ç€å®ƒæ¥å—ä¸€ä¸ªæ¨¡æ¿å˜é‡çš„å­—å…¸å¹¶ç”Ÿæˆä¸€ä¸ªPromptValue ã€‚PromptValue å¯ä»¥ä¼ é€’ç»™ modelï¼ˆå®ƒä»¥å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼‰æˆ– ChatModelï¼ˆå®ƒä»¥æ¶ˆæ¯åºåˆ—ä½œä¸ºè¾“å…¥ï¼‰ã€‚
+ Modelï¼šå°† PromptValue ä¼ é€’ç»™ modelã€‚å¦‚æœæˆ‘ä»¬çš„ model æ˜¯ä¸€ä¸ª ChatModelï¼Œè¿™æ„å‘³ç€å®ƒå°†è¾“å‡ºä¸€ä¸ª BaseMessage ã€‚
+ OutputParserï¼šå°† model çš„è¾“å‡ºä¼ é€’ç»™ output_parserï¼Œå®ƒæ˜¯ä¸€ä¸ª BaseOutputParserï¼Œæ„å‘³ç€å®ƒå¯ä»¥æ¥å—å­—ç¬¦ä¸²æˆ– BaseMessage ä½œä¸ºè¾“å…¥ã€‚
+ chainï¼šæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ | è¿ç®—ç¬¦è½»æ¾åˆ›å»ºè¿™ä¸ªChainã€‚ | è¿ç®—ç¬¦åœ¨ LangChain ä¸­ç”¨äºå°†ä¸¤ä¸ªå…ƒç´ ç»„åˆåœ¨ä¸€èµ·ã€‚

# LCELä»‹ç»
## ä»€ä¹ˆæ˜¯ LCEL
åœ¨ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆmodelï¼‰åº”ç”¨çš„æ„å»ºä¸­ï¼ŒLangChain æä¾›äº†ä¸€ç§å…¨æ–°çš„è¡¨è¾¾èŒƒå¼ï¼Œè¢«ç§°ä¸ºLCELï¼ˆLangChain Expression Languageï¼‰ã€‚å®ƒä¸ä»…ç®€åŒ–äº†æ¨¡å‹äº¤äº’çš„ç¼–æ’è¿‡ç¨‹ï¼Œè¿˜å¢å¼ºäº†ç»„åˆçš„çµæ´»æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

LCELï¼Œå…¨ç§°ä¸º LangChain Expression Languageï¼Œæ˜¯ä¸€ç§ä¸“ä¸º LangChain æ¡†æ¶è®¾è®¡çš„è¡¨è¾¾è¯­è¨€ã€‚å®ƒé€šè¿‡ä¸€ç§é“¾å¼ç»„åˆçš„æ–¹å¼ï¼Œå…è®¸å¼€å‘è€…ä½¿ç”¨æ¸…æ™°ã€å£°æ˜å¼çš„è¯­æ³•æ¥æ„å»ºè¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨æµç¨‹ã€‚

ç®€å•æ¥è¯´ï¼ŒLCEL æ˜¯ä¸€ç§â€œå‡½æ•°å¼ç®¡é“é£æ ¼â€çš„ç»„ä»¶ç»„åˆæœºåˆ¶ï¼Œç”¨äºè¿æ¥å„ç§å¯æ‰§è¡Œå•å…ƒï¼ˆRunnableï¼‰ã€‚è¿™äº›å•å…ƒåŒ…æ‹¬æç¤ºæ¨¡æ¿ã€è¯­è¨€æ¨¡å‹ã€è¾“å‡ºè§£æå™¨ã€å·¥å…·å‡½æ•°ç­‰ã€‚

## è®¾è®¡ç›®çš„
LCEL çš„è®¾è®¡åˆè¡·åœ¨äºï¼š

1. **æ¨¡å—åŒ–æ„å»º**ï¼šå°†æ¨¡å‹è°ƒç”¨æµç¨‹æ‹†è§£ä¸ºç‹¬ç«‹ã€å¯é‡ç”¨çš„ç»„ä»¶ã€‚
2. **é€»è¾‘å¯è§†åŒ–**ï¼šé€šè¿‡è¯­æ³•ç¬¦å·ï¼ˆå¦‚ç®¡é“ç¬¦ `|`ï¼‰å‘ˆç°å‡ºæ˜ç¡®çš„æ•°æ®æµè·¯å¾„ã€‚
3. **ç»Ÿä¸€è¿è¡Œæ¥å£**ï¼šæ‰€æœ‰ LCEL ç»„ä»¶éƒ½å®ç°äº† `.invoke()`ã€`.stream()`ã€`.batch()` ç­‰æ ‡å‡†æ–¹æ³•ï¼Œä¾¿äºåœ¨åŒæ­¥ã€å¼‚æ­¥æˆ–æ‰¹å¤„ç†ç¯å¢ƒä¸‹è°ƒç”¨ã€‚
4. **è„±ç¦»æ¡†æ¶é™åˆ¶**ï¼šç›¸æ¯”ä¼ ç»Ÿçš„ `Chain` ç±»å’Œ `Agent` æ¶æ„ï¼ŒLCEL æ›´è½»é‡ã€æ›´å…·è¡¨è¾¾åŠ›ï¼Œå‡å°‘ä¾èµ–çš„â€œé»‘ç›’â€é€»è¾‘ã€‚

## å…¸å‹ä¼˜åŠ¿
| ç‰¹æ€§ | æè¿° |
| --- | --- |
| ç®€æ´è¯­æ³• | ä½¿ç”¨ | è¿ç®—ç¬¦æå‡å¯è¯»æ€§ |
| çµæ´»ç»„åˆ | å¯ä»»æ„ç»„åˆ Promptã€æ¨¡å‹ã€å·¥å…·ã€å‡½æ•°ç­‰ç»„ä»¶ |
| æ˜ç¡®è¾¹ç•Œ | æ¯ä¸ªæ­¥éª¤èŒè´£åˆ†æ˜ï¼Œæ–¹ä¾¿è°ƒè¯•ä¸é‡ç”¨ |
| å¯åµŒå¥—æ‰©å±• | æ”¯æŒå‡½æ•°åŒ…è£…ã€è‡ªå®šä¹‰ä¸­é—´ç»„ä»¶å’Œæµå¼æ‹“å±• |
| ä¸ Gradio/FastAPI é›†æˆè‰¯å¥½ | å¯ç”¨äºæ„å»º APIã€UI èŠå¤©ç­‰å¤šç§åœºæ™¯ |


# LCELæ ¸å¿ƒåˆ†æ
## Runnable æ¥å£
`Runnable` æ˜¯ LangChain ä¸­æ‰€æœ‰é“¾çš„é€šç”¨æ¥å£ï¼Œç”¨äºæè¿°â€œå¯ä»¥æ‰§è¡Œçš„æ•°æ®æµèŠ‚ç‚¹â€ã€‚ç”¨äºæ„å»ºæ‰€æœ‰é“¾ï¼ˆChainï¼‰ç»„ä»¶ã€‚å®ƒä»£è¡¨â€œä¸€ä¸ªå¯ä»¥è°ƒç”¨ï¼ˆè¿è¡Œï¼‰çš„æµç¨‹å•å…ƒâ€ï¼Œæ— è®ºæ˜¯ï¼š

+ å•ä¸ªç»„ä»¶ï¼ˆå¦‚ promptã€modelï¼‰
+ ä¸€ä¸ªåºåˆ—æµç¨‹ï¼ˆå¦‚ prompt â†’ model â†’ parserï¼‰
+ å¹¶è¡Œã€å¤šè·¯ã€å¤šè¾“å…¥å¤šè¾“å‡ºçš„å¤åˆç»“æ„

åªè¦å®ç°äº† `Runnable` æ¥å£ï¼Œå®ƒå°±å¯ä»¥åƒå‡½æ•°ä¸€æ · `.invoke()`ï¼Œæˆ–ç”¨ç®¡é“ç¬¦ `|` ç»„åˆã€‚

åœ¨Runnableæ¥å£ä¸­å®šä¹‰äº†ä»¥ä¸‹æ ¸å¿ƒæ–¹æ³•ï¼š

`invoke(input)`ï¼šåŒæ­¥æ‰§è¡Œï¼Œå¤„ç†å•ä¸ªè¾“å…¥ï¼Œæœ€å¸¸ç”¨çš„æ–¹æ³•

`batch(inputs)`ï¼šæ‰¹é‡æ‰§è¡Œï¼Œå¤„ç†å¤šä¸ªè¾“å…¥ï¼Œæå‡å¤„ç†æ•ˆç‡

`stream(input)`ï¼šæµå¼æ‰§è¡Œï¼Œé€æ­¥è¿”å›ç»“æœï¼Œç»å…¸çš„ä½¿ç”¨åœºæ™¯æ˜¯å¤§æ¨¡å‹æ˜¯ä¸€ç‚¹ç‚¹è¾“å‡ºçš„ï¼Œä¸æ˜¯ä¸€ä¸‹è¿”å›æ•´ä¸ªç»“æœï¼Œå¯ä»¥é€šè¿‡ `stream()` æ–¹æ³•ï¼Œè¿›è¡Œæµå¼è¾“å‡º

`ainvoke(input)`ï¼šå¼‚æ­¥æ‰§è¡Œï¼Œç”¨äºé«˜å¹¶å‘åœºæ™¯ã€‚

## ç®¡é“è¿ç®—ç¬¦ 
è¿™æ˜¯ LCEL æœ€å…·ç‰¹è‰²çš„è¯­æ³•ç¬¦å·ã€‚å¤šä¸ª `Runnable` å¯¹è±¡å¯ä»¥é€šè¿‡ `|` ä¸²è”èµ·æ¥ï¼Œå½¢æˆæ¸…æ™°çš„æ•°æ®å¤„ç†é“¾ã€‚ä¾‹å¦‚ï¼š

```python
prompt | model | parser
```

è¡¨ç¤ºæ•°æ®å°†ä¾æ¬¡ä¼ å…¥æç¤ºæ¨¡æ¿ã€æ¨¡å‹å’Œè¾“å‡ºè§£æå™¨ï¼Œæœ€ç»ˆè¾“å‡ºç»“æ„åŒ–ç»“æœã€‚

## PromptTemplate ä¸ OutputParser
LCEL å¼ºè°ƒç»„ä»¶ä¹‹é—´çš„èŒè´£æ˜ç¡®ï¼ŒPrompt åªè´Ÿè´£æ¨¡æ¿åŒ–è¾“å…¥ï¼ŒParser åªè´Ÿè´£æ ¼å¼åŒ–è¾“å‡ºï¼ŒModel åªè´Ÿè´£æ¨ç†ã€‚

## Runnable ç±»ç»§æ‰¿å…³ç³»
åˆ†æLangChainæºç å¯çŸ¥ï¼Œåœ¨ LangChain çš„ç±»ç»“æ„ä¸­ï¼Œé¡¶å±‚åŸºç±»æ˜¯ `Runnable`ï¼Œç”¨äºå®šä¹‰æ‰€æœ‰å¯æ‰§è¡Œå¯¹è±¡çš„ç»Ÿä¸€æ¥å£ï¼Œå®ç°äº†æŠŠâ€œæ‰§è¡Œä¸€ä¸ªé€»è¾‘å•å…ƒâ€æŠ½è±¡ä¸ºä¸€ä¸ªç»Ÿä¸€çš„è¿è¡Œå•å…ƒã€‚åŒ…æ‹¬ï¼š

+ `invoke(input)`ï¼šåŒæ­¥æ‰§è¡Œ
+ `ainvoke(input)`ï¼šå¼‚æ­¥æ‰§è¡Œ
+ `batch(inputs)`ï¼šæ‰¹é‡æ‰§è¡Œ
+ `stream(input)`ï¼šæµå¼è¾“å‡º

è€Œ `RunnableSerializable` åœ¨ `Runnable` åŸºç¡€ä¸Šå¢åŠ  **åºåˆ—åŒ–/ååºåˆ—åŒ–** èƒ½åŠ›ï¼Œä½œä¸º LangChain å†…éƒ¨é“¾è·¯çš„çˆ¶ç±»åŸºç±»ã€‚

æˆ‘ä»¬å¸¸ç”¨çš„Promptã€Parserã€LLM éƒ½ç»§æ‰¿è‡ªè¿™ä¸ªç±»ï¼Œå› è€Œå®ƒä»¬éƒ½å¯ä»¥è¢«ç»„åˆè¿› Chain / Graph ä¸­ã€‚

![](https://via.placeholder.com/800x600?text=Image+c53e144615fc6568)

# é“¾å¼è°ƒç”¨åŸºç¡€ç”¨æ³•
## é¡ºåºé“¾
LangChain çš„ä¸€ä¸ªå…¸å‹é“¾æ¡ç”±Promptã€Modelã€OutputParser ï¼ˆå¯æ²¡æœ‰ï¼‰ç»„æˆï¼Œç„¶åå¯ä»¥é€šè¿‡ é“¾ï¼ˆChainï¼‰ æŠŠå®ƒä»¬é¡ºåºç»„åˆèµ·æ¥ï¼Œè®©ä¸€ä¸ªä»»åŠ¡çš„è¾“å‡ºæˆä¸ºä¸‹ä¸€ä¸ªä»»åŠ¡çš„è¾“å…¥ã€‚

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from loguru import logger

# åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿ï¼ŒåŒ…å«ç³»ç»Ÿè§’è‰²è®¾å®šå’Œç”¨æˆ·é—®é¢˜è¾“å…¥
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œè¯·ç®€çŸ­å›ç­”æˆ‘æå‡ºçš„é—®é¢˜"),
    ("human", "è¯·å›ç­”:{question}")
])

# ä½¿ç”¨å…·ä½“å‚æ•°å®ä¾‹åŒ–æç¤ºæ¨¡æ¿å¹¶è®°å½•æ—¥å¿—
prompt = chat_prompt.invoke({"role": "AIåŠ©æ‰‹", "question": "ä»€ä¹ˆæ˜¯LangChain"})
logger.info(prompt)

# åˆå§‹åŒ–OllamaèŠå¤©æ¨¡å‹ï¼ŒæŒ‡å®šä½¿ç”¨qwen3:8bæ¨¡å‹ï¼Œå…³é—­æ¨ç†æ¨¡å¼
model = ChatOllama(model="qwen3:8b", reasoning=False)

# è°ƒç”¨æ¨¡å‹è·å–åŸå§‹å“åº”å¹¶è®°å½•æ—¥å¿—
result = model.invoke(prompt)
logger.info(f"æ¨¡å‹åŸå§‹è¾“å‡º:\n{result}")

# åˆ›å»ºå­—ç¬¦ä¸²è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå¤„ç†æ¨¡å‹è¾“å‡º
parser = StrOutputParser ()

# è§£ææ¨¡å‹è¾“å‡ºä¸ºç»“æ„åŒ–ç»“æœå¹¶è®°å½•æ—¥å¿—
response = parser.invoke(result)
logger.info(f"è§£æåçš„ç»“æ„åŒ–ç»“æœ:\n{response}")

# è®°å½•è§£æç»“æœçš„æ•°æ®ç±»å‹
logger.info(f"ç»“æœç±»å‹: {type(response)}")

# æ„å»ºå¤„ç†é“¾ï¼šæç¤ºæ¨¡æ¿ -> æ¨¡å‹ -> è¾“å‡ºè§£æå™¨
chain = chat_prompt | model | parser

# æ‰§è¡Œå¤„ç†é“¾å¹¶è®°å½•æœ€ç»ˆç»“æœåŠæ•°æ®ç±»å‹
result_chain = chain.invoke({"role": "AIåŠ©æ‰‹", "question": "ä»€ä¹ˆæ˜¯LangChain"})
logger.info(f"Chainæ‰§è¡Œç»“æœ:\n {result_chain}")
logger.info(f"Chainæ‰§è¡Œç»“æœç±»å‹: {type(result_chain)}")
```

æ‰§è¡Œç»“æœå¦‚ä¸‹ï¼š

```python
2025-10-27 10:40:46.704 | INFO     | __main__:<module>:14 - messages=[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·ç®€çŸ­å›ç­”æˆ‘æå‡ºçš„é—®é¢˜', additional_kwargs={}, response_metadata={}), HumanMessage(content='è¯·å›ç­”:ä»€ä¹ˆæ˜¯LangChain', additional_kwargs={}, response_metadata={})]
2025-10-27 10:40:50.085 | INFO     | __main__:<module>:21 - æ¨¡å‹åŸå§‹è¾“å‡º:
content='LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€è®­ç»ƒå’Œéƒ¨ç½²è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œå®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’Œåº“ï¼Œå¸®åŠ©å¼€å‘è€…æ›´é«˜æ•ˆåœ°å¤„ç†è‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç®¡ç†ç­‰ã€‚' additional_kwargs={} response_metadata={'model': 'qwen3:8b', 'created_at': '2025-10-27T02:40:50.084694786Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3339143523, 'load_duration': 25314262, 'prompt_eval_count': 38, 'prompt_eval_duration': 72360888, 'eval_count': 46, 'eval_duration': 3238436499, 'model_name': 'qwen3:8b'} id='run--7c5d4e06-d18d-4606-8b98-6e5c3c3df06b-0' usage_metadata={'input_tokens': 38, 'output_tokens': 46, 'total_tokens': 84}
2025-10-27 10:40:50.085 | INFO     | __main__:<module>:28 - è§£æåçš„ç»“æ„åŒ–ç»“æœ:
LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€è®­ç»ƒå’Œéƒ¨ç½²è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œå®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’Œåº“ï¼Œå¸®åŠ©å¼€å‘è€…æ›´é«˜æ•ˆåœ°å¤„ç†è‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç®¡ç†ç­‰ã€‚
2025-10-27 10:40:50.086 | INFO     | __main__:<module>:31 - ç»“æœç±»å‹: <class 'str'>
2025-10-27 10:40:52.802 | INFO     | __main__:<module>:38 - Chainæ‰§è¡Œç»“æœ:
LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œå®ƒæä¾›å·¥å…·å’Œåº“æ¥æ„å»ºã€è®­ç»ƒå’Œéƒ¨ç½²åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ç¨‹åºã€‚
2025-10-27 10:40:52.802 | INFO     | __main__:<module>:39 - Chainæ‰§è¡Œç»“æœç±»å‹: <class 'str'>
```

å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨ LCEL è¯­æ³•åï¼Œè°ƒç”¨æ–¹æ³•å’Œè¿è¡Œç»“æœä¿æŒä¸å˜ï¼Œä½†ä»£ç è¯­æ³•å˜å¾—æ›´åŠ ç®€æ´ï¼Œæ‰©å±•æ€§ä¹Ÿæ›´å¥½ã€‚

## åˆ†æ”¯é“¾
åœ¨LangChainä¸­æä¾›äº†ç±»RunnableBranchæ¥å®ŒæˆLCELä¸­çš„æ¡ä»¶åˆ†æ”¯åˆ¤æ–­ï¼Œå®ƒå¯ä»¥æ ¹æ®è¾“å…¥çš„ä¸åŒé‡‡ç”¨ä¸åŒçš„å¤„ç†é€»è¾‘ï¼Œå…·ä½“ç¤ºä¾‹å¦‚ä¸‹ï¼Œåœ¨ä¸‹æ–¹ç¤ºä¾‹ä¸­ç¨‹åºä¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ä¸­æ˜¯å¦åŒ…å«è‹±è¯­ã€éŸ©è¯­ç­‰å…³é”®è¯ï¼Œæ¥é€‰æ‹©å¯¹åº”çš„æç¤ºè¯è¿›è¡Œå¤„ç†ã€‚æ ¹æ®åˆ¤æ–­ç»“æœï¼Œå†æ‰§è¡Œä¸åŒçš„é€»è¾‘åˆ†æ”¯ã€‚

![ç”»æ¿](https://via.placeholder.com/800x600?text=Image+88fc713b8e4708b4)

ä»£ç å¦‚ä¸‹

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch
from langchain_ollama import ChatOllama
from loguru import logger

# æ„å»ºæç¤ºè¯
english_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªè‹±è¯­ç¿»è¯‘ä¸“å®¶ï¼Œä½ å«å°è‹±"),
    ("human", "{query}")
])

japanese_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæ—¥è¯­ç¿»è¯‘ä¸“å®¶ï¼Œä½ å«å°æ—¥"),
    ("human", "{query}")
])

korean_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªéŸ©è¯­ç¿»è¯‘ä¸“å®¶ï¼Œä½ å«å°éŸ©"),
    ("human", "{query}")
])


def determine_language(inputs):
    """åˆ¤æ–­è¯­è¨€ç§ç±»"""
    query = inputs["query"]
    if "æ—¥è¯­" in query:
        return "japanese"
    elif "éŸ©è¯­" in query:
        return "korean"
    else:
        return "english"


# åˆå§‹åŒ–OllamaèŠå¤©æ¨¡å‹ï¼ŒæŒ‡å®šä½¿ç”¨qwen3:8bæ¨¡å‹ï¼Œå…³é—­æ¨ç†æ¨¡å¼
model = ChatOllama(model="qwen3:8b", reasoning=False)
# åˆ›å»ºå­—ç¬¦ä¸²è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå¤„ç†æ¨¡å‹è¾“å‡º
parser = StrOutputParser()
# åˆ›å»ºä¸€ä¸ªå¯è¿è¡Œçš„åˆ†æ”¯é“¾ï¼Œæ ¹æ®è¾“å…¥æ–‡æœ¬çš„è¯­è¨€ç±»å‹é€‰æ‹©ç›¸åº”çš„å¤„ç†æµç¨‹
# è¿”å›å€¼ï¼š
#   RunnableBranchå¯¹è±¡ï¼Œå¯æ ¹æ®è¾“å…¥åŠ¨æ€é€‰æ‹©æ‰§è¡Œè·¯å¾„çš„å¯è¿è¡Œé“¾
chain = RunnableBranch(
    (lambda x: determine_language(x) == "japanese", japanese_prompt | model | parser),
    (lambda x: determine_language(x) == "korean", korean_prompt | model | parser),
    (english_prompt | model | parser)
)

# æµ‹è¯•æŸ¥è¯¢
test_queries = [
    {'query': 'è¯·ä½ ç”¨éŸ©è¯­ç¿»è¯‘è¿™å¥è¯:"è§åˆ°ä½ å¾ˆé«˜å…´"'},
    {'query': 'è¯·ä½ ç”¨æ—¥è¯­ç¿»è¯‘è¿™å¥è¯:"è§åˆ°ä½ å¾ˆé«˜å…´"'},
    {'query': 'è¯·ä½ ç”¨è‹±è¯­ç¿»è¯‘è¿™å¥è¯:"è§åˆ°ä½ å¾ˆé«˜å…´"'}
]

for query_input in test_queries:
    # åˆ¤æ–­ä½¿ç”¨å“ªä¸ªæç¤ºè¯
    lang = determine_language(query_input)
    logger.info(f"æ£€æµ‹åˆ°è¯­è¨€ç±»å‹: {lang}")

    # æ ¹æ®è¯­è¨€ç±»å‹é€‰æ‹©å¯¹åº”çš„æç¤ºè¯å¹¶æ ¼å¼åŒ–
    if lang == "japanese":
        prompt = japanese_prompt
    elif lang == "korean":
        prompt = korean_prompt
    else:
        prompt = english_prompt

    # æ ¼å¼åŒ–æç¤ºè¯å¹¶æ‰“å°
    formatted_messages = prompt.format_messages(**query_input)
    logger.info("æ ¼å¼åŒ–åçš„æç¤ºè¯:")
    for msg in formatted_messages:
        logger.info(f"[{msg.type}]: {msg.content}")

    # æ‰§è¡Œé“¾
    result = chain.invoke(query_input)
    logger.info(f"è¾“å‡ºç»“æœ: {result}\n")

```

æ‰§è¡Œç»“æœå¦‚ä¸‹

```bash
2025-10-29 09:32:38.244 | INFO     | __main__:<module>:54 - æ£€æµ‹åˆ°è¯­è¨€ç±»å‹: korean
2025-10-29 09:32:38.245 | INFO     | __main__:<module>:66 - æ ¼å¼åŒ–åçš„æç¤ºè¯:
2025-10-29 09:32:38.245 | INFO     | __main__:<module>:68 - [system]: ä½ æ˜¯ä¸€ä¸ªéŸ©è¯­ç¿»è¯‘ä¸“å®¶ï¼Œä½ å«å°éŸ©
2025-10-29 09:32:38.245 | INFO     | __main__:<module>:68 - [human]: è¯·ä½ ç”¨éŸ©è¯­ç¿»è¯‘è¿™å¥è¯:"è§åˆ°ä½ å¾ˆé«˜å…´"
2025-10-29 09:32:39.728 | INFO     | __main__:<module>:72 - è¾“å‡ºç»“æœ: ì•ˆë…•í•˜ì„¸ìš”, ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤.

2025-10-29 09:32:39.728 | INFO     | __main__:<module>:54 - æ£€æµ‹åˆ°è¯­è¨€ç±»å‹: japanese
2025-10-29 09:32:39.728 | INFO     | __main__:<module>:66 - æ ¼å¼åŒ–åçš„æç¤ºè¯:
2025-10-29 09:32:39.728 | INFO     | __main__:<module>:68 - [system]: ä½ æ˜¯ä¸€ä¸ªæ—¥è¯­ç¿»è¯‘ä¸“å®¶ï¼Œä½ å«å°æ—¥
2025-10-29 09:32:39.728 | INFO     | __main__:<module>:68 - [human]: è¯·ä½ ç”¨æ—¥è¯­ç¿»è¯‘è¿™å¥è¯:"è§åˆ°ä½ å¾ˆé«˜å…´"
2025-10-29 09:32:41.239 | INFO     | __main__:<module>:72 - è¾“å‡ºç»“æœ: ã“ã¡ã‚‰ã“ãã€ãŠä¼šã„ã§ãã¦å…‰æ „ã§ã™ã€‚

2025-10-29 09:32:41.240 | INFO     | __main__:<module>:54 - æ£€æµ‹åˆ°è¯­è¨€ç±»å‹: english
2025-10-29 09:32:41.240 | INFO     | __main__:<module>:66 - æ ¼å¼åŒ–åçš„æç¤ºè¯:
2025-10-29 09:32:41.240 | INFO     | __main__:<module>:68 - [system]: ä½ æ˜¯ä¸€ä¸ªè‹±è¯­ç¿»è¯‘ä¸“å®¶ï¼Œä½ å«å°è‹±
2025-10-29 09:32:41.240 | INFO     | __main__:<module>:68 - [human]: è¯·ä½ ç”¨è‹±è¯­ç¿»è¯‘è¿™å¥è¯:"è§åˆ°ä½ å¾ˆé«˜å…´"
2025-10-29 09:32:42.098 | INFO     | __main__:<module>:72 - è¾“å‡ºç»“æœ: Nice to meet you!
```

## ä¸²è¡Œé“¾
ä¾‹å¦‚æˆ‘ä»¬éœ€è¦å¤šæ¬¡è°ƒç”¨å¤§æ¨¡å‹ï¼Œå°†å¤šä¸ªæ­¥éª¤ä¸²è”èµ·æ¥å®ç°åŠŸèƒ½ï¼Œæµç¨‹å¦‚ä¸‹ï¼š

![ç”»æ¿](https://via.placeholder.com/800x600?text=Image+fdfcd6697a223079)

ä»£ç å¦‚ä¸‹ï¼š

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from loguru import logger

# è®¾ç½®æœ¬åœ°æ¨¡å‹ï¼Œä¸ä½¿ç”¨æ·±åº¦æ€è€ƒ
model = ChatOllama(model="qwen3:8b", reasoning=False)

# å­é“¾1æç¤ºè¯
prompt1 = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„è®¡ç®—æœºä¸“å®¶ï¼Œè¯·ç”¨ä¸­æ–‡ç®€çŸ­å›ç­”"),
    ("human", "è¯·ç®€çŸ­ä»‹ç»ä»€ä¹ˆæ˜¯{topic}")
])
# å­é“¾1è§£æå™¨
parser1 = StrOutputParser()
# å­é“¾1ï¼šç”Ÿæˆå†…å®¹
chain1 = prompt1 | model | parser1

# å­é“¾2æç¤ºè¯
prompt2 = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œå°†ç”¨æˆ·è¾“å…¥å†…å®¹ç¿»è¯‘æˆè‹±æ–‡"),
    ("human", "{input}")
])
# å­é“¾2è§£æå™¨
parser2 = StrOutputParser()

# å­é“¾2ï¼šç¿»è¯‘å†…å®¹
chain2 = prompt2 | model | parser2

# ç»„åˆæˆä¸€ä¸ªå¤åˆ Chainï¼Œä½¿ç”¨ lambda å‡½æ•°å°†chain1æ‰§è¡Œç»“æœcontentå†…å®¹æ·»åŠ inputé”®ä½œä¸ºå‚æ•°ä¼ é€’ç»™chain2
full_chain = chain1 | (lambda content: {"input": content}) | chain2

# è°ƒç”¨å¤åˆé“¾
result = full_chain.invoke({"topic": "langchain"})
logger.info(result)
```

ç”Ÿæˆç»“æœå¦‚ä¸‹ï¼š

```python
2025-10-29 09:42:13.473 | INFO     | __main__:<module>:35 - LangChain is a framework used for building, training, and deploying language models (such as large language models), providing tools and modules to manage model input and output, data processing, prompt engineering, model invocation, and more, helping developers build language model-based applications more efficiently.
```

## å¹¶è¡Œé“¾
åœ¨ **Langchain** ä¸­ï¼Œåˆ›å»º**å¹¶è¡Œé“¾ï¼ˆParallel Chainsï¼‰**ï¼Œæ˜¯æŒ‡**åŒæ—¶è¿è¡Œå¤šä¸ªå­é“¾ï¼ˆChainï¼‰**ï¼Œå¹¶åœ¨å®ƒä»¬éƒ½å®Œæˆåæ±‡æ€»ç»“æœã€‚è¿™åœ¨ä»¥ä¸‹åœºæ™¯ä¸­éå¸¸æœ‰ç”¨ï¼š

+ åŒæ—¶é—®å¤šä¸ªé—®é¢˜å¹¶èšåˆç»“æœ
+ å¤šä¸ª model åŒæ—¶å·¥ä½œå–æœ€ä¼˜ç­”æ¡ˆ
+ å¤šè·¯å¾„æ¨ç†ã€å¤šæ¨¡æ€å¤„ç†ï¼ˆå¦‚å›¾ç‰‡+æ–‡å­—ï¼‰

![ç”»æ¿](https://via.placeholder.com/800x600?text=Image+4baa8a3da2074d41)

ä¾‹å¦‚ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥å†…å®¹ï¼ŒåŒæ—¶ç”Ÿæˆä¸­æ–‡å’Œè‹±æ–‡å›å¤ã€‚

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain_core.runnables import RunnableParallel
from loguru import logger

# è®¾ç½®æœ¬åœ°æ¨¡å‹ï¼Œä¸ä½¿ç”¨æ·±åº¦æ€è€ƒ
model = ChatOllama(model="qwen3:8b", reasoning=False)

# å¹¶è¡Œé“¾1æç¤ºè¯
prompt1 = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„è®¡ç®—æœºä¸“å®¶ï¼Œè¯·ç”¨ä¸­æ–‡ç®€çŸ­å›ç­”"),
    ("human", "è¯·ç®€çŸ­ä»‹ç»ä»€ä¹ˆæ˜¯{topic}")
])
# å¹¶è¡Œé“¾1è§£æå™¨
parser1 = StrOutputParser()
# å¹¶è¡Œé“¾1ï¼šç”Ÿæˆä¸­æ–‡ç»“æœ
chain1 = prompt1 | model | parser1

# å¹¶è¡Œé“¾2æç¤ºè¯
prompt2 = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„è®¡ç®—æœºä¸“å®¶ï¼Œè¯·ç”¨è‹±æ–‡ç®€çŸ­å›ç­”"),
    ("human", "è¯·ç®€çŸ­ä»‹ç»ä»€ä¹ˆæ˜¯{topic}")
])
# å¹¶è¡Œé“¾2è§£æå™¨
parser2 = StrOutputParser()

# å¹¶è¡Œé“¾2ï¼šç”Ÿæˆè‹±æ–‡ç»“æœ
chain2 = prompt2 | model | parser2

# åˆ›å»ºå¹¶è¡Œé“¾,ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªè¯­è¨€å¤„ç†é“¾
parallel_chain = RunnableParallel({
    "chinese": chain1,
    "english": chain2
})

# è°ƒç”¨å¤åˆé“¾
result = parallel_chain.invoke({"topic": "langchain"})
logger.info(result)

```

æ‰§è¡Œç»“æœå¦‚ä¸‹

```python
2025-10-29 09:50:56.712 | INFO     | __main__:<module>:39 - {'chinese': 'LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œæä¾›å·¥å…·å’Œæ¨¡å—æ¥å®ç°æç¤ºå·¥ç¨‹ã€æ•°æ®å¤„ç†ã€æ¨¡å‹è°ƒç”¨ç­‰åŠŸèƒ½ï¼Œå¸®åŠ©å¼€å‘è€…æ›´é«˜æ•ˆåœ°å¼€å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLMï¼‰çš„åº”ç”¨ã€‚', 'english': 'LangChain is a framework that enables developers to build applications using large language models (LLMs) by providing tools for task execution, memory, and model interaction. It allows for chaining multiple LLM calls and integrating them with external data sources.'}
```

# é“¾å¼è°ƒç”¨è¿›é˜¶ç”¨æ³•
## å‡½æ•°è½¬å¯æ‰§è¡Œé“¾
`RunnableLambda` æ˜¯ LangChain çš„ä¸€ä¸ªåŒ…è£…å™¨ï¼Œå®ƒå¯ä»¥æŠŠä¸€ä¸ªæ™®é€šçš„ Python å‡½æ•°ï¼ˆlambda æˆ– defï¼‰ è½¬æ¢ä¸ºä¸€ä¸ª å¯æ‰§è¡Œçš„é“¾ï¼ˆRunnableï¼‰ã€‚ç„¶åæˆ‘ä»¬å°±å¯ä»¥åƒå¯¹å¾…æ¨¡å‹ã€Promptã€Parser ä¸€æ ·ï¼ŒæŠŠå®ƒä¸å…¶ä»–ç»„ä»¶ç”¨ `|` è¿ç®—ç¬¦è¿æ¥ã€‚

ä½¿ç”¨åœºæ™¯ï¼šç”±äºæ¯æ¬¡ AI ç”Ÿæˆç»“æœçš„ä¸ç¡®å®šæ€§ï¼Œåœ¨å¼€å‘è¿‡ç¨‹ä¸­å¯èƒ½éœ€è¦æ·»åŠ ä¸€äº›è‡ªå®šä¹‰èŠ‚ç‚¹å®ç°åŠŸèƒ½ï¼Œæ¯”å¦‚ æ ¼å¼åŒ–ã€è¿‡æ»¤ã€æ˜ å°„ç­‰æ“ä½œã€‚ä¾‹å¦‚æ‰§è¡Œæ‰“å°å‡½æ•°æŸ¥çœ‹ç¬¬ä¸€é˜¶æ®µç”Ÿæˆç»“æœï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_ollama import ChatOllama
from loguru import logger

# è®¾ç½®æœ¬åœ°æ¨¡å‹ï¼Œä¸ä½¿ç”¨æ·±åº¦æ€è€ƒ
model = ChatOllama(model="qwen3:8b", reasoning=False)


# ä¸€ä¸ªç®€å•çš„æ‰“å°å‡½æ•°ï¼Œè°ƒè¯•ç”¨
def debug_print(x):
    logger.info(f"ä¸­é—´ç»“æœ:{x}")
    return {"input": x}


# å­é“¾1æç¤ºè¯
prompt1 = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„è®¡ç®—æœºä¸“å®¶ï¼Œè¯·ç”¨ä¸­æ–‡ç®€çŸ­å›ç­”"),
    ("human", "è¯·ç®€çŸ­ä»‹ç»ä»€ä¹ˆæ˜¯{topic}")
])
# å­é“¾1è§£æå™¨
parser1 = StrOutputParser()
# å­é“¾1ï¼šç”Ÿæˆå†…å®¹
chain1 = prompt1 | model | parser1

# å­é“¾2æç¤ºè¯
prompt2 = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œå°†ç”¨æˆ·è¾“å…¥å†…å®¹ç¿»è¯‘æˆè‹±æ–‡"),
    ("human", "{input}")
])
# å­é“¾2è§£æå™¨
parser2 = StrOutputParser()

# å­é“¾2ï¼šç¿»è¯‘å†…å®¹
chain2 = prompt2 | model | parser2
# åˆ›å»ºä¸€ä¸ªå¯è¿è¡Œçš„è°ƒè¯•èŠ‚ç‚¹ï¼Œç”¨äºæ‰“å°ä¸­é—´ç»“æœ
debug_node = RunnableLambda(debug_print)

# æ„å»ºå®Œæ•´çš„å¤„ç†é“¾ï¼Œå°†chain1ã€è°ƒè¯•æ‰“å°å’Œchain2ä¸²è”èµ·æ¥
full_chain = chain1 | debug_print | chain2

# è°ƒç”¨å¤åˆé“¾
result = full_chain.invoke({"topic": "langchain"})
logger.info(f"æœ€ç»ˆç»“æœ:{result}")
```

æ‰§è¡Œç»“æœå¦‚ä¸‹

```python
2025-10-29 10:00:21.723 | INFO     | __main__:debug_print:13 - ä¸­é—´ç»“æœ:LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºã€è®­ç»ƒå’Œéƒ¨ç½²è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œå®ƒæä¾›äº†ä¸€ç³»åˆ—å·¥å…·å’Œæ¥å£ï¼Œå¸®åŠ©å¼€å‘è€…æ›´é«˜æ•ˆåœ°ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLMï¼‰è¿›è¡Œä»»åŠ¡å¦‚æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚
2025-10-29 10:00:26.358 | INFO     | __main__:<module>:45 - æœ€ç»ˆç»“æœ:LangChain is a framework for building, training, and deploying applications based on language models. It provides a series of tools and interfaces that help developers efficiently use large language models (LLMs) to perform tasks such as text generation, question-answering, and chat systems.
```

## å‚æ•°ä¼ é€’
`RunnableParallel` æ˜¯ LangChain æ„å»ºâ€œå¤šè·¯å¹¶å‘æ•°æ®æµâ€çš„æ ¸å¿ƒæ¨¡å—ï¼Œå®ƒèƒ½è®©æ£€ç´¢ã€é¢„å¤„ç†ã€ç¿»è¯‘ç­‰æ“ä½œå¹¶è¡Œæ‰§è¡Œï¼Œå¹¶å°†ç»“æœæ— ç¼è¡”æ¥åˆ°åç»­çš„ LLM æ¨ç†ä¸­ã€‚  

ä¸‹é¢ç¤ºä¾‹å±•ç¤ºäº†æ¨¡æ‹Ÿåœ¨å’Œå¤§è¯­è¨€æ¨¡å‹äº¤äº’ä¹‹å‰ï¼Œå…ˆæ£€ç´¢æ–‡æ¡£çš„æ“ä½œï¼Œé€šè¿‡RunnableParallelå°†æ‰§è¡Œç»“æœä½œä¸ºæç¤ºè¯æ¨¡æ¿çš„è¾“å…¥å‚æ•°ï¼Œå°†è¾“å‡ºç»“æœç»§ç»­å‘ä¸‹ä¼ é€’ã€‚

ç›¸å½“äºä¼ é€’ç»™æç¤ºè¯æ¨¡æ¿çš„å‚æ•°ä»æœ€å¼€å§‹çš„ä¸€ä¸ªquestionï¼Œåˆå¢åŠ äº†ä¸€ä¸ªæ£€ç´¢æ–‡æ¡£ç»“æœçš„å‚æ•°retrieval_infoï¼Œå¹¶ä¸”ï¼Œè¿™é‡Œä½¿ç”¨äº†ç®€å†™æ–¹å¼ï¼Œåœ¨LCELè¡¨è¾¾å¼ä¸­ï¼Œä½¿ç”¨å­—å…¸ç»“æ„åŒ…è£¹å¹¶åœ¨ç®¡é“ç¬¦ä¸¤ä¾§çš„ï¼Œéƒ½ä¼šè‡ªåŠ¨åŒ…è£…æˆRunnableParallelã€‚

```python
from operator import itemgetter
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from loguru import logger


def retrieval_doc(question):
    """æ¨¡æ‹ŸçŸ¥è¯†åº“æ£€ç´¢"""
    logger.info(f"æ£€ç´¢å™¨æ¥æ”¶åˆ°ç”¨æˆ·æå‡ºé—®é¢˜ï¼š{question}")
    return "ä½ æ˜¯ä¸€ä¸ªè¯´è¯é£è¶£å¹½é»˜çš„AIåŠ©æ‰‹ï¼Œä½ å«äº®ä»”"


# è®¾ç½®æœ¬åœ°æ¨¡å‹ï¼Œä¸ä½¿ç”¨æ·±åº¦æ€è€ƒ
model = ChatOllama(model="qwen3:8b", reasoning=False)

# æ„å»ºæç¤ºè¯
prompt = ChatPromptTemplate.from_messages([
    ("system", "{retrieval_info}"),
    ("human", "è¯·ç®€çŸ­å›ç­”{question}")
])
# åˆ›å»ºå­—ç¬¦ä¸²è¾“å‡ºè§£æå™¨
parser = StrOutputParser()
# æ„å»ºå®Œæ•´é“¾æ¡ï¼ˆChainï¼‰ï¼š
# - é¦–å…ˆä»è¾“å…¥ä¸­å–å‡º questionï¼ˆé—®é¢˜ï¼‰å¹¶ä¼ ç»™ä¸¤ä¸ªå‡½æ•°ï¼š
#   1. ä¼ ç»™ lambda è·å– retrieval_infoï¼ˆè§’è‰²è®¾å®šï¼‰
#   2. ä½¿ç”¨ itemgetter ä¿ç•™ question åŸæ–‡
# - ç„¶åå°†è¿™äº›å†…å®¹è¾“å…¥ prompt æ¨¡æ¿
# - æ¨¡å‹æ‰§è¡Œæ¨ç†
# - æœ€åè§£ææ¨¡å‹è¾“å‡ºä¸ºçº¯æ–‡æœ¬
chain = {
            "retrieval_info": lambda x: retrieval_doc(x["question"]),
            "question": itemgetter("question")
        } | prompt | model | parser

# 5.æ‰§è¡Œé“¾
result = chain.invoke({'question': 'ä½ æ˜¯è°ï¼Œä»€ä¹ˆå«LangChainï¼Ÿ'})
logger.info(result)
```

æ‰§è¡Œç»“æœå¦‚ä¸‹

```python
2025-10-29 10:08:16.772 | INFO     | __main__:retrieval_doc:11 - æ£€ç´¢å™¨æ¥æ”¶åˆ°ç”¨æˆ·æå‡ºé—®é¢˜ï¼šä½ æ˜¯è°ï¼Œä»€ä¹ˆå«LangChainï¼Ÿ
2025-10-29 10:08:21.219 | INFO     | __main__:<module>:39 - å˜¿ï¼Œæˆ‘æ˜¯äº®ä»”ï¼Œä¸€ä¸ªè¯´è¯é£è¶£çš„AIåŠ©æ‰‹ï¼ğŸ˜„
è‡³äºLangChainï¼Œå®ƒå°±åƒæ˜¯AIç•Œçš„â€œæ­ç§¯æœ¨â€å·¥å…·ï¼Œå¸®ä½ æŠŠå„ç§AIæ¨¡å‹ã€æ•°æ®ã€å·¥å…·ä¸²èµ·æ¥ï¼Œç©å‡ºèŠ±æ¥ã€‚ç®€å•è¯´ï¼Œå°±æ˜¯è®©AIå˜å¾—æ›´çµæ´»ã€æ›´å¼ºå¤§ï¼
```

##  æ•°æ®é€ä¼ 
RunnablePassthroughæ˜¯ä¸€ä¸ªç›¸å¯¹ç‰¹æ®Šçš„ç»„ä»¶ï¼Œå®ƒçš„ä½œç”¨æ˜¯å°†è¾“å…¥æ•°æ®åŸæ ·ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªå¯æ‰§è¡Œç»„ä»¶ï¼ŒåŒæ—¶è¿˜èƒ½å¯¹ä¼ é€’çš„æ•°æ®è¿›è¡Œæ•°æ®é‡ç»„ã€‚è™½ç„¶åŠŸèƒ½ç®€å•ï¼Œä½†åœ¨å¤æ‚çš„ Chain æ„å»ºä¸­éå¸¸å¸¸ç”¨ï¼Œå°¤å…¶ç”¨äº ä¿æŒè¾“å…¥æ•°æ®æµä¸ä¸­æ–­ æˆ– ä¸å¹¶è¡Œåˆ†æ”¯ç»“åˆã€‚  

RunnablePassthroughæœ€å¼ºå¤§çš„åŠŸèƒ½æ˜¯å¯ä»¥é‡æ–°ç»„ç»‡æ•°æ®ç»“æ„ï¼Œä¸ºåç»­é“¾æ‰§è¡Œåšå‡†å¤‡ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼Œæˆ‘ä»¬æ”¹å†™äº†ä¹‹å‰ä½¿ç”¨RunnableParallelè¿›è¡Œæ£€ç´¢çš„ç¤ºä¾‹ï¼Œé€šè¿‡RunnablePassthrough.assign()æ–¹æ³•ä¹Ÿèƒ½è¾¾åˆ°ç›®çš„ï¼Œå¯ä»¥å‘å…¥å‚ä¸­æ·»åŠ æ–°çš„å±æ€§ï¼Œä¸‹é¢ç¤ºä¾‹æ·»åŠ äº†æ£€ç´¢ç»“æœå±æ€§retrieval_infoï¼Œå°†æ–°çš„æ•°æ®ç»§ç»­å‘ä¸‹ä¼ é€’ã€‚

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_ollama import ChatOllama
from loguru import logger


def retrieval_doc(question):
    """æ¨¡æ‹ŸçŸ¥è¯†åº“æ£€ç´¢"""
    logger.info(f"æ£€ç´¢å™¨æ¥æ”¶åˆ°ç”¨æˆ·æå‡ºé—®é¢˜ï¼š{question}")
    return "ä½ æ˜¯ä¸€ä¸ªè¯´è¯é£è¶£å¹½é»˜çš„AIåŠ©æ‰‹ï¼Œä½ å«äº®ä»”"


# è®¾ç½®æœ¬åœ°æ¨¡å‹ï¼Œä¸ä½¿ç”¨æ·±åº¦æ€è€ƒ
model = ChatOllama(model="qwen3:8b", reasoning=False)

# æ„å»ºæç¤ºè¯
prompt = ChatPromptTemplate.from_messages([
    ("system", "{retrieval_info}"),
    ("human", "è¯·ç®€çŸ­å›ç­”{question}")
])
# åˆ›å»ºå­—ç¬¦ä¸²è¾“å‡ºè§£æå™¨
parser = StrOutputParser()
# æ„å»ºé“¾
# 1. ä½¿ç”¨ RunnablePassthrough.assign æ³¨å…¥ retrieval_info å­—æ®µï¼Œ
#    å®é™…ä¸Šæ˜¯è®© `retrieval_doc` å‡½æ•°åœ¨é“¾å¼€å§‹æ—¶æ‰§è¡Œï¼Œå¹¶å°†å…¶ç»“æœåŠ åˆ° inputs å­—å…¸ä¸­ã€‚
#    å³ï¼šè¾“å…¥ {"question": "xxx"} -> è¾“å‡º {"question": "xxx", "retrieval_info": "ä½ æ˜¯ä¸€ä¸ªæ„¤æ€’çš„è¯­æ–‡è€å¸ˆ..."}
# 2. è¯¥å®Œæ•´å­—å…¸è¢«ä¼ å…¥ prompt ä¸­ç”Ÿæˆå¯¹è¯æ¶ˆæ¯
# 3. ç„¶åä¼ å…¥ model è·å–å›ç­”
# 4. æœ€åä½¿ç”¨ parser æå–å­—ç¬¦ä¸²è¾“å‡º
chain = RunnablePassthrough.assign(retrieval_info=retrieval_doc) | prompt | model | parser

# æ‰§è¡Œé“¾
result = chain.invoke({'question': 'ä½ æ˜¯è°ï¼Œä»€ä¹ˆæ˜¯LangChain'})
logger.info(result)
```

æ‰§è¡Œç»“æœå¦‚ä¸‹

```python
2025-10-29 10:17:23.750 | INFO     | __main__:retrieval_doc:10 - æ£€ç´¢å™¨æ¥æ”¶åˆ°ç”¨æˆ·æå‡ºé—®é¢˜ï¼š{'question': 'ä½ æ˜¯è°ï¼Œä»€ä¹ˆæ˜¯LangChain'}
2025-10-29 10:17:27.869 | INFO     | __main__:<module>:35 - å˜¿ï¼Œæˆ‘æ˜¯äº®ä»”ï¼Œä¸€ä¸ªè¯´è¯é£è¶£çš„AIåŠ©æ‰‹ï¼  
LangChain æ˜¯ä¸€ä¸ªç”¨æ¥æ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œç®€å•è¯´å°±æ˜¯å¸®ä½ æŠŠå¤§æ¨¡å‹ï¼ˆæ¯”å¦‚æˆ‘ï¼‰å˜æˆèƒ½å¹²æ´»çš„å·¥å…·ï¼Œæ¯”å¦‚å†™ä»£ç ã€åšåˆ†æã€èŠå¤©ç­‰ç­‰ã€‚
```

## å›¾å½¢åŒ–æ‰“å°é“¾å›¾
Langchain æ”¯æŒåœ¨ç»ˆç«¯å›¾å½¢åŒ–åœ°æ‰“å°é“¾ç»“æ„å›¾ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨ Langchain Expression Language (LCEL) åˆ›å»ºé“¾ï¼ˆæ¯”å¦‚ `RunnableSequence`, `RunnableParallel` ç­‰ï¼‰åï¼Œå¯ä»¥é€šè¿‡å†…ç½®çš„ `.get_graph().print_ascii()` æ¥ç”Ÿæˆç±»ä¼¼â€œæµç¨‹å›¾â€çš„è¾“å‡ºï¼Œéå¸¸é€‚åˆè°ƒè¯•å’Œç†è§£é“¾çš„ç»“æ„ã€‚  

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain_core.runnables import RunnableParallel
from loguru import logger

# è®¾ç½®æœ¬åœ°æ¨¡å‹ï¼Œä¸ä½¿ç”¨æ·±åº¦æ€è€ƒ
model = ChatOllama(model="qwen3:8b", reasoning=False)

# å¹¶è¡Œé“¾1æç¤ºè¯
prompt1 = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„è®¡ç®—æœºä¸“å®¶ï¼Œè¯·ç”¨ä¸­æ–‡ç®€çŸ­å›ç­”"),
    ("human", "è¯·ç®€çŸ­ä»‹ç»ä»€ä¹ˆæ˜¯{topic}")
])
# å¹¶è¡Œé“¾1è§£æå™¨
parser1 = StrOutputParser()
# å¹¶è¡Œé“¾1ï¼šç”Ÿæˆä¸­æ–‡ç»“æœ
chain1 = prompt1 | model | parser1

# å¹¶è¡Œé“¾2æç¤ºè¯
prompt2 = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„è®¡ç®—æœºä¸“å®¶ï¼Œè¯·ç”¨è‹±æ–‡ç®€çŸ­å›ç­”"),
    ("human", "è¯·ç®€çŸ­ä»‹ç»ä»€ä¹ˆæ˜¯{topic}")
])
# å¹¶è¡Œé“¾2è§£æå™¨
parser2 = StrOutputParser()

# å¹¶è¡Œé“¾2ï¼šç”Ÿæˆè‹±æ–‡ç»“æœ
chain2 = prompt2 | model | parser2

# åˆ›å»ºå¹¶è¡Œé“¾,ç”¨äºåŒæ—¶æ‰§è¡Œå¤šä¸ªè¯­è¨€å¤„ç†é“¾
parallel_chain = RunnableParallel({
    "chinese": chain1,
    "english": chain2
})
# å°†å¹¶è¡Œé“¾çš„è®¡ç®—å›¾ç»˜åˆ¶ä¸ºPNGå›¾ç‰‡å¹¶ä¿å­˜
# parallel_chain.get_graph().draw_png("chain.png")
# æ‰“å°å¹¶è¡Œé“¾çš„ASCIIå›¾å½¢è¡¨ç¤º
parallel_chain.get_graph().print_ascii()
# è°ƒç”¨å¤åˆé“¾
result = parallel_chain.invoke({"topic": "langchain"})
logger.info(f"æœ€ç»ˆç»“æœ:{result}")
```

æ‰§è¡Œç»“æœå¦‚ä¸‹

```python
            +--------------------------------+             
            | Parallel<chinese,english>Input |             
            +--------------------------------+             
                   ***               ***                   
                ***                     ***                
              **                           **              
+--------------------+              +--------------------+ 
| ChatPromptTemplate |              | ChatPromptTemplate | 
+--------------------+              +--------------------+ 
           *                                   *           
           *                                   *           
           *                                   *           
    +------------+                      +------------+     
    | ChatOllama |                      | ChatOllama |     
    +------------+                      +------------+     
           *                                   *           
           *                                   *           
           *                                   *           
  +-----------------+                 +-----------------+  
  | StrOutputParser |                 | StrOutputParser |  
  +-----------------+                 +-----------------+  
                   ***               ***                   
                      ***         ***                      
                         **     **                         
            +---------------------------------+            
            | Parallel<chinese,english>Output |            
            +---------------------------------+            
2025-10-29 10:22:34.925 | INFO     | __main__:<module>:42 - æœ€ç»ˆç»“æœ:{'chinese': 'LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œå®ƒæä¾›å·¥å…·å’Œåº“ï¼Œå¸®åŠ©å¼€å‘è€…é«˜æ•ˆåœ°æ•´åˆã€æ‰©å±•å’Œéƒ¨ç½²åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLMï¼‰çš„åº”ç”¨ç¨‹åºã€‚', 'english': 'LangChain is a framework that enables developers to build applications using large language models (LLMs) by providing tools for task execution, memory, and integration with other systems. It allows for chaining multiple LLM calls and managing complex workflows.'}
```


