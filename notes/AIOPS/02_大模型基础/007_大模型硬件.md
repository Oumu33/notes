# 大模型硬件
# GPU
## <font style="color:rgb(0, 0, 0);">GPU是什么？</font>
GPU的英文全称Graphics Processing Unit，图形处理单元。

说直白一点：GPU是一款专门的图形处理芯片，做图形渲染、数值分析、金融分析、密码破解，以及其他数学计算与几何运算的。GPU可以在PC、工作站、游戏主机、手机、平板等多种智能终端设备上运行。

GPU和显卡的关系，就像是CPU和主板的关系。前者是显卡的心脏，后者是主板的心脏。有些小伙伴会把GPU和显卡当成一个东西，其实还有些差别的，显卡不仅包括GPU，还有一些显存、VRM稳压模块、MRAM芯片、总线、风扇、外围设备接口等等。

## CPU 与 GPU 区别
CPU和GPU都是运算的处理器，在架构组成上都包括3个部分：运算单元ALU、控制单元Control和缓存单元Cache。

但是，三者的组成比例却相差很大。

在CPU中缓存单元大概占50%，控制单元25%，运算单元25%；

在GPU中缓存单元大概占5%，控制单元5%，运算单元90%。

结构组成上的巨大差异说明：CPU的运算能力更加均衡，但是不适合做大量的运算；GPU更适合做大量运算。

这倒不是说GPU更牛X，实际上GPU更像是一大群工厂流水线上的工人，适合做大量的简单运算，很复杂的搞不了。但是简单的事情做得非常快，比CPU要快得多。

相比GPU，CPU更像是技术专家，可以做复杂的运算，比如逻辑运算、响应用户请求、网络通信等。但是因为ALU占比较少、内核少，所以适合做相对少量的复杂运算。

## 常见 GPU 参数
| GPU型号 | 架构 | FP16性能 | FP32性能 | 显存 | 显存类型 | 带宽 |
| --- | --- | --- | --- | --- | --- | --- |
| H100 | Hopper | 1,671 TFLOPS | 60 TFLOPS | 80GB | HBM3 | 3.9 TB/s |
| A100 | Ampere | 312 TFLOPS | 19.5 TFLOPS | 40GB / 80GB | HBM2 | 2,039 GB/s |
| A6000 | Ampere | 77.4 TFLOPS | 38.7 TFLOPS | 48GB | GDDR6 | 768 GB/s |
| A4000 | Ampere | 19.17 TFLOPS | 19.17 TFLOPS | 16GB | GDDR6 | 448 GB/s |
| V100 | Volta | 125 TFLOPS | 15.7 TFLOPS | 32GB | HBM2 | 900 GB/s |
| P6000 | Pascal | 12 TFLOPS | 12 TFLOPS | 24GB | GDDR5X | 432 GB/s |
| RTX 4000 | Turing | 14.2 TFLOPS | 7.1 TFLOPS | 8GB | GDDR6 | 416 GB/s |
| L40s | Ada Lovelace | 731 TFLOPS | 91.6 TFLOPS | 48GB | GDDR6 | 864GB/s |
| L4 | Ada Lovelace | 242 TFLOPS (Tensor Core) | 30 TFLOPS | 24GB | GDDR6 | 300GB/s |


# 通信协议
## PCIE
是一种高速串行计算机扩展总线标准，由英特尔在2001年提出，旨在替代旧的PCI、PCI-X和AGP总线标准

+ 差分通信：使用差分信号传输，具有抗干扰能力强、信号完整性好的特点，通过提高总线频率来提高总线带宽
+ 多lane支持：由多个lane组成，每个lane由两组差分线组成，一组用于发送，另一组用于接收。

## NVLink
同主机内不同 GPU 之间的一种高速互联方式。

+ 短距离通信优化：是一种短距离通信链路，保证包的成功传输，更高性能，替代 PCIe，
+ 多lane支持：link 带宽随 lane 数量线性增长，
+ 互联通信：同一台 node 内的 GPU 通过 NVLink 以 full-mesh 方式（类似 spine-leaf）互联
+ NVIDIA专利技术：专为NVIDIA的GPU设计，以实现最佳的性能和效率。

## NVSwitch
+ NVSwitch 是 NVIDIA 的一款交换芯片，封装在 GPU module 上，并不是主机外的独立交换机，用来连接同一台主机内的 GPU。 
+ 2022 年，NVIDIA 把这块芯片拿出来真的做成了交换机，叫 NVLink Switch ， 用来跨主机连接 GPU 设备。

## 技术对比
| 特性 | PCIe | NVLink | NVSwitch |
| --- | --- | --- | --- |
| 主要用途 | 通用互联接口，连接 GPU、CPU、SSD 等设备 | GPU-GPU 或 GPU-CPU 的高速互联 | 多 GPU 系统的全互联，实现 GPU 间直接通信 |
| 连接设备数量 | 理论无限制，但共享总带宽 | 通常最多 8 块 GPU | 支持 16 GPU 以上的全互联 |
| 单连接带宽（双向） | PCIe 4.0: 32GB/sPCIe 5.0: 64GB/s | NVLink 3.0 (A100): 600GB/sNVLink 4.0 (H100): 900GB/s | 7.2TB/s (H100 集群内部的总带宽) |
| 延迟 | 相对较高 (受制于主板总线和 CPU 调度) | 较低，GPU-GPU 间直接通信 | 更低，支持多 GPU 高效协作 |
| 拓扑结构 | 树形结构 | 点对点互联 (可链式或星型) | 多 GPU 全互联 |
| 适用范围 | 通用，支持所有扩展设备 | NVIDIA GPU 专用 | 高端 GPU 集群 (如 DGX 系列) |
| 成本 | 主板自带，无额外费用 | 需要 NVLink Bridge，成本较高 | 集成于 DGX 系列服务器，需配套硬件 |
| 典型应用场景 | 单机小规模 GPU 系统或通用计算 | 深度学习分布式训练、GPU 高效协作 | 超大规模分布式集群 (16+ GPU，全互联任务) |


# HBM
全称：高带宽内存（High Bandwidth Memory）

## GDDR6 和 GDDR6X 封装
封装位置：GDDR6和GDDR6X显存芯片通常被直接焊接在显卡的PCB（印刷电路板）上，围绕着GPU芯片。

封装方式：这些显存芯片通常采用平面封装（Planar Packaging），并以多个芯片的形式排列在GPU周围。

散热设计：由于显存芯片和GPU都在PCB上，显卡通常会使用散热片和风扇等散热解决方案来散热。

应用示例：

+ NVIDIA GeForce RTX 30系列显卡使用GDDR6X显存，显存芯片围绕在GPU周围排列
+ AMD Radeon RX 6000系列显卡使用GDDR6显存，封装方式类似

## HBM封装
封装位置：HBM显存与传统显存不同，它采用3D堆叠封装，并通过硅通孔（TSV）技术实现垂直连接。HBM显存芯片堆叠在一起，并与GPU一起封装在一个多芯片封装（MCP）中。

封装方式：HBM显存与GPU封装在同一个基板（Interposer）上，形成一个紧凑的整体。这种设计使得显存与GPU之间的信号路径更短，从而显著提高了带宽和降低了延迟。将多个 DDR 芯片堆叠之后与 GPU 芯片封装到一起，这样每片 GPU 和它自己的显存交互时，就不用再去 PCIe 交换芯片绕一圈，速度最高可以提升一个量级。

散热设计：由于HBM和GPU封装在一起，散热设计需要同时考虑两者的散热需求。通常会使用高效的散热方案，如液冷或高性能散热片。

应用示例：

+ AMD Radeon VII显卡使用HBM2显存，显存芯片与GPU一起封装在一个基板上
+ NVIDIA Tesla V100使用HBM2显存，采用类似的封装方式。

# 大模型精度
## 大模型精度是什么  
 在深度学习中，“精度”指的是模型在计算中表示数值的**位数和范围**，通常用浮点数格式表示（FP = Floating Point）。  
主要几种常见精度如下（以 IEEE 754 格式为例）：  

| 精度类型 | 位数 | 说明 | 优点 | 缺点 |
| --- | --- | --- | --- | --- |
| **FP64 (double)** | 64 位 | 1 位符号 + 11 位指数 + 52 位尾数 | 精度最高，科学计算用 | 速度慢、显存占用大 |
| **FP32 (single)** | 32 位 | 1 位符号 + 8 位指数 + 23 位尾数 | 精度较高，常用于训练 | 占显存大，计算慢于低精度 |
| **FP16 (half)** | 16 位 | 1 位符号 + 5 位指数 + 10 位尾数 | 显存占用小，速度快 | 精度损失，易出现溢出/下溢 |
| **BF16 (bfloat16)** | 16 位 | 1 位符号 + 8 位指数 + 7 位尾数 | 与 FP32 相同指数范围，精度低 | 速度快，易训练大模型 |
| **FP8** | 8 位 | 两种常见格式（E5M2 / E4M3） | 极小显存占用，速度极快 | 精度损失明显，需配合量化和算法优化 |


**关键点：**

+ 精度越高，表示范围和数值精确度越好，但计算更慢、显存占用更多。
+ 低精度计算（如 FP16、BF16、FP8）可以加快训练速度、降低显存压力，但需要一定的数值稳定性优化。

## 不同尺寸、精度大模型所需显存
| 精度 | 7B (GB) | 13B (GB) | 30B (GB) | 70B (GB) | 110B (GB) |
| --- | --- | --- | --- | --- | --- |
| FP16 | 12 | 24 | 60 | 120 | 200 |
| INT8 | 8 | 16 | 40 | 80 | 140 |
| INT4 | 6 | 12 | 24 | 48 | 72 |
| INT2 | 4 | 8 | 16 | 32 | 48 |


## 混合精度  
混合精度训练指在同一模型的训练过程中同时使用多种数值精度（通常是 FP16/BF16 和 FP32）来计算。  
其核心目标是在不影响模型最终精度的情况下，提升训练速度并减少显存占用。

典型的混合精度训练做法：

+ 权重存储

主权重（master weights）用 FP32 保存，以避免精度损失。

+ 前向传播和反向传播计算

大部分矩阵乘法、卷积运算用 FP16 / BF16 计算，以加快速度并节省显存。

+ 梯度缩放（Gradient Scaling）

由于 FP16 数值范围较小，为防止梯度下溢（underflow），会在反向传播前先乘一个缩放因子，再在更新前还原。

+ 损失函数计算

一般用 FP32 计算，保持数值稳定性。

## 混合精度的好处
+ 更快：FP16/BF16 运算在 GPU 张量核心（Tensor Core）上速度更快（尤其是 NVIDIA Volta、Ampere、Hopper 架构）。
+ 更省显存：同样显存下，可以训练更大的 batch size 或更大的模型。
+ 几乎无精度损失：合理使用梯度缩放等技术，最终模型精度和全 FP32 基本一致。

## 使用场景
+ 大模型训练（LLM、Transformer）：常用 BF16/FP16 混合精度。
+ 推理：甚至可以用 FP8 / INT8 混合精度（量化推理）。
+ 分布式训练：混合精度能减少通信带宽压力。


