# PG管理
# 查看 PG 信息
## 查看所有 PG 的状态
```bash
root@ceph-1:~# ceph pg stat
60 pgs: 60 active+clean; 449 KiB data, 207 MiB used, 150 GiB / 150 GiB avail
```

+ 显示 PG 总数及状态汇总（如 `active+clean`, `degraded`, `undersized` 等）。

## 列出详细 PG 状态
```bash
root@ceph-1:~# ceph pg dump
version 815
stamp 2024-12-02T13:00:51.549753+0000
last_osdmap_epoch 0
last_pg_scan 0
PG_STAT  OBJECTS  MISSING_ON_PRIMARY  DEGRADED  MISPLACED  UNFOUND  BYTES   OMAP_BYTES*  OMAP_KEYS*  LOG  LOG_DUPS  DISK_LOG  STATE                  STATE_STAMP                      VERSION  REPORTED  UP       UP_PRIMARY  ACTING   ACTING_PRIMARY  LAST_SCRUB  SCRUB_STAMP                      LAST_DEEP_SCRUB  DEEP_SCRUB_STAMP                 SNAPTRIMQ_LEN  LAST_SCRUB_DURATION  SCRUB_SCHEDULING                                            OBJECTS_SCRUBBED  OBJECTS_TRIMMED
2.34           0                   0         0          0        0       0            0           0    0         0         0           active+clean  2024-12-02T12:55:42.087227+0000      0'0   374:374  [1,0,2]           1  [1,0,2]               1         0'0  2024-12-02T12:53:42.934232+0000              0'0  2024-12-02T12:53:42.934232+0000              0                 
   0  periodic scrub scheduled @ 2024-12-03T18:21:14.778403+0000                 0                0
2.33           0                   0         0          0        0       0            0           0    0         0         0           active+clean  2024-12-02T12:55:46.130328+0000      0'0   374:374  [1,0,2]           1  [1,0,2]               1         0'0  2024-12-02T12:53:42.934232+0000              0'0  2024-12-02T12:53:42.934232+0000              0                 
   0  periodic scrub scheduled @ 2024-12-03T13:30:56.539579+0000                 0                0
2.32           0                   0         0          0        0       0            0           0    0         0         0           active+clean  2024-12-02T12:55:50.160867+0000      0'0   377:378  [2,1,0]           2  [2,1,0]               2         0'0  2024-12-02T12:53:42.934232+0000              0'0  2024-12-02T12:53:42.934232+0000              0                 
   0  periodic scrub scheduled @ 2024-12-03T17:36:25.981017+0000                 0                0
2.31           0                   0         0          0        0       0            0           0    0         0         0           active+clean  2024-12-02T12:55:54.243631+0000      0'0   375:375  [0,2,1]           0  [0,2,1]               0         0'0  2024-12-02T12:53:42.934232+0000              0'0  2024-12-02T12:53:42.934232+0000              0                 
   0  periodic scrub scheduled @ 2024-12-03T19:26:23.658732+0000                 0                0
2.30           0                   0         0          0        0       0            0           0    0         0         0           active+clean  2024-12-02T12:55:58.229218+0000      0'0   377:378  [2,1,0]           2  [2,1,0]               2         0'0  2024-12-02T12:53:42.934232+0000              0'0  2024-12-02T12:53:42.934232+0000              0                 
```

+ 输出详细的 PG 分布、状态和 OSD 信息。

## 列出特定状态的 PG
```plain
ceph pg ls <状态>
```

+ 列出指定状态的 PG（如 `active+clean`、`degraded`）。
+ 示例：

```plain
ceph pg ls degraded
```

## 显示特定 PG 的详细信息
```plain
ceph pg <PG_ID> query
```

+ 获取某个 PG 的详细信息，包括副本位置、对象分布等。
+ 示例：

```json
root@ceph-1:~# ceph pg 2.13 query
{
  "snap_trimq": "[]",
  "snap_trimq_len": 0,
  "state": "active+clean",
  "epoch": 481,
  "up": [
    2,
    1,
    0
  ]
  ……
```

# 调整 PG 数量
## PG 数计算
参考文档：[https://docs.ceph.com/en/quincy/rados/operations/placement-groups/#choosing-the-number-of-pgs](https://docs.ceph.com/en/quincy/rados/operations/placement-groups/#choosing-the-number-of-pgs)

如果您的 OSD 超过 50 个，我们建议每个 OSD 使用大约 50-100 个 PG，以平衡资源使用、数据持久性和数据分布。 如果您的 OSD 少于 50 个，请遵循预选部分中的指导。 对于单个池，使用以下公式获取基线值：

PG 总数 = （osd数 * 100）/ pool-size

这里的池大小要么是复制池的副本数量，要么是纠删码池的 K+M 总和。 要检索此总和，请运行命令

ceph osderasure-code-profile get

## 设置池的 PG 数量
```plain
ceph osd pool set <pool_name> pg_num <value>
```

## 重新平衡 PG
+ 手动触发 PG 重新平衡

```plain
ceph osd reweight-by-utilization
```

+ 根据 OSD 的使用情况调整权重，改善数据分布。
+ 设置集群平衡速率

```plain
ceph tell osd.* injectargs '--osd_max_backfills <value>'
```

+ 调整单个 OSD 的最大回填任务数量（默认 1~2）。

示例：

```plain
bash


复制代码
ceph tell osd.* injectargs '--osd_max_backfills 4'
```

+ 设置 PG 回填速率

```plain
ceph config set osd osd_recovery_max_active <value>
```

+ 调整回填任务的最大并发数。

```plain
ceph config set osd osd_recovery_max_active 4
```

# PG 数据分布和监控
## 检查数据分布情况
```plain
bash


复制代码
ceph osd df
```

+ 查看每个 OSD 的存储容量和使用率。

## 获取集群的 CRUSH 映射
```plain
bash


复制代码
ceph osd getcrushmap -o crushmap.bin
crushtool -d crushmap.bin -o crushmap.txt
```

# 统计 PG 数据
## 显示特定 PG 的对象数量
```plain
ceph pg <PG_ID> list_objects
```

+ 列出 PG 中的对象及其详细信息。

```plain
ceph pg 1.3 list_objects
```

# PG 修复
1. 查看 PG 状态：

```bash
ceph pg stat
```

这个命令可以显示 Ceph 集群中所有 PG 的状态。

2. 查看特定 PG 的详细信息：

```plain
ceph pg <pg_id> query
```

用于查询某个 PG 的详细信息，`<pg_id>` 是具体的 PG ID。

3. 强制重建 PG： 如果某个 PG 长时间没有修复，可以尝试强制进行 PG 修复：

```plain
ceph pg repair <pg_id>
```

这个命令会触发 PG 的修复过程，通常用于当 PG 出现不一致或丢失副本时。

4. 启动集群的 PG 修复： 如果需要修复整个集群的所有 PG，可以使用：

```plain
ceph pg repair
```

5. 检查集群中的 PG 错误： 如果出现错误的 PG，可能需要检查其状态：

```plain
ceph health detail
```

这个命令显示更详细的集群健康信息，并指出哪些 PG 可能处于不健康状态。

6. 查看和清理 Orphaned PGs： Orphaned PGs 是指没有父对象或已丢失的 PG。如果发现这类 PG，可以尝试清理：

```plain
ceph pg scrub <pg_id>
```

用于清理某个 PG。

7. 清理整个集群中的垃圾数据： 如果某些 PG 数据已经腐败或过时，可以使用：

```plain
ceph osd scrub <osd_id>
```

8. 查看 PG 的迁移情况：

```plain
ceph pg dump
```

显示所有 PG 相关的详细信息，包括迁移和修复状态。

9. 停止 PG 修复： 如果需要停止 PG 修复过程（例如，暂时避免过度负载），可以使用：

```plain
ceph osd noout
```

# PG 与 PGP 区别
+ PG 是指定存储池存储对象的目录有多少个，PGP 是存储池 PG 的 OSD 分布组合个数。
+ PG 的增加会引起 PG 内的数据进行分裂，分裂到相同的 OSD 上新生成的 PG 当中。
+ PGP 的增加会引起部分 PG 的分布进行变化，但是不会引起 PG 内对象的变动。

# 

