# 特定OSD上创建存储池
Ceph集群通常由具有多个磁盘驱动器的多个节点组成。并且，这些磁盘驱动器可以是混合类型。您的Ceph节点可能包含SATA，NL-SAS，SAS，SSD甚至PCIe等类型的磁盘，依此类推。Ceph 为您提供了创建池的灵活性在特定的驱动器类型。

例如，我们可以从一组SSD磁盘创建高性能SSD池， 也可以使用SATA磁盘驱动器创建高容量，低成本的池。  

![img_560.png](https://raw.githubusercontent.com/Oumu33/notes/main/notes/images/img_560.png)

# 实验介绍
创建一个名为 ssd-pool SSD磁盘支持的池，以及另一个名 为 sata-pool SATA的池，该池由SATA磁盘支持。

假设 osd.0、osd.1、osd.2 是 SSD 磁盘，osd.3、osd.4、osd.5 是 HDD 磁盘  

[https://blog.csdn.net/li4528503/article/details/106256466](https://blog.csdn.net/li4528503/article/details/106256466)

# 修改调整 OSD
## 查看当前osd分布情况
```bash
root@ceph-1:~# ceph osd df
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP    META     AVAIL    %USE  VAR   PGS  STATUS
 0    hdd  0.04880   1.00000   50 GiB   42 MiB  6.6 MiB  19 KiB   35 MiB   50 GiB  0.08  1.51   18      up
 3    hdd  0.09769   1.00000  100 GiB   44 MiB   17 MiB   5 KiB   27 MiB  100 GiB  0.04  0.80   47      up
 1    hdd  0.04880   1.00000   50 GiB   44 MiB   13 MiB  16 KiB   31 MiB   50 GiB  0.09  1.58   22      up
 4    hdd  0.09769   1.00000  100 GiB   38 MiB   11 MiB   6 KiB   27 MiB  100 GiB  0.04  0.68   43      up
 2    hdd  0.04880   1.00000   50 GiB   38 MiB  6.5 MiB  18 KiB   31 MiB   50 GiB  0.07  1.36   26      up
 5    hdd  0.09769   1.00000  100 GiB   44 MiB   17 MiB   6 KiB   27 MiB  100 GiB  0.04  0.80   39      up
                       TOTAL  450 GiB  249 MiB   71 MiB  73 KiB  177 MiB  450 GiB  0.05
MIN/MAX VAR: 0.68/1.58  STDDEV: 0.02
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.43945  root default
-3         0.14648      host ceph-1
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
 3    hdd  0.09769          osd.3        up   1.00000  1.00000
-5         0.14648      host ceph-2
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
 4    hdd  0.09769          osd.4        up   1.00000  1.00000
-7         0.14648      host ceph-3
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
 5    hdd  0.09769          osd.5        up   1.00000  1.00000
```

## 查看当前集群的crush class
```bash
root@ceph-1:~# ceph osd crush class ls
[
    "hdd"
]
```

## 删除 class
> 删除osd.0，osd.1，osd.2 的class
>

```bash
root@ceph-1:~# ceph osd crush rm-device-class osd.0
done removing class of osd(s): 0
root@ceph-1:~# ceph osd crush rm-device-class osd.1
done removing class of osd(s): 1
root@ceph-1:~# ceph osd crush rm-device-class osd.2
done removing class of osd(s): 2
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.43945  root default
-3         0.14648      host ceph-1
 0         0.04880          osd.0        up   1.00000  1.00000
 3    hdd  0.09769          osd.3        up   1.00000  1.00000
-5         0.14648      host ceph-2
 1         0.04880          osd.1        up   1.00000  1.00000
 4    hdd  0.09769          osd.4        up   1.00000  1.00000
-7         0.14648      host ceph-3
 2         0.04880          osd.2        up   1.00000  1.00000
 5    hdd  0.09769          osd.5        up   1.00000  1.00000
```

## 设置 class
> 设置 osd.0，osd.1，osd.2 的class为ssd
>

```bash
root@ceph-1:~# ceph osd crush set-device-class ssd osd.0
set osd(s) 0 to class 'ssd'
root@ceph-1:~# ceph osd crush set-device-class ssd osd.1
set osd(s) 1 to class 'ssd'
root@ceph-1:~# ceph osd crush set-device-class ssd osd.2
set osd(s) 2 to class 'ssd'
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.43945  root default
-3         0.14648      host ceph-1
 3    hdd  0.09769          osd.3        up   1.00000  1.00000
 0    ssd  0.04880          osd.0        up   1.00000  1.00000
-5         0.14648      host ceph-2
 4    hdd  0.09769          osd.4        up   1.00000  1.00000
 1    ssd  0.04880          osd.1        up   1.00000  1.00000
-7         0.14648      host ceph-3
 5    hdd  0.09769          osd.5        up   1.00000  1.00000
 2    ssd  0.04880          osd.2        up   1.00000  1.00000
```

## 再次查看crush class
```bash
root@ceph-1:~# ceph osd crush class ls
[
    "hdd",
    "ssd"
]
```

# 创建 Crush 规则
为 SSD 和 HDD 分别创建自定义 CRUSH 规则。

## 查看当前 CRUSH 规则
```bash
root@ceph-1:~# ceph osd crush rule ls
replicated_rule
```

## 创建CRUSH 规则
+ 创建 SSD 的 crush 规则

```bash
root@ceph-1:~# ceph osd crush rule create-replicated rule_ssd default host ssd 
```

+ 创建 HDD 的 CRUSH 规则

```bash
root@ceph-1:~# ceph osd crush rule create-replicated rule_hdd default host hdd 
```

## 再次查看当前 CRUSH 规则
```bash
root@ceph-1:~# ceph osd crush rule ls
replicated_rule
rule_ssd
rule_hdd
```

# 创建存储池
## 创建 SSD 存储池
<font style="color:rgb(77, 77, 77);">创建一个使用rule_ssd规则的存储池</font>

```bash
root@ceph-1:~# ceph osd pool create ssd_pool 16 16 rule_ssd
pool 'ssd_pool' created
```

# 验证配置
## 验证池的规则
```bash
root@ceph-1:~# ceph osd pool get ssd_pool crush_rule
crush_rule: rule_ssd
```

## 检查存储池状态
```bash
root@ceph-1:~# ceph df
--- RAW STORAGE ---
CLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED
hdd    300 GiB  300 GiB  229 MiB   229 MiB       0.07
ssd    150 GiB  150 GiB  117 MiB   117 MiB       0.08
TOTAL  450 GiB  450 GiB  346 MiB   346 MiB       0.08
--- POOLS ---
POOL      ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr       1    1  748 KiB        2  2.2 MiB      0    142 GiB
mypool     2   32      0 B        0      0 B      0    142 GiB
rbd-data   6   32   10 MiB       19   31 MiB      0    142 GiB
ssd-pool   7   16      0 B        0      0 B      0     47 GiB
hdd-pool   8   16      0 B        0      0 B      0     95 GiB
```

## 上传数据测试
 为存储池启用应用程序  

```bash
root@ceph-1:~# ceph osd pool application enable ssd_pool rbd
enabled application 'rbd' on pool 'ssd_pool'
```

初始化存储池

```bash
root@ceph-1:~# rbd pool init -p ssd_pool
```

上传测试文件

```plain
root@ceph-1:~# rados -p ssd_pool put test.txt test.txt
```

查询对象的OSD组

```plain
root@ceph-1:~# ceph osd map ssd_pool test.txt
osdmap e771 pool 'ssd_pool' (12) object 'test.txt' -> pg 12.8b0b6108 (12.8) -> up ([0,1,2], p0) acting ([0,1,2], p0)
```

从查询结果可以看出，对象所在的OSD，是class为ssd的OSD 的 0 1 2 上面

# 删除 Crush 规则
## 列出当前的 CRUSH 规则
首先列出所有 CRUSH 规则，找到需要删除的规则名称：

```bash
root@ceph-1:~# ceph osd crush rule ls
replicated_rule
rule_ssd
rule_hdd
```

## 检查是否有池正在使用该规则
确保没有存储池在使用目标规则。如果有存储池正在使用，需要先更改存储池的规则或删除存储池：

1. 列出所有存储池及其 CRUSH 规则：

```plain
ceph osd pool ls detail
```

2. 如果某存储池正在使用目标规则，可以更改为默认规则或其他合适的规则：

```plain
ceph osd pool set <pool-name> crush_rule <new-crush-rule>
```

3. 如果确认该规则不再使用，继续删除。

## 删除 CRUSH 规则
使用以下命令删除指定的 CRUSH 规则：

```bash
root@ceph-1:~# ceph osd crush rule rm rule_ssd
```

## 验证 CRUSH 规则是否删除成功
再次列出 CRUSH 规则，确保规则已被删除：

```plain
ceph osd crush rule ls
```


