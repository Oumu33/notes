# Rook快速使用
> 官方文档链接： [https://rook.github.io/docs/rook/latest-release/Getting-Started/quickstart/](https://rook.github.io/docs/rook/latest-release/Getting-Started/quickstart/)，本篇主要是带领大家快速体验使用 rook，默认情况下会扫描所有 k8s 节点，只有存在未被格式化的磁盘均会加入 ceph 集群 osd 资源，如果需要自定义角色，可参考下一篇定制 rook 集群。
>

# 准备工作
## 集群规划
| 主机名 | <font style="color:#404040;">IP</font> | 磁数据盘空间 | 节点标签 |
| --- | --- | --- | --- |
| master1 | <font style="color:#404040;">192.168.10.151</font> | nvme0n2 100G 不格式化 用于 osd | storage:true |
| master2 | 192.168.10.152 | nvme0n2 100G 不格式化 用于 osd | storage:true |
| master3 | 192.168.10.153 | nvme0n2 100G 不格式化 用于 osd | storage:true |
| work1 | 192.168.10.154 | nvme0n2 100G不格式化 测试用 |  |
| work2 | 192.168.10.155 | nvme0n2 100G不格式化 测试用 |  |
| work3 | 192.168.10.156 | nvme0n2 100G 不格式化 测试用 |  |


## 其他条件
1. 支持 Kubernetes 版本 v1.26+。
2. k8s集群至少3节点。
3. ceph节点需要有一块超5G且未被格式化的磁盘。
4. ceph节点安装 lvm2工具包

# 安装operator
```bash
# 获取资源清单
git clone --single-branch --branch v1.16.0 https://github.com/rook/rook.git
# 安装operator
cd rook/deploy/examples 
# 创建资源
kubectl create -f common.yaml -f crds.yaml -f operator.yaml
```

查看资源信息

```bash
# kubectl get pod -n rook-ceph -o wide
NAME                                  READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES
rook-ceph-operator-7dfdf865d5-wbqds   1/1     Running   0          10s   10.244.5.64   work3   <none>           <none>
```

# 初始化ceph集群
## 获取配置文件
访问[https://github.com/rook/rook/blob/release-1.16/deploy/examples/cluster.yaml](https://github.com/rook/rook/blob/release-1.16/deploy/examples/cluster.yaml)，查看示例配置文件

## 创建ceph集群
```bash
# kubectl apply -f cluster.yaml                                                                                                                                                       
cephcluster.ceph.rook.io/rook-ceph created
```

## 查看资源
```bash
# kubectl get pod -n rook-ceph -o wide
NAME                                                READY   STATUS      RESTARTS        AGE     IP               NODE      NOMINATED NODE   READINESS GATES
csi-cephfsplugin-4kzkp                              3/3     Running     1 (4m18s ago)   4m51s   192.168.10.156   work3     <none>           <none>
csi-cephfsplugin-7jqx8                              3/3     Running     1 (4m19s ago)   4m51s   192.168.10.152   master2   <none>           <none>
csi-cephfsplugin-cpwgf                              3/3     Running     1 (4m19s ago)   4m51s   192.168.10.151   master1   <none>           <none>
csi-cephfsplugin-jt4ns                              3/3     Running     2 (3m28s ago)   4m51s   192.168.10.154   work1     <none>           <none>
csi-cephfsplugin-m4bww                              3/3     Running     1 (4m19s ago)   4m51s   192.168.10.155   work2     <none>           <none>
csi-cephfsplugin-provisioner-6d866886bf-bsfjx       6/6     Running     3 (3m10s ago)   4m51s   10.244.0.39      master1   <none>           <none>
csi-cephfsplugin-provisioner-6d866886bf-qk9wx       6/6     Running     1 (4m17s ago)   4m51s   10.244.2.34      master3   <none>           <none>
csi-cephfsplugin-z79z5                              3/3     Running     1 (4m17s ago)   4m51s   192.168.10.153   master3   <none>           <none>
csi-rbdplugin-5btjq                                 3/3     Running     1 (4m19s ago)   4m51s   192.168.10.153   master3   <none>           <none>
csi-rbdplugin-5hqck                                 3/3     Running     1 (4m18s ago)   4m51s   192.168.10.156   work3     <none>           <none>
csi-rbdplugin-6p87v                                 3/3     Running     1 (4m19s ago)   4m51s   192.168.10.155   work2     <none>           <none>
csi-rbdplugin-969vq                                 3/3     Running     2 (3m30s ago)   4m51s   192.168.10.154   work1     <none>           <none>
csi-rbdplugin-jlm7q                                 3/3     Running     1 (4m19s ago)   4m51s   192.168.10.151   master1   <none>           <none>
csi-rbdplugin-provisioner-5f87858d77-7bjnb          6/6     Running     1 (4m16s ago)   4m51s   10.244.3.97      work1     <none>           <none>
csi-rbdplugin-provisioner-5f87858d77-zws4v          6/6     Running     1 (4m15s ago)   4m51s   10.244.5.66      work3     <none>           <none>
csi-rbdplugin-sgjv2                                 3/3     Running     1 (4m19s ago)   4m51s   192.168.10.152   master2   <none>           <none>
rook-ceph-crashcollector-master1-6d4fddb76b-8nf2z   1/1     Running     0               63s     10.244.0.42      master1   <none>           <none>
rook-ceph-crashcollector-master2-649885f4dc-jghlj   1/1     Running     0               2m18s   10.244.1.28      master2   <none>           <none>
rook-ceph-crashcollector-master3-74c7d76644-l6dtl   1/1     Running     0               68s     10.244.2.36      master3   <none>           <none>
rook-ceph-crashcollector-work1-854c978f55-lk27s     1/1     Running     0               68s     10.244.3.100     work1     <none>           <none>
rook-ceph-crashcollector-work2-94f9b8c8-c7fd8       1/1     Running     0               2m9s    10.244.4.114     work2     <none>           <none>
rook-ceph-crashcollector-work3-58cd6c5877-hk2d8     1/1     Running     0               2m10s   10.244.5.74      work3     <none>           <none>
rook-ceph-exporter-master1-66657559dd-7bk2l         1/1     Running     0               63s     10.244.0.43      master1   <none>           <none>
rook-ceph-exporter-master2-75bbbf5dd5-ljgt8         1/1     Running     0               2m18s   10.244.1.29      master2   <none>           <none>
rook-ceph-exporter-master3-559ff777f8-drjhd         1/1     Running     0               68s     10.244.2.37      master3   <none>           <none>
rook-ceph-exporter-work1-7bb77577c7-wjcdn           1/1     Running     0               68s     10.244.3.101     work1     <none>           <none>
rook-ceph-exporter-work2-77697bf66d-m772b           1/1     Running     0               2m6s    10.244.4.115     work2     <none>           <none>
rook-ceph-exporter-work3-6f58856cb7-q7vkf           1/1     Running     0               2m6s    10.244.5.75      work3     <none>           <none>
rook-ceph-mgr-a-7f75f6b64d-jtx2b                    3/3     Running     0               2m38s   10.244.4.109     work2     <none>           <none>
rook-ceph-mgr-b-665d9bdf49-2nbsv                    3/3     Running     0               2m37s   10.244.5.69      work3     <none>           <none>
rook-ceph-mon-a-d7cd4ffbf-2hg6v                     2/2     Running     0               3m54s   10.244.5.68      work3     <none>           <none>
rook-ceph-mon-b-75b869fdc7-ddmgp                    2/2     Running     0               2m57s   10.244.4.108     work2     <none>           <none>
rook-ceph-mon-c-5fc9775b49-r48wt                    2/2     Running     0               2m48s   10.244.1.27      master2   <none>           <none>
rook-ceph-operator-7dfdf865d5-wbqds                 1/1     Running     0               5m27s   10.244.5.64      work3     <none>           <none>
rook-ceph-osd-0-7db6595784-9qvbn                    2/2     Running     0               2m10s   10.244.5.73      work3     <none>           <none>
rook-ceph-osd-1-7db7745955-wsc76                    2/2     Running     0               2m9s    10.244.4.113     work2     <none>           <none>
rook-ceph-osd-2-5875f9458b-qntrq                    2/2     Running     0               68s     10.244.1.31      master2   <none>           <none>
rook-ceph-osd-3-6644f49796-dwt2m                    2/2     Running     0               68s     10.244.3.99      work1     <none>           <none>
rook-ceph-osd-4-68b45bf69c-g6rd7                    2/2     Running     0               68s     10.244.2.38      master3   <none>           <none>
rook-ceph-osd-5-bfcdd7774-5pqmb                     2/2     Running     0               63s     10.244.0.41      master1   <none>           <none>
rook-ceph-osd-prepare-master1-qrv66                 0/1     Completed   0               2m14s   10.244.0.40      master1   <none>           <none>
rook-ceph-osd-prepare-master2-8jk4k                 0/1     Completed   0               2m13s   10.244.1.30      master2   <none>           <none>
rook-ceph-osd-prepare-master3-xbm7h                 0/1     Completed   1               2m13s   10.244.2.35      master3   <none>           <none>
rook-ceph-osd-prepare-work1-l99dv                   0/1     Completed   0               2m12s   10.244.3.98      work1     <none>           <none>
rook-ceph-osd-prepare-work2-sp88c                   0/1     Completed   0               2m16s   10.244.4.112     work2     <none>           <none>
rook-ceph-osd-prepare-work3-j4pf4                   0/1     Completed   0               2m15s   10.244.5.72      work3     <none>           <none>
# kubectl get svc -n rook-ceph
NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
rook-ceph-exporter        ClusterIP   10.111.149.143   <none>        9926/TCP            3m1s
rook-ceph-mgr             ClusterIP   10.103.249.7     <none>        9283/TCP            2m59s
rook-ceph-mgr-dashboard   ClusterIP   10.105.163.101   <none>        8443/TCP            2m59s
rook-ceph-mon-a           ClusterIP   10.98.206.13     <none>        6789/TCP,3300/TCP   4m19s
rook-ceph-mon-b           ClusterIP   10.97.34.37      <none>        6789/TCP,3300/TCP   3m20s
rook-ceph-mon-c           ClusterIP   10.101.250.158   <none>        6789/TCP,3300/TCP   3m12s
```

如果出现mgr和osd未启动，执行以下命令清理，其中/dev/sdb是磁盘名称。

```bash
# yum -y install gdisk lvm2
# wipefs -a /dev/sdb && sgdisk --zap-all /dev/sdb && dd if=/dev/zero of=/dev/sdb bs=1M count=1
# rm -rvf /var/lib/rook/* /dev/bdb* /dev/nbd* /dev/ceph-*
```

# 客户端访问
## 部署客户端连接工具
资源清单参考文件：[https://github.com/rook/rook/blob/release-1.16/deploy/examples/toolbox.yaml](https://github.com/rook/rook/blob/release-1.16/deploy/examples/toolbox.yaml)

```bash
# kubectl apply -f toolbox.yaml          
deployment.apps/rook-ceph-tools created
# kubectl get pod -n rook-ceph | grep tools
rook-ceph-tools-767b99dbdd-jhlkf                1/1     Running   0          21s
# kubectl exec -it -n rook-ceph rook-ceph-tools-767b99dbdd-2wzzc -- bash
bash-5.1$ ceph -s
  cluster:
    id:     59348b06-014a-40e7-828e-06f5fca811b6
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum a,b,c (age 4m)
    mgr: a(active, since 3m), standbys: b
    osd: 6 osds: 6 up (since 118s), 6 in (since 2m)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 2 objects, 449 KiB
    usage:   161 MiB used, 600 GiB / 600 GiB avail
    pgs:     1 active+clean
 
bash-5.1$ ceph osd status
ID  HOST      USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE      
 0  work3    26.6M  99.9G      0        0       0        0   exists,up  
 1  work2    27.0M  99.9G      0        0       0        0   exists,up  
 2  master2  26.6M  99.9G      0        0       0        0   exists,up  
 3  work1    27.0M  99.9G      0        0       0        0   exists,up  
 4  master3  27.0M  99.9G      0        0       0        0   exists,up  
 5  master1  26.6M  99.9G      0        0       0        0   exists,up 
```

可以看到，默认情况下会扫描所有 k8s 节点，只有存在未被格式化的磁盘均会加入 ceph 集群 osd 资源。

## 访问dashboard
查看svc信息

```bash
# kubectl get svc -n rook-ceph | grep dashboard
rook-ceph-mgr-dashboard   ClusterIP   10.105.79.152    <none>        8443/TCP            7m19s
```

创建ingress

```yaml
apiVersion: traefik.io/v1alpha1
kind: ServersTransport
metadata:
  name: ceph-transport
  namespace: rook-ceph
spec:
  serverName: "ceph.local.com"
  insecureSkipVerify: true
---
apiVersion: traefik.io/v1alpha1
kind: IngressRoute
metadata:
  name: dashboard
  namespace: rook-ceph
spec:
  entryPoints:
    - websecure
  routes:
  - match: Host(`ceph.local.com`) # 域名
    kind: Rule
    services:
      - name: rook-ceph-mgr-dashboard  # 与svc的name一致
        port: 8443           # 与svc的port一致
        serversTransport: ceph-transport
```

添加域名后访问测试

![](https://via.placeholder.com/800x600?text=Image+c11a4ea1c88d427c)

获取admin用户默认密码

```bash
# kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
bQTh<J:B[.,riC,NnSN=
```

## 密码重置
```bash
# kubectl exec -it -n rook-ceph rook-ceph-tools-7b9c675947-pzfl8 -- bash                                                     
bash-5.1$ echo "1234qwer" > /tmp/password.txt
bash-5.1$ ceph dashboard ac-user-set-password admin -i /tmp/password.txt 
{"username": "admin", "password": "$2b$12$78W1IMpXKJdFp058K4dVouZsWPe9huEkTYjyz6YgSV.jVvKEVNXre", "roles": ["administrator"], "name": null, "email": null, "lastUpdate": 1733797087, "enabled": false, "pwdExpirationDate": null, "pwdUpdateRequired": false}
```

# 卸载 Rook
> 参考文档：[https://www.rook.io/docs/rook/latest-release/Getting-Started/ceph-teardown/](https://www.rook.io/docs/rook/latest-release/Getting-Started/ceph-teardown/)
>

1. 删除使用了 ceph 存储的有状态应用
2. 删除 pvc、pv
3. 删除 pool

```bash
# kubectl get cephblockpool -A 
NAMESPACE   NAME      PHASE   TYPE         FAILUREDOMAIN   AGE
rook-ceph   k8s-rbd   Ready   Replicated   host            128d
# kubectl delete cephblockpool -n rook-ceph k8s-rbd --force --grace-period=0
```

4. 删除 storageclass

```bash
# kubectl get sc
NAME            PROVISIONER                                         RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
ceph-fs         rook-ceph.cephfs.csi.ceph.com                       Delete          Immediate              true                   128d
ceph-rbd        rook-ceph.rbd.csi.ceph.com                          Delete          Immediate              true                   128d
ceph-rgw        rook-ceph.ceph.rook.io/bucket                       Delete          Immediate              false
# kubectl delete sc ceph-fs ceph-rbd ceph-rgw 
```

5. 删除 rook

```bash
# kubectl delete -f cluster.yaml
# kubectl delete -f operator.yaml
# kubectl delete -f common.yaml
# kubectl delete -f crds.yaml
```

5. 删除 rook 数据目录

```bash
# for i in {1..5};do ssh node-${i} "rm -rf /var/lib/rook/";done
```

6. 删除节点 vg 和 pv

```bash
# vgremove ceph-1a17705e-abd6-4b9d-8eee-d0f57a62801a --yes #通过
vgs可以查看
# pvremove /dev/vdb
```

7. 清理残留数据

```bash
# yum -y install gdisk lvm2
# wipefs -a /dev/sdb && sgdisk --zap-all /dev/sdb && dd if=/dev/zero of=/dev/sdb bs=1M count=1
# rm -rvf /var/lib/rook/* /dev/bdb* /dev/nbd* /dev/ceph-*
```

8. 删除 crd 资源

```bash
# kubectl get crd | grep ceph                         
cephclusters.ceph.rook.io                                  2024-12-26T06:19:11Z
cephfilesystems.ceph.rook.io                               2024-12-26T06:19:11Z
cephobjectstores.ceph.rook.io                              2024-12-26T06:19:12Z

# kubectl patch crd cephclusters.ceph.rook.io -p '{"metadata":{"finalizers":[]}}' --type=merge
# kubectl patch crd cephfilesystems.ceph.rook.io -p '{"metadata":{"finalizers":[]}}' --type=merge
# kubectl patch crd cephobjectstores.ceph.rook.io -p '{"metadata":{"finalizers":[]}}' --type=merge

# kubectl delete crd cephclusters.ceph.rook.io
# kubectl delete crd cephfilesystems.ceph.rook.io
# kubectl delete crd cephobjectstores.ceph.rook.io

# kubectl get crd | grep objectbucket
objectbucketclaims.objectbucket.io                         2024-12-30T11:07:19Z
objectbuckets.objectbucket.io                              2024-12-30T11:07:19Z

# kubectl patch crd objectbucketclaims.objectbucket.io -p '{"metadata":{"finalizers":[]}}' --type=merge
# kubectl patch crd objectbuckets.objectbucket.io -p '{"metadata":{"finalizers":[]}}' --type=merge

# kubectl delete crd objectbucketclaims.objectbucket.io --grace-period=0 --force
# kubectl delete crd objectbuckets.objectbucket.io --grace-period=0 --force
```

9. 删除名称空间

```bash
# kubectl delete ns rook-ceph
```


