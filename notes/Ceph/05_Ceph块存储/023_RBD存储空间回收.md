# RBD存储空间回收
删除完成的数据只是标记为已经被删除，但是不会从块存储立即清空

# 挂载 RBD
参考上篇文档：[https://www.cuiliangblog.cn/detail/section/196656143](https://www.cuiliangblog.cn/detail/section/196656143)

挂载后的效果如下：

```bash
root@ceph-client:~# rbd showmapped
id  pool      namespace  image      snap  device   
0   rbd-data             data-img1  -     /dev/rbd0
root@ceph-client:~# df -h | grep rbd
/dev/rbd0                          3.0G   54M  3.0G   2% /data
```

# 数据测试
## 集群状态
```bash
root@ceph-client:~# ceph --id cuiliang df
--- RAW STORAGE ---
CLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED
hdd    150 GiB  150 GiB  204 MiB   204 MiB       0.13
TOTAL  150 GiB  150 GiB  204 MiB   204 MiB       0.13
 
--- POOLS ---
POOL      ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr       1    1  449 KiB        2  1.3 MiB      0     47 GiB
mypool     2   32      0 B        0      0 B      0     47 GiB
rbd-data   4   32   10 MiB       20   31 MiB   0.02     47 GiB
```

## 创建数据
```bash
# 创建1G文件
root@ceph-client:~# dd if=/dev/zero of=/data/ceph-test-file bs=1G count=1
1+0 records in
1+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 3.77068 s, 285 MB/s
```

## 集群状态验证
```bash
root@ceph-client:~# ceph --id cuiliang df
--- RAW STORAGE ---
CLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED
hdd    150 GiB  147 GiB  3.3 GiB   3.3 GiB       2.18
TOTAL  150 GiB  147 GiB  3.3 GiB   3.3 GiB       2.18
 
--- POOLS ---
POOL      ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr       1    1  449 KiB        2  1.3 MiB      0     46 GiB
mypool     2   32      0 B        0      0 B      0     46 GiB
rbd-data   4   32  1.0 GiB      273  3.0 GiB   2.13     46 GiB # 已经使用1G
```

## 删除数据
```bash
root@ceph-client:~# rm -rf /data/ceph-test-file
```

删除完成的数据只是标记为已经被删除，但是不会从块存储立即清空，因此在删除完成后使用ceph df 查看并没有回收空间

```bash
root@ceph-client:~# df -h
Filesystem                         Size  Used Avail Use% Mounted on
udev                               1.9G     0  1.9G   0% /dev
tmpfs                              389M  1.5M  388M   1% /run
/dev/mapper/ubuntu--vg-ubuntu--lv   48G  7.6G   38G  17% /
tmpfs                              1.9G     0  1.9G   0% /dev/shm
tmpfs                              5.0M     0  5.0M   0% /run/lock
tmpfs                              1.9G     0  1.9G   0% /sys/fs/cgroup
/dev/loop0                          92M   92M     0 100% /snap/lxd/29619
/dev/sda2                          2.0G  109M  1.7G   6% /boot
/dev/loop2                          64M   64M     0 100% /snap/core20/1828
/dev/loop1                          92M   92M     0 100% /snap/lxd/24061
/dev/loop3                          45M   45M     0 100% /snap/snapd/23258
/dev/loop4                          64M   64M     0 100% /snap/core20/2434
/dev/loop5                          50M   50M     0 100% /snap/snapd/18357
tmpfs                              389M     0  389M   0% /run/user/0
/dev/rbd0                          3.0G   54M  3.0G   2% /data
```

# 空间回收
`fstrim` 是用于对支持 TRIM 操作的文件系统执行空间回收的命令。它将未使用的文件系统空间标记为可供底层存储设备（如 SSD 或 Ceph RBD）重用，从而释放空间或优化存储性能，有以下几个注意事项：

+ **性能考虑：**

对于 SSD 或 Ceph RBD 镜像，TRIM 操作可能需要额外的 IO，建议在业务低峰期执行。

+ **文件系统支持：**

常见文件系统如 `ext4`、`xfs` 和 `btrfs` 支持 TRIM。

+ **TRIM 不等于删除数据：**

TRIM 只是告诉底层存储设备这些空间可以重用，不会安全擦除数据。

## 手动立即回收空间
```bash
root@ceph-client:~# fstrim -v /data
/data: 3 GiB (3210330112 bytes) trimmed
root@ceph-client:~# ceph --id cuiliang df
--- RAW STORAGE ---
CLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED
hdd    150 GiB  150 GiB  269 MiB   269 MiB       0.18
TOTAL  150 GiB  150 GiB  269 MiB   269 MiB       0.18
 
--- POOLS ---
POOL      ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr       1    1  449 KiB        2  1.3 MiB      0     47 GiB
mypool     2   32      0 B        0      0 B      0     47 GiB
rbd-data   4   32   10 MiB       77   31 MiB   0.02     47 GiB
```

## 定时自动回收空间
1. 检查是否已启用 `fstrim.timer`：

```bash
systemctl list-timers fstrim.timer
```

2. 如果未启用，启动定时器：

```plain
systemctl enable --now fstrim.timer
```

3. 查看定时任务的默认周期：

```plain
systemctl cat fstrim.timer
```

默认配置一般是每周运行一次。如果需要调整，可以自定义一个 `fstrim.timer`。


