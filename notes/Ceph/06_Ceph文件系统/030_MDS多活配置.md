# MDS多活配置
[https://docs.ceph.com/en/latest/cephfs/add-remove-mds/](https://docs.ceph.com/en/latest/cephfs/add-remove-mds/)

ceph mds作为ceph的访问入口，需要实现高性能及数据备份，而MDS支持多MDS架构，甚至还能实现类似于redis cluster的多主从架构，以实现MDS服务的高性能和高可用，假设启动4个MDS进程，设置最大max_mds为2，这时候有2个MDS成为主节点，另外2个MDS作为备份节点。



# 前提条件
## 已经创建了一个文件系统  
```bash
root@ceph-1:~# ceph fs ls
name: mycephfs, metadata pool: cephfs-metadata, data pools: [cephfs-data ]
```

## 至少有两个MDS 守护进程部署节点
```bash
root@ceph-1:~# ceph mds stat
mycephfs:1 {0=cephfs.ceph-1.tljivc=up:active} 1 up:standby
root@ceph-1:~# ceph orch ps | grep mds
mds.cephfs.ceph-1.tljivc   ceph-1                    running (9m)      9m ago   9m    13.1M        -  18.2.4   2bc0b0f4375d  1b8c5dd3df72  
mds.cephfs.ceph-3.bihunn   ceph-3                    running (9m)      9m ago   9m    13.7M        -  18.2.4   2bc0b0f4375d  2662a4d84668
```

# MDS 多活配置
## **部署多个 MDS 守护进程**
MDS 守护进程负责处理 CephFS 的元数据操作，配置多活需要至少两个 MDS 实例。

扩展 MDS 服务：

```bash
root@ceph-1:~# ceph orch apply mds mycephfs --placement 2
Scheduled mds.mycephfs update...
```

验证 MDS 服务是否已部署：

```bash
root@ceph-1:~# ceph mds stat
mycephfs:1/1 {0=mycephfs.ceph-2.ptiwyu=up:rejoin} 3 up:standby
```

## 启用多活模式
将mycephfs的活跃 MDS 数量设置为 2：

```plain
root@ceph-1:~# ceph fs set mycephfs max_mds 2
```

## 验证多活配置
检查 MDS 的状态和活跃数量：

```bash
root@ceph-1:~# ceph fs status
mycephfs - 0 clients
========
RANK  STATE            MDS               ACTIVITY     DNS    INOS   DIRS   CAPS  
 0    active  mycephfs.ceph-2.ptiwyu  Reqs:    0 /s    10     13     12      0   
 1    active  mycephfs.ceph-3.xqmral  Reqs:    0 /s    10     13     11      0   
      POOL         TYPE     USED  AVAIL  
cephfs-metadata  metadata   168k  47.4G  
  cephfs-data      data       0   47.4G  
    STANDBY MDS       
cephfs.ceph-3.bihunn  
cephfs.ceph-1.tljivc  
MDS version: ceph version 18.2.4 (e7ad5345525c7aa95470c26863873b581076945d) reef (stable)
```

这表示有两个活跃的 MDS 实例，正在处理文件系统的请求。

# mds节点切换流程
宕机-->replay（重新心跳检测）-->resolve（再次心跳检测）-->reconnect（重新连接）-->rejoin（备节点加入）-->active（主备切换完成）

```bash
[root@ceph-mgr1 ~]#tail -100 /var/log/ceph/ceph-mds.ceph-mgr1.log 
2023-09-25T03:40:11.157+0800 7fec41d01780  0 set uid:gid to 64045:64045 (ceph:ceph)
2023-09-25T03:40:11.157+0800 7fec41d01780  0 ceph version 16.2.14 (238ba602515df21ea7ffc75c88db29f9e5ef12c9) pacific (stable), process ceph-mds, pid 46097
2023-09-25T03:40:11.157+0800 7fec41d01780  1 main not setting numa affinity
2023-09-25T03:40:11.157+0800 7fec41d01780  0 pidfile_write: ignore empty --pid-file
2023-09-25T03:40:11.165+0800 7fec3d49e700  1 mds.ceph-mgr1 Updating MDS map to version 126 from mon.1
2023-09-25T03:40:11.221+0800 7fec3d49e700  1 mds.ceph-mgr1 Updating MDS map to version 127 from mon.1
2023-09-25T03:40:11.221+0800 7fec3d49e700  1 mds.ceph-mgr1 Monitors have assigned me to become a standby.
2023-09-25T03:40:21.665+0800 7fec3d49e700  1 mds.ceph-mgr1 Updating MDS map to version 129 from mon.1
2023-09-25T03:40:21.669+0800 7fec3d49e700  1 mds.1.129 handle_mds_map i am now mds.1.129
2023-09-25T03:40:21.669+0800 7fec3d49e700  1 mds.1.129 handle_mds_map state change up:standby --> up:replay			# 重新心跳检测
2023-09-25T03:40:21.669+0800 7fec3d49e700  1 mds.1.129 replay_start
2023-09-25T03:40:21.669+0800 7fec3d49e700  1 mds.1.129  waiting for osdmap 344 (which blocklists prior instance)
2023-09-25T03:40:21.681+0800 7fec36c91700  0 mds.1.cache creating system inode with ino:0x101
2023-09-25T03:40:21.681+0800 7fec36c91700  0 mds.1.cache creating system inode with ino:0x1
2023-09-25T03:40:21.681+0800 7fec35c8f700  1 mds.1.129 Finished replaying journal
2023-09-25T03:40:21.681+0800 7fec35c8f700  1 mds.1.129 making mds journal writeable
2023-09-25T03:40:22.669+0800 7fec3d49e700  1 mds.ceph-mgr1 Updating MDS map to version 130 from mon.1
2023-09-25T03:40:22.669+0800 7fec3d49e700  1 mds.1.129 handle_mds_map i am now mds.1.129
2023-09-25T03:40:22.669+0800 7fec3d49e700  1 mds.1.129 handle_mds_map state change up:replay --> up:resolve			# 再次心跳检测
2023-09-25T03:40:22.669+0800 7fec3d49e700  1 mds.1.129 resolve_start
2023-09-25T03:40:22.669+0800 7fec3d49e700  1 mds.1.129 reopen_log
2023-09-25T03:40:22.669+0800 7fec3d49e700  1 mds.1.129  recovery set is 0
2023-09-25T03:40:22.669+0800 7fec3d49e700  1 mds.1.129  recovery set is 0
2023-09-25T03:40:22.673+0800 7fec3d49e700  1 mds.ceph-mgr1 parse_caps: cannot decode auth caps buffer of length 0
2023-09-25T03:40:22.673+0800 7fec3d49e700  1 mds.1.129 resolve_done
2023-09-25T03:40:23.673+0800 7fec3d49e700  1 mds.ceph-mgr1 Updating MDS map to version 131 from mon.1
2023-09-25T03:40:23.673+0800 7fec3d49e700  1 mds.1.129 handle_mds_map i am now mds.1.129
2023-09-25T03:40:23.673+0800 7fec3d49e700  1 mds.1.129 handle_mds_map state change up:resolve --> up:reconnect		# 重新连接
2023-09-25T03:40:23.673+0800 7fec3d49e700  1 mds.1.129 reconnect_start
2023-09-25T03:40:23.673+0800 7fec3d49e700  1 mds.1.129 reconnect_done
2023-09-25T03:40:24.677+0800 7fec3d49e700  1 mds.ceph-mgr1 Updating MDS map to version 132 from mon.1
2023-09-25T03:40:24.677+0800 7fec3d49e700  1 mds.1.129 handle_mds_map i am now mds.1.129
2023-09-25T03:40:24.677+0800 7fec3d49e700  1 mds.1.129 handle_mds_map state change up:reconnect --> up:rejoin		# 备份节点加入
2023-09-25T03:40:24.677+0800 7fec3d49e700  1 mds.1.129 rejoin_start
2023-09-25T03:40:24.677+0800 7fec3d49e700  1 mds.1.129 rejoin_joint_start
2023-09-25T03:40:24.681+0800 7fec3d49e700  1 mds.1.129 rejoin_done
2023-09-25T03:40:25.682+0800 7fec3d49e700  1 mds.ceph-mgr1 Updating MDS map to version 133 from mon.1
2023-09-25T03:40:25.682+0800 7fec3d49e700  1 mds.1.129 handle_mds_map i am now mds.1.129
2023-09-25T03:40:25.682+0800 7fec3d49e700  1 mds.1.129 handle_mds_map state change up:rejoin --> up:active			# 主备切换完成
2023-09-25T03:40:25.682+0800 7fec3d49e700  1 mds.1.129 recovery_done -- successful recovery!
2023-09-25T03:40:25.682+0800 7fec3d49e700  1 mds.1.129 active_start
2023-09-25T03:40:25.682+0800 7fec3d49e700  1 mds.1.129 cluster recovered.
2023-09-25T03:53:09.090+0800 7fec3eca1700 -1 received  signal: Terminated from /sbin/init maybe-ubiquity  (PID: 1) UID: 0
2023-09-25T03:53:09.090+0800 7fec3eca1700 -1 mds.ceph-mgr1 *** got signal Terminated ***
2023-09-25T03:53:09.090+0800 7fec3eca1700  1 mds.ceph-mgr1 suicide! Wanted state up:active
2023-09-25T03:53:12.826+0800 7fec3eca1700  1 mds.1.129 shutdown: shutting down rank 1
2023-09-25T03:53:12.826+0800 7fec3d49e700  0 ms_deliver_dispatch: unhandled message 0x55f2498bc1c0 osd_map(348..348 src has 1..348) v4 from mon.1 v2:10.0.0.52:3300/0
2023-09-25T03:53:12.826+0800 7fec3d49e700  0 ms_deliver_dispatch: unhandled message 0x55f24a61b6c0 mdsmap(e 138) v2 from mon.1 v2:10.0.0.52:3300/0
2023-09-25T03:53:12.826+0800 7fec3d49e700  0 ms_deliver_dispatch: unhandled message 0x55f24a61a1a0 mdsmap(e 4294967295) v2 from mon.1 v2:10.0.0.52:3300/0
2023-09-25T03:53:12.826+0800 7fec3d49e700  0 ms_deliver_dispatch: unhandled message 0x55f24a5a3d40 mdsmap(e 139) v2 from mon.1 v2:10.0.0.52:3300/0
2023-09-25T03:53:12.826+0800 7fec3d49e700  0 ms_deliver_dispatch: unhandled message 0x55f24a5ce000 mdsmap(e 140) v2 from mon.1 v2:10.0.0.52:3300/0
```


