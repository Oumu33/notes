# 集群扩缩容

> 来源: Ceph
> 创建时间: 2020-12-03T09:16:31+08:00
> 更新时间: 2026-01-11T09:43:50.841857+08:00
> 阅读量: 82 | 点赞: 0

---

# 集群扩容
当Ceph的存储空间不够时候，需要对Ceph进⾏扩容，Ceph能⽀持横向动态⽔平扩容，通常两种⽅式： 

+ 添加更多的osd 
+ 添加额外的host

## 前提条件
rook默认使⽤“所有节点上所有的磁盘”，采⽤默认策略只要添加了磁盘或者主机就会按照 ROOK_DISCOVER_DEVICES_INTERVAL设定的间隔扩容磁盘，前⾯安装时候调整了相关的策略为关闭状态，因此需要⼿动定义nodes信息，将需要扩展的磁盘添加到列表中。

## 准备磁盘与节点
现在新增了一台 work1 节点，并添加了两块磁盘，接下来将 100G 和 50G 的硬盘都加入到 OSD 中，节点信息如下：

```bash
[root@work1 ~]# lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
nvme0n1     259:0    0   50G  0 disk 
├─nvme0n1p1 259:1    0    1G  0 part /boot
└─nvme0n1p2 259:2    0   49G  0 part 
  ├─rl-root 253:0    0   47G  0 lvm  /
  └─rl-swap 253:1    0    2G  0 lvm  
nvme0n2     259:3    0  100G  0 disk 
nvme0n3     259:4    0   50G  0 disk
```

## 添加节点标签
接下来添加 storage=true 标签，让相关资源调度到新的节点上。

```bash
# kubectl label nodes work1 storage=true
node/work1 labeled
```

## 修改节点列表
修改 cluster.yaml

```bash
    nodes:
      - name: "master1"
        devices:
          - name: "nvme0n2"
      - name: "master2"
        devices:
          - name: "nvme0n2"
      - name: "master3"
        devices:
          - name: "nvme0n2"
      - name: "work1" # 新增work1节点2块磁盘
        devices:
          - name: "nvme0n2"
          - name: "nvme0n3"
```

## 更新集群配置
```bash
kubectl apply -f cluster.yaml                
cephcluster.ceph.rook.io/rook-ceph configured
```

## 查看验证
查看 pod 信息，osd-3 和osd-4 已成功调度至 work1。

```bash
# kubectl get pod -n rook-ceph -o wide | grep work1
csi-cephfsplugin-n9nsb                              3/3     Running       6 (14m ago)   146m    192.168.10.154   work1     <none>           <none>
csi-rbdplugin-provisioner-5f87858d77-nbfjp          6/6     Running       7 (15m ago)   146m    10.244.3.107     work1     <none>           <none>
csi-rbdplugin-sw87x                                 3/3     Running       6 (14m ago)   146m    192.168.10.154   work1     <none>           <none>
rook-ceph-crashcollector-work1-69547786c4-cxjxv     1/1     Running       0             91s     192.168.10.154   work1     <none>           <none>
rook-ceph-exporter-work1-795b6f467b-pzm5n           1/1     Running       0             91s     192.168.10.154   work1     <none>           <none>
rook-ceph-osd-3-7f5c4fb554-8j6dt                    2/2     Running       0             91s     192.168.10.154   work1     <none>           <none>
rook-ceph-osd-4-64fd5f6f47-6mk2d                    2/2     Running       0             91s     192.168.10.154   work1     <none>           <none>
rook-ceph-osd-prepare-work1-xthxv                   0/1     Completed     0             2m19s   10.244.3.110     work1     <none>           <none>
```

查看 ceph 集群信息，work1 节点的两块磁盘已经成功加入集群。

```bash
# kubectl exec -it -n rook-ceph rook-ceph-tools-699dcdd8bb-d99mp -- bash
bash-5.1$ ceph -s
  cluster:
    id:     70105570-263d-40a1-b9d8-1a566a1c44d2
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum a,b,c (age 15m)
    mgr: a(active, since 15m), standbys: b
    mds: 2/2 daemons up, 2 hot standby
    osd: 5 osds: 5 up (since 69s), 5 in (since 2m)
    rgw: 3 daemons active (3 hosts, 1 zones)
 
  data:
    volumes: 1/1 healthy
    pools:   12 pools, 169 pgs
    objects: 283 objects, 636 KiB
    usage:   164 MiB used, 450 GiB / 450 GiB avail
    pgs:     169 active+clean
 
  io:
    client:   2.6 KiB/s rd, 170 B/s wr, 5 op/s rd, 0 op/s wr

bash-5.1$ ceph osd status
ID  HOST      USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE      
 0  master2  38.1M  99.9G      0        0       4      212   exists,up  
 1  master1  34.3M  99.9G      0        0       0        0   exists,up  
 2  master3  38.3M  99.9G      0        0       1        0   exists,up  
 3  work1    32.9M  99.9G      0        0       1        0   exists,up  
 4  work1    28.2M  49.9G      0        0       0        0   exists,up 
```

# 集群缩容
如果OSD所在的磁盘故障了或者需要更换配置，这时需要将OSD从集群中删除，删除OSD的注意事项

1. 删除osd后确保集群有⾜够的容量 
2. 删除osd后确保PG状态正常 
3. 单次尽可能不要删除过多的osd 
4. 删除多个osd需要等待数据同步同步完毕后再执⾏（rebalancing）

## 模拟故障
模拟osd.3、4 故障，osd故障时候pods状态会变成CrashLoopBackoff或者error状态，此时ceph中osd 的状态也会变成down状态，通过如下⽅式可以模拟：

```bash
# kubectl scale deployment -n rook-ceph rook-ceph-osd-3 --replicas=0
deployment.apps/rook-ceph-osd-3 scaled
# kubectl scale deployment -n rook-ceph rook-ceph-osd-4 --replicas=0
deployment.apps/rook-ceph-osd-4 scaled
```

此时查看 osd 节点状态，已经变成了 down 状态

```bash
bash-5.1$ ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.43954  root default                               
-7         0.09769      host master1                           
 1   nvme  0.09769          osd.1         up   1.00000  1.00000
-3         0.09769      host master2                           
 0   nvme  0.09769          osd.0         up   1.00000  1.00000
-5         0.09769      host master3                           
 2   nvme  0.09769          osd.2         up   1.00000  1.00000
-9         0.14648      host work1                             
 3   nvme  0.09769          osd.3       down   1.00000  1.00000
 4   nvme  0.04880          osd.4       down   1.00000  1.00000
```

## 停止 operator
operator 会自动拉取 osd 相关服务，为了避免影响，在缩容之前先停止 operator。

```bash
# kubectl scale deployment -n rook-ceph rook-ceph-operator --replicas=0
deployment.apps/rook-ceph-operator scaled
```

## 
## 通过 Job 方式下线
参考脚本：[https://github.com/rook/rook/blob/release-1.16/deploy/examples/osd-purge.yaml](https://github.com/rook/rook/blob/release-1.16/deploy/examples/osd-purge.yaml)

1. 创建 yaml 文件，内容如下：

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: rook-ceph-purge-osd
  namespace: rook-ceph # namespace:cluster
  labels:
    app: rook-ceph-purge-osd
spec:
  template:
    metadata:
      labels:
        app: rook-ceph-purge-osd
    spec:
      serviceAccountName: rook-ceph-purge-osd
      containers:
        - name: osd-removal
          image: docker.io/rook/ceph:v1.16.0
          # TODO: Insert the OSD ID in the last parameter that is to be removed
          # The OSD IDs are a comma-separated list. For example: "0" or "0,2".
          # If you want to preserve the OSD PVCs, set `--preserve-pvc true`.
          #
          # A --force-osd-removal option is available if the OSD should be destroyed even though the
          # removal could lead to data loss.
          args:
            - "ceph"
            - "osd"
            - "remove"
            - "--preserve-pvc"
            - "false"
            - "--force-osd-removal"
            - "false"
            - "--osd-ids"
            - "3" # 指定将osd3下线处理
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: ROOK_MON_ENDPOINTS
              valueFrom:
                configMapKeyRef:
                  key: data
                  name: rook-ceph-mon-endpoints
            - name: ROOK_CEPH_USERNAME
              valueFrom:
                secretKeyRef:
                  key: ceph-username
                  name: rook-ceph-mon
            - name: ROOK_CONFIG_DIR
              value: /var/lib/rook
            - name: ROOK_CEPH_CONFIG_OVERRIDE
              value: /etc/rook/config/override.conf
            - name: ROOK_FSID
              valueFrom:
                secretKeyRef:
                  key: fsid
                  name: rook-ceph-mon
            - name: ROOK_LOG_LEVEL
              value: DEBUG
          volumeMounts:
            - mountPath: /etc/ceph
              name: ceph-conf-emptydir
            - mountPath: /var/lib/rook
              name: rook-config
            - name: ceph-admin-secret
              mountPath: /var/lib/rook-ceph-mon
      volumes:
        - name: ceph-admin-secret
          secret:
            secretName: rook-ceph-mon
            optional: false
            items:
              - key: ceph-secret
                path: secret.keyring
        - emptyDir: {}
          name: ceph-conf-emptydir
        - emptyDir: {}
          name: rook-config
      restartPolicy: Never
```

2. 创建 job

```bash
# kubectl apply -f osd-down.yaml    
job.batch/rook-ceph-purge-osd created
#  kubectl logs -n rook-ceph rook-ceph-purge-osd-lhtl4 -f 
```

3. 待 job 执行完后查看验证，osd3 已经成功下线。

```bash
ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.34186  root default                               
-7         0.09769      host master1                           
 1   nvme  0.09769          osd.1         up   1.00000  1.00000
-3         0.09769      host master2                           
 0   nvme  0.09769          osd.0         up   1.00000  1.00000
-5         0.09769      host master3                           
 2   nvme  0.09769          osd.2         up   1.00000  1.00000
-9         0.04880      host work1                             
 4   nvme  0.04880          osd.4       down         0  1.00000
```



## 通过手动方式下线
除了使⽤云原⽣的⽅式删除osd之外，也可以使⽤Ceph标准的⽅式进⾏删除，以下是删除的⽅法  .

## 停止 operator


operator 会自动拉取 osd 相关服务，为了避免影响，在缩容之前先停止 operator。

1. 将 osd 标记为 out，此时会进行元数据的同步

```bash
bash-5.1$ ceph osd out osd.4
marked out osd.4. 
```

2. 删除 osd，此时会进行数据迁移。

```bash
bash-5.1$ ceph osd purge 4 --yes-i-really-mean-it
purged osd.4
```

3. 待数据迁移完成后，查看验证，此时 osd4 已经下线

```bash
bash-5.1$ ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.29306  root default                               
-7         0.09769      host master1                           
 1   nvme  0.09769          osd.1         up   1.00000  1.00000
-3         0.09769      host master2                           
 0   nvme  0.09769          osd.0         up   1.00000  1.00000
-5         0.09769      host master3                           
 2   nvme  0.09769          osd.2         up   1.00000  1.00000
-9               0      host work1 
```

4. 删除 CRUSH，虽然 work1 的两块磁盘都已删除，但 work1 节点信息还存在，需要删除。

```bash
bash-5.1$ ceph osd crush rm work1
removed item id -9 name 'work1' from crush map
bash-5.1$ ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.29306  root default                               
-7         0.09769      host master1                           
 1   nvme  0.09769          osd.1         up   1.00000  1.00000
-3         0.09769      host master2                           
 0   nvme  0.09769          osd.0         up   1.00000  1.00000
-5         0.09769      host master3                           
 2   nvme  0.09769          osd.2         up   1.00000  1.00000
```

## 删除 deployment
接下来我们将 osd3 和 4 对应的 deployment 资源删除。

```bash
# kubectl delete deployments.apps -n rook-ceph rook-ceph-osd-3 rook-ceph-osd-4
deployment.apps "rook-ceph-osd-4" deleted
deployment.apps "rook-ceph-osd-3" deleted
```

## 恢复 operator
```bash
# kubectl scale deployment -n rook-ceph rook-ceph-operator --replicas=1
deployment.apps/rook-ceph-operator scaled
```

## 更新集群配置
更新 cluster.yaml 中节点相关配置， 避免apply之后重新添加回集群  

```bash
    nodes:
      - name: "master1"
        devices:
          - name: "nvme0n2"
      - name: "master2"
        devices:
          - name: "nvme0n2"
      - name: "master3"
        devices:
          - name: "nvme0n2"
      # - name: "work1"
      #   devices:
      #     - name: "nvme0n2"
      #     - name: "nvme0n3"
```




