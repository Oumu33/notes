# 集群缩减
> 以删除 ceph-4 节点以及上面的两个 osd 为例。
>

# 删除 OSD
首先，您需要从集群中删除 OSD。确保删除 OSD 之前，数据已经通过 `rebalance` 转移到其他 OSD 上。

## 查看 OSD 的 ID 和状态
```bash
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.34177  root default
-3         0.04880      host ceph-1
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
-5         0.04880      host ceph-2
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
-7         0.04880      host ceph-3
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
-9         0.19537      host ceph-4
 3    hdd  0.09769          osd.3        up   1.00000  1.00000
 4    hdd  0.09769          osd.4        up   1.00000  1.00000
```

## 标记 OSD 为 "out"
在删除 OSD 之前，应该将 OSD 标记为 "out"。这会触发 Ceph 将该 OSD 上的数据迁移到其他 OSD。

```bash
root@ceph-1:~# ceph osd out osd.3
marked out osd.3. 
root@ceph-1:~# ceph osd out osd.4
marked out osd.4.
```

## 等待数据迁移完成
使用以下命令检查 PGs 的状态，确认数据已经迁移到其他 OSD：

```bash
root@ceph-1:~# ceph -s
  cluster:
    id:     402d9800-afef-11ef-92d7-9fbbd69ceccd
    health: HEALTH_OK

  services:
    mon: 4 daemons, quorum ceph-1,ceph-3,ceph-2,ceph-4 (age 12m)
    mgr: ceph-1.cuuabg(active, since 103m), standbys: ceph-3.uhtqme
    osd: 5 osds: 5 up (since 9m), 3 in (since 27s)

  data:
    pools:   2 pools, 33 pgs
    objects: 2 objects, 449 KiB
    usage:   223 MiB used, 150 GiB / 150 GiB avail
    pgs:     33 active+clean
```

确保所有 PGs 的状态为 `active+clean`，并且没有任何 PGs处于 `degraded` 或 `incomplete` 状态。

## 停止 OSD 服务
```bash
root@ceph-4:~# systemctl stop ceph-402d9800-afef-11ef-92d7-9fbbd69ceccd@osd.3.service
root@ceph-4:~# systemctl stop ceph-402d9800-afef-11ef-92d7-9fbbd69ceccd@osd.4.service
root@ceph-4:~# systemctl disable ceph-402d9800-afef-11ef-92d7-9fbbd69ceccd@osd.3.service
Removed /etc/systemd/system/ceph-402d9800-afef-11ef-92d7-9fbbd69ceccd.target.wants/ceph-402d9800-afef-11ef-92d7-9fbbd69ceccd@osd.3.service.
root@ceph-4:~# systemctl disable ceph-402d9800-afef-11ef-92d7-9fbbd69ceccd@osd.4.service
Removed /etc/systemd/system/ceph-402d9800-afef-11ef-92d7-9fbbd69ceccd.target.wants/ceph-402d9800-afef-11ef-92d7-9fbbd69ceccd@osd.4.service.
```

停止后 osd 状态为 down

```bash
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.34177  root default
-3         0.04880      host ceph-1
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
-5         0.04880      host ceph-2
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
-7         0.04880      host ceph-3
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
-9         0.19537      host ceph-4
 3    hdd  0.09769          osd.3      down         0  1.00000
 4    hdd  0.09769          osd.4      down         0  1.00000
```

## 删除守护进程
```bash
root@ceph-1:~# ceph orch daemon rm osd.3 --force
Removed osd.3 from host 'ceph-4'
root@ceph-1:~# ceph orch daemon rm osd.4 --force
Removed osd.4 from host 'ceph-4'
```

## 删除 OSD
一旦数据迁移完成，可以从 Ceph 集群中彻底删除 OSD。使用以下命令删除 OSD：

```bash
root@ceph-1:~# ceph osd purge osd.3 --yes-i-really-mean-it
purged osd.3
root@ceph-1:~# ceph osd purge osd.4 --yes-i-really-mean-it
purged osd.4
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.14639  root default
-3         0.04880      host ceph-1
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
-5         0.04880      host ceph-2
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
-7         0.04880      host ceph-3
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
-9               0      host ceph-4
```

## 删除crush map
```bash
root@ceph-1:~# ceph osd crush rm ceph-4
removed item id -9 name 'ceph-4' from crush map
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.14639  root default
-3         0.04880      host ceph-1
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
-5         0.04880      host ceph-2
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
-7         0.04880      host ceph-3
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
```

# 删除节点
删除 OSD 后，可以从集群中移除主机（即物理节点）。

## 驱逐节点服务
```bash
root@ceph-1:~# ceph orch host drain ceph-4
Scheduled to remove the following daemons from host 'ceph-4'
type                 id
-------------------- ---------------
osd                  4
crash                ceph-4
mon                  ceph-4
ceph-exporter        ceph-4
node-exporter        ceph-4
osd                  3
root@ceph-1:~# ceph orch host ls
HOST    ADDR           LABELS                         STATUS  
ceph-1  192.168.10.91  _admin
ceph-2  192.168.10.92  _admin
ceph-3  192.168.10.93  _admin
ceph-4  192.168.10.94  _no_schedule,_no_conf_keyring
```

## 节点删除
确保新节点已成功从集群中移除：

```bash
root@ceph-1:~# ceph orch host rm ceph-4
Removed  host 'ceph-4'
root@ceph-1:~# ceph orch host ls
HOST    ADDR           LABELS  STATUS  
ceph-1  192.168.10.91  _admin
ceph-2  192.168.10.92  _admin
ceph-3  192.168.10.93  _admin
3 hosts in cluster
root@ceph-1:~# ceph -s
  cluster:
    id:     402d9800-afef-11ef-92d7-9fbbd69ceccd
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph-1,ceph-3,ceph-2 (age 12m)
    mgr: ceph-1.cuuabg(active, since 2h), standbys: ceph-3.uhtqme
    osd: 3 osds: 3 up (since 15m), 3 in (since 22m)

  data:
    pools:   2 pools, 33 pgs
    objects: 2 objects, 449 KiB
    usage:   223 MiB used, 150 GiB / 150 GiB avail
    pgs:     33 active+clean
```

确保节点已从集群中移除，并且集群健康状态是 `HEALTH_OK`。

# 清理节点数据
在节点从 Ceph 集群中完全移除后，可以删除相关的配置文件和日志，以便释放空间。

```bash
root@ceph-4:~# rm -rf /etc/ceph
root@ceph-4:~# rm -rf /var/log/ceph
```

## 删除 lvm
```bash
root@ceph-4:~# lvs
  LV                                             VG                                        Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  osd-block-190fa79c-1703-45ac-9dad-7f8dc311c870 ceph-9c9dc788-5a5f-4ed8-af33-105e645c507e -wi-a----- <100.00g
  osd-block-2afefa66-63b9-41c9-985a-ee6665bca658 ceph-a4531721-cc21-4039-8950-30e428a53166 -wi-a----- <100.00g
  ubuntu-lv                                      ubuntu-vg                                 -wi-ao----  <49.00g
root@ceph-4:~# vgremove ceph-9c9dc788-5a5f-4ed8-af33-105e645c507e 
Do you really want to remove volume group "ceph-9c9dc788-5a5f-4ed8-af33-105e645c507e" containing 1 logical volumes? [y/n]: y
Do you really want to remove and DISCARD active logical volume ceph-9c9dc788-5a5f-4ed8-af33-105e645c507e/osd-block-190fa79c-1703-45ac-9dad-7f8dc311c870? [y/n]: y
  Logical volume "osd-block-190fa79c-1703-45ac-9dad-7f8dc311c870" successfully removed
  Volume group "ceph-9c9dc788-5a5f-4ed8-af33-105e645c507e" successfully removed
root@ceph-4:~# vgremove ceph-a4531721-cc21-4039-8950-30e428a53166 
Do you really want to remove volume group "ceph-a4531721-cc21-4039-8950-30e428a53166" containing 1 logical volumes? [y/n]: y
Do you really want to remove and DISCARD active logical volume ceph-a4531721-cc21-4039-8950-30e428a53166/osd-block-2afefa66-63b9-41c9-985a-ee6665bca658? [y/n]: y
  Logical volume "osd-block-2afefa66-63b9-41c9-985a-ee6665bca658" successfully removed
  Volume group "ceph-a4531721-cc21-4039-8950-30e428a53166" successfully removed
root@ceph-4:~# pvremove /dev/sdb 
  Labels on physical volume "/dev/sdb" successfully wiped.
root@ceph-4:~# pvremove /dev/sdc
  Labels on physical volume "/dev/sdc" successfully wiped.
# 查看验证
root@ceph-4:~# pvs
  PV         VG        Fmt  Attr PSize   PFree 
  /dev/sda3  ubuntu-vg lvm2 a--  <98.00g 49.00g
root@ceph-4:~# vgs
  VG        #PV #LV #SN Attr   VSize   VFree 
  ubuntu-vg   1   1   0 wz--n- <98.00g 49.00g
root@ceph-4:~# lvs
  LV        VG        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  ubuntu-lv ubuntu-vg -wi-ao---- <49.00g
root@ceph-4:~# lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0                       7:0    0 63.3M  1 loop /snap/core20/1828
loop1                       7:1    0 91.9M  1 loop /snap/lxd/29619
loop2                       7:2    0 49.9M  1 loop /snap/snapd/18357
loop3                       7:3    0 91.9M  1 loop /snap/lxd/24061
loop4                       7:4    0 44.3M  1 loop /snap/snapd/23258
loop5                       7:5    0 63.7M  1 loop /snap/core20/2434
sda                         8:0    0  100G  0 disk
├─sda1                      8:1    0    1M  0 part
├─sda2                      8:2    0    2G  0 part /boot
└─sda3                      8:3    0   98G  0 part
  └─ubuntu--vg-ubuntu--lv 253:0    0   49G  0 lvm  /
sdb                         8:16   0  100G  0 disk
sdc                         8:32   0  100G  0 disk
```

## 磁盘初始化
如果需要重新加入 ceph 集群，需要执行如下命令

```bash
# apt -y install gdisk lvm2
# wipefs -a /dev/sdb && sgdisk --zap-all /dev/sdb && dd if=/dev/zero of=/dev/sdb bs=1M count=1
# wipefs -a /dev/sdc && sgdisk --zap-all /dev/sdc && dd if=/dev/zero of=/dev/sdc bs=1M count=1
```


