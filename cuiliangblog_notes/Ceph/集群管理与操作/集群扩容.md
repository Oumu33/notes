# 集群扩容

> 分类: Ceph > 集群管理与操作
> 更新时间: 2026-01-10T23:35:15.593147+08:00

---

> 以新增一个节点 2 个 OSD 为例，演示集群如何扩容。
>

# 节点准备
节点信息如下：

```bash
root@ceph-4:~# lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 20.04.6 LTS
Release:        20.04
Codename:       focal
root@ceph-4:~# hostname
ceph-4
root@ceph-4:~# ip a | grep 192.168
    inet 192.168.10.94/24 brd 192.168.10.255 scope global ens33
root@ceph-4:~# lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0                       7:0    0 63.3M  1 loop /snap/core20/1828
loop1                       7:1    0 91.9M  1 loop /snap/lxd/29619
loop2                       7:2    0 49.9M  1 loop /snap/snapd/18357
loop3                       7:3    0 91.9M  1 loop /snap/lxd/24061
sda                         8:0    0  100G  0 disk
├─sda1                      8:1    0    1M  0 part
├─sda2                      8:2    0    2G  0 part /boot
└─sda3                      8:3    0   98G  0 part
  └─ubuntu--vg-ubuntu--lv 253:0    0   49G  0 lvm  /
sdb                         8:16   0  100G  0 disk
sdc                         8:32   0  100G  0 disk
```

## 系统初始化
+ 修改主机名
+ 添加hosts解析
+ 配置时间同步
+ 关闭防火墙、seLinux
+ 安装docker
+ 挂载数据盘，不要格式化

## 安装 Ceph 相关工具
在新节点上安装 Cephadm 工具

```bash
root@ceph-4:~# cat > /etc/apt/sources.list.d/ceph.list << EOF
deb https://mirrors.ustc.edu.cn/ceph/debian-reef/ focal main 
EOF
root@ceph-4:~# wget -q -O- 'https://mirrors.ustc.edu.cn/ceph/keys/release.asc' | sudo apt-key add -
root@ceph-4:~# apt update && apt install cephadm -y
root@ceph-4:~# cephadm version
cephadm version 18.2.4 (e7ad5345525c7aa95470c26863873b581076945d) reef (stable)
```

拉取镜像

```bash
root@ceph-4:~# cephadm pull
Pulling container image quay.io/ceph/ceph:v18...
{
    "ceph_version": "ceph version 18.2.4 (e7ad5345525c7aa95470c26863873b581076945d) reef (stable)",
    "image_id": "2bc0b0f4375ddf4270a9a865dfd4e53063acc8e6c3afd7a2546507cafd2ec86a",
    "repo_digests": [
        "quay.io/ceph/ceph@sha256:6ac7f923aa1d23b43248ce0ddec7e1388855ee3d00813b52c3172b0b23b37906"
    ]
}
root@ceph-4:~# docker images
REPOSITORY          TAG       IMAGE ID       CREATED        SIZE
quay.io/ceph/ceph   v18       2bc0b0f4375d   4 months ago   1.22GB
```

安装 ceph 客户端

```bash
root@ceph-4:~# cephadm install ceph-common
# 或者使用apt命令安装
root@ceph-4:~# apt install ceph-common -y
```

# 将新节点加入集群
## 把秘钥放到其他节点上
```bash
root@ceph-1:~# ssh-copy-id -f -i /etc/ceph/ceph.pub ceph-4
```

## 添加节点
```bash
root@ceph-1:~# ceph orch host add ceph-4 192.168.10.94
Added host 'ceph-4' with addr '192.168.10.94'
```

## 查看节点列表
```bash
root@ceph-1:~# ceph orch host ls
HOST    ADDR           LABELS  STATUS  
ceph-1  192.168.10.91  _admin
ceph-2  192.168.10.92  _admin
ceph-3  192.168.10.93  _admin
ceph-4  192.168.10.94
4 hosts in cluster
```

# 新增 OSD 磁盘
## 查看osd设备列表
```bash
root@ceph-1:~# ceph orch device ls
HOST    PATH      TYPE  DEVICE ID   SIZE  AVAILABLE  REFRESHED  REJECT REASONS
ceph-1  /dev/sdb  hdd              50.0G  No         15s ago    Has a FileSystem, Insufficient space (<10 extents) on vgs, LVM detected
ceph-2  /dev/sdb  hdd              50.0G  No         11s ago    Has a FileSystem, Insufficient space (<10 extents) on vgs, LVM detected
ceph-3  /dev/sdb  hdd              50.0G  No         10s ago    Has a FileSystem, Insufficient space (<10 extents) on vgs, LVM detected
ceph-4  /dev/sdb  hdd               100G  Yes        99s ago
ceph-4  /dev/sdc  hdd               100G  Yes        99s ago
```

## 添加OSD设备到集群
```bash
root@ceph-1:~# ceph orch daemon add osd ceph-4:/dev/sdb
Created osd(s) 3 on host 'ceph-4'
root@ceph-1:~# ceph orch daemon add osd ceph-4:/dev/sdc
Created osd(s) 4 on host 'ceph-4'
```

## 查看OSD信息
```bash
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.34177  root default
-3         0.04880      host ceph-1
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
-5         0.04880      host ceph-2
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
-7         0.04880      host ceph-3
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
-9         0.19537      host ceph-4
 3    hdd  0.09769          osd.3        up   1.00000  1.00000
 4    hdd  0.09769          osd.4        up   1.00000  1.00000
```

# 集群调整（可选）
## 调整 CRUSH 映射
添加新的 OSD 后，需要更新 CRUSH 映射，以确保数据均匀分布：

```bash
ceph osd crush add-bucket <new_bucket_name> host
ceph osd crush move <new_bucket_name> root=default
```

## 重新平衡数据
Ceph 集群在扩容后，数据会自动开始平衡。您可以使用以下命令查看平衡状态：

```plain
ceph osd reweight-by-utilization
```

并且检查集群状态：

```plain
ceph -s
```

确保集群处于 **HEALTH_OK** 状态，且 **PGs** 状态为 `active+clean`。

## 调整池的副本数
如果希望使用新的节点和磁盘来增加副本数（例如在扩容时增加副本数以提高冗余），可以调整池的副本数：

```plain
ceph osd pool set <pool_name> size <new_size>
```

