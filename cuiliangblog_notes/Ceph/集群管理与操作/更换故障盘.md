# 更换故障盘
> 随着 ceph 集群数据盘的增加，磁盘故障率也会随之增加，因此更换故障盘就是 ceph 集群最常见的故障维护操作。
>

# 移除故障盘
## 模拟磁盘故障
直接在虚拟机设置中移除一块 ceph4 节点的数据盘。

## 识别故障磁盘
查看磁盘的健康状况，发现 osd.4 状态为 down。

```bash
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.34177  root default                              
-3         0.04880      host ceph-1                           
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
-5         0.04880      host ceph-2                           
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
-7         0.04880      host ceph-3                           
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
-9         0.19537      host ceph-4                           
 3    hdd  0.09769          osd.3        up   1.00000  1.00000
 4    hdd  0.09769          osd.4      down   1.00000  1.00000
```

## 标记 OSD 为 `out`
在更换磁盘之前，需要将该 OSD 标记为 `out`，使得数据可以被其他 OSD 重新平衡（如果启用了 CRUSH 算法，数据会重新分布到集群中其他健康的 OSD）。

```bash
root@ceph-1:~# ceph osd out osd.4
marked out osd.4.
```

等待数据迁移完成，直到所有数据被复制到其他 OSD 上。

```bash
root@ceph-1:~# ceph -s
  cluster:
    id:     402d9800-afef-11ef-92d7-9fbbd69ceccd
    health: HEALTH_OK
 
  services:
    mon: 4 daemons, quorum ceph-1,ceph-3,ceph-2,ceph-4 (age 112s)
    mgr: ceph-1.cuuabg(active, since 22m), standbys: ceph-3.uhtqme
    osd: 5 osds: 4 up (since 107s), 4 in (since 19s)
 
  data:
    pools:   2 pools, 33 pgs
    objects: 2 objects, 449 KiB
    usage:   156 MiB used, 250 GiB / 250 GiB avail
    pgs:     33 active+clean
```

## 停止故障 OSD 的服务
在磁盘故障确认后，停止相关 OSD 服务：

```bash
root@ceph-4:~# systemctl stop ceph-402d9800-afef-11ef-92d7-9fbbd69ceccd@osd.4.service
root@ceph-4:~# systemctl disable ceph-402d9800-afef-11ef-92d7-9fbbd69ceccd@osd.4.service
```

## 删除 crush map 
 从 Ceph crush map 中删除故障磁盘 OSD  

```bash
root@ceph-1:~# ceph osd crush rm osd.4
removed item id 4 name 'osd.4' from crush map
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.24408  root default                              
-3         0.04880      host ceph-1                           
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
-5         0.04880      host ceph-2                           
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
-7         0.04880      host ceph-3                           
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
-9         0.09769      host ceph-4                           
 3    hdd  0.09769          osd.3        up   1.00000  1.00000
 4               0  osd.4              down         0  1.00000
```

## 从集群中删除 OSD
```bash
root@ceph-1:~# ceph osd rm osd.4
removed osd.4
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.24408  root default                              
-3         0.04880      host ceph-1                           
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
-5         0.04880      host ceph-2                           
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
-7         0.04880      host ceph-3                           
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
-9         0.09769      host ceph-4                           
 3    hdd  0.09769          osd.3        up   1.00000  1.00000
```

# 添加新磁盘
## 添加磁盘设备
新加入一块 50G 的 sdc 磁盘设备。

```bash
root@ceph-4:~# lsblk
NAME                                                                     MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0                                                                      7:0    0 63.3M  1 loop /snap/core20/1828
loop1                                                                      7:1    0 63.7M  1 loop /snap/core20/2434
loop2                                                                      7:2    0 44.3M  1 loop /snap/snapd/23258
loop3                                                                      7:3    0 49.9M  1 loop /snap/snapd/18357
loop4                                                                      7:4    0 91.9M  1 loop /snap/lxd/29619
loop5                                                                      7:5    0 91.9M  1 loop /snap/lxd/24061
sda                                                                        8:0    0  100G  0 disk 
├─sda1                                                                     8:1    0    1M  0 part 
├─sda2                                                                     8:2    0    2G  0 part /boot
└─sda3                                                                     8:3    0   98G  0 part 
  └─ubuntu--vg-ubuntu--lv                                                253:1    0   49G  0 lvm  /
sdb                                                                        8:16   0  100G  0 disk 
└─ceph--23327d19--5f00--4af9--a6a4--7377da2b94fb-osd--block--6d9a5240--a90d--4bd6--80b3--3ae4e2e808ee
                                                                         253:0    0  100G  0 lvm  
sdc                                                                        8:32   0   50G  0 disk 
```

## 刷新磁盘信息
```bash
# yum -y install gdisk lvm2
# wipefs -a /dev/sdc && sgdisk --zap-all /dev/sdc && dd if=/dev/zero of=/dev/sdc bs=1M count=1
# rm -rvf /var/lib/rook/* /dev/bdb* /dev/nbd* /dev/ceph-*
```

## 查看磁盘信息
+ 在新磁盘上成功创建 OSD 后，使用以下命令将其添加回 Ceph 集群：

```bash
root@ceph-1:~# ceph orch device ls --refresh
HOST    PATH      TYPE  DEVICE ID   SIZE  AVAILABLE  REFRESHED  REJECT REASONS                                                           
ceph-1  /dev/sdb  hdd              50.0G  No         61s ago    Has a FileSystem, Insufficient space (<10 extents) on vgs, LVM detected  
ceph-2  /dev/sdb  hdd              50.0G  No         67s ago    Has a FileSystem, Insufficient space (<10 extents) on vgs, LVM detected  
ceph-3  /dev/sdb  hdd              50.0G  No         67s ago    Has a FileSystem, Insufficient space (<10 extents) on vgs, LVM detected  
ceph-4  /dev/sdb  hdd               100G  No         62s ago    Has a FileSystem, Insufficient space (<10 extents) on vgs, LVM detected  
ceph-4  /dev/sdc  hdd              50.0G  Yes        62s ago
```

## 将 OSD 添加到集群
```bash
root@ceph-1:~# ceph orch daemon add osd ceph-4:/dev/sdb
```

## 检查集群
```plain
ceph -s
```





