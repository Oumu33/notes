# OSD管理

> 来源: Ceph
> 创建时间: 2024-12-01T18:08:43+08:00
> 更新时间: 2026-01-11T09:43:40.725283+08:00
> 阅读量: 130 | 点赞: 0

---

# 查看 OSD 状态
## 查看 OSD 总体状态
```plain
root@ceph-1:~# ceph osd stat
3 osds: 3 up (since 52m), 3 in (since 23h); epoch: e481
```

+ 显示 OSD 总数、`up` 和 `in` 的数量。

## 查看具体 OSD 列表和状态
```plain
root@ceph-1:~# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.14639  root default
-3         0.04880      host ceph-1
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
-5         0.04880      host ceph-2
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
-7         0.04880      host ceph-3
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
```

+ 以树状结构显示每个 OSD 的分布及状态。

## 查看每个 OSD 的存储使用情况
```plain
root@ceph-1:~# ceph osd df
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP    META     AVAIL    %USE  VAR   PGS  STATUS
 0    hdd  0.04880   1.00000   50 GiB   74 MiB  4.2 MiB   4 KiB   70 MiB   50 GiB  0.14  1.00   33      up
 1    hdd  0.04880   1.00000   50 GiB   74 MiB  4.2 MiB   4 KiB   70 MiB   50 GiB  0.14  1.00   33      up
 2    hdd  0.04880   1.00000   50 GiB   74 MiB  4.2 MiB   4 KiB   70 MiB   50 GiB  0.14  1.00   33      up
                       TOTAL  150 GiB  222 MiB   13 MiB  13 KiB  210 MiB  150 GiB  0.14
MIN/MAX VAR: 1.00/1.00  STDDEV: 0
```

+ 显示每个 OSD 的磁盘使用率和可用容量。

## 查看特定 OSD 的状态
```plain
root@ceph-1:~# ceph osd metadata 0
{
    "id": 0,
    "arch": "x86_64",
    "back_addr": "[v2:192.168.10.91:6802/112374299,v1:192.168.10.91:6803/112374299]",
    ……
}
```

+ 返回 OSD 的详细元数据信息。

# OSD 启用/禁用
## 将 OSD 移出集群
```plain
ceph osd out <OSD_ID>
```

+ 标记 OSD 为 `out` 状态，停止接收新数据写入，但数据仍可读取。

## 将 OSD 加入集群
```plain
ceph osd in <OSD_ID>
```

+ 将 OSD 标记为 `in` 状态，重新加入集群开始服务。

## 检查特定 OSD 的副本状态
```plain
ceph pg ls-by-osd <OSD_ID>
```

+ 查看与该 OSD 相关的 PG。

# OSD 启停操作
## 停止指定 OSD 服务
```plain
systemctl stop ceph-osd@<OSD_ID>
```

## 启动指定 OSD 服务
```plain
systemctl start ceph-osd@<OSD_ID>
```

## 重启 OSD 服务
```plain
systemctl restart ceph-osd@<OSD_ID>
```

## 检查 OSD 服务状态
```plain
systemctl status ceph-osd@<OSD_ID>
```

# 添加新 OSD
## 准备磁盘
```plain
ceph-volume lvm prepare --data /dev/<disk>
```

+ 示例：

```plain
ceph-volume lvm prepare --data /dev/sdb
```

## 激活 OSD
```plain
ceph-volume lvm activate --all
```

+ 激活所有已准备的 OSD。

## 查看磁盘部署状态
```plain
ceph-volume lvm list
```

+ 列出所有通过 LVM 管理的 OSD。

## 动态部署 OSD
```plain
ceph orch daemon add osd <HOST>:<DISK>
```

# 移除 OSD
## 标记为 out
```plain
ceph osd out <OSD_ID>
```

## 停止服务
```plain
systemctl stop ceph-osd@<OSD_ID>
```

## 从集群中移除
```plain
ceph osd purge <OSD_ID> --yes-i-really-mean-it
```

## 强制移除 OSD
```plain
ceph osd rm <OSD_ID>
```

## 清理磁盘上的 Ceph 数据
```plain
ceph-volume lvm zap /dev/<disk>
```

# OSD 设置
## 调整 OSD 权重
```plain
bash


复制代码
ceph osd crush reweight <OSD_ID> <weight>
```

+ 示例：

```plain
bash


复制代码
ceph osd crush reweight 2 0.8
```

## 设置 OSD 的 CRUSH 位置
```plain
bash


复制代码
ceph osd crush set <OSD_ID> <CRUSH_WEIGHT> <CRUSH_LOCATION>
```

+ 示例：

```plain
bash


复制代码
ceph osd crush set 2 1.0 root=default host=node1
```


