# 客户端使用RBD

> 分类: Ceph > Ceph块存储
> 更新时间: 2026-01-10T23:35:16.039860+08:00

---

> RBD（RADOS Block Devices）即块存储设备，RBD可以为KVM、VMware等虚拟化技术和云服务（OpenStack、kubernetes）提供高性能和无限可扩展的存储后端，客户端基于librbd库即可将RADOS存储集群用作块设备。
>

# 服务端创建资源
## 创建RBD
创建一个名为 rbd-data 的存储池，并在启用rbd功能后对其进行初始化。

```bash
# 创建存储池，指定pg和pgp的数量，pgp是对存在于pg的数据进行组合存储，pgp通常等于pg的值
root@ceph-1:~# ceph osd pool create rbd-data 32 32
pool 'rbd-data' created
 
# 查看存储池
root@ceph-1:~# ceph osd pool ls
.mgr
mypool
rbd-data
 
# 对存储池开启rbd功能
root@ceph-1:~# ceph osd pool application enable rbd-data rbd
enabled application 'rbd' on pool 'rbd-data'
 
# 使用rbd命令对存储池初始化
root@ceph-1:~# rbd pool init -p rbd-data
```

## 创建并验证镜像
rbd存储池并不能直接用于块设备，而是需要事先在其中按需创建镜像(image) ,并把映像文件作为块设备使用, rbd命令可用于创建、查看及删除块设备所在的镜像(image),以及克隆镜像、创建快照、将镜像回滚到快照和查看快照等管理操作。

```bash
# 创建两个镜像
root@ceph-1:~# rbd create data-img1 --size 3G --pool rbd-data
root@ceph-1:~# rbd create data-img2 --size 4G --pool rbd-data

# 列出镜像更多信息
root@ceph-1:~# rbd ls --pool rbd-data -l
NAME       SIZE   PARENT  FMT  PROT  LOCK
data-img1  3 GiB            2
data-img2  4 GiB            2     
 
 
# 查看镜像详细信息
root@ceph-1:~# rbd --image data-img1 --pool rbd-data info
rbd image 'data-img1':
        size 3 GiB in 768 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 1be2277ba2307
        block_name_prefix: rbd_data.1be2277ba2307
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features:
        flags:
        create_timestamp: Tue Dec  3 22:21:58 2024
        access_timestamp: Tue Dec  3 22:21:58 2024
        modify_timestamp: Tue Dec  3 22:21:58 2024
```

## 创建普通用户并授权
```bash
# 创建普通用户
root@ceph-1:~# cd /etc/ceph/
root@ceph-1:/etc/ceph# ceph auth get-or-create client.cuiliang mon 'allow r' osd 'allow rwx pool=rbd-data'
[client.cuiliang]
        key = AQCAFE9nbqjlLxAA8NiNT9Ag8lxu2gwwyWwpMg==
 
# 验证用户信息
root@ceph-1:/etc/ceph# ceph auth get client.cuiliang
[client.cuiliang]
        key = AQCAFE9nbqjlLxAA8NiNT9Ag8lxu2gwwyWwpMg==
        caps mon = "allow r"
        caps osd = "allow rwx pool=rbd-data"
 
# 创建keyring文件
root@ceph-1:/etc/ceph# ceph-authtool --create-keyring ceph.client.cuiliang
creating ceph.client.cuiliang
 
# 导出用户
root@ceph-1:/etc/ceph# ceph auth get client.cuiliang -o ceph.client.cuiliang.keyring
 
# 验证用户的keyring文件
root@ceph-1:/etc/ceph# cat ceph.client.cuiliang.keyring
[client.cuiliang]
        key = AQCAFE9nbqjlLxAA8NiNT9Ag8lxu2gwwyWwpMg==
        caps mon = "allow r"
        caps osd = "allow rwx pool=rbd-data"
```

# 客户端挂载资源
## 安装ceph客户端
Ubuntu客户端安装ceph-common

```bash
root@ceph-client:~# cat > /etc/apt/sources.list.d/ceph.list << EOF
deb https://mirrors.ustc.edu.cn/ceph/debian-reef/ focal main 
EOF
root@ceph-client:~# wget -q -O- 'https://mirrors.ustc.edu.cn/ceph/keys/release.asc' | sudo apt-key add -
root@ceph-client:~# apt update && apt install ceph-common -y
root@ceph-client:~# ceph -v
ceph version 18.2.4 (e7ad5345525c7aa95470c26863873b581076945d) reef (stable)
```

## 同步普通用户认证文件
从部署服务器发送集群配置文件和秘钥环文件

```bash
root@ceph-1:~# scp /etc/ceph/ceph.conf /etc/ceph/ceph.client.cuiliang.keyring 192.168.10.94:/etc/ceph/
```

## 验证客户端权限
```bash
root@ceph-client:~# ls /etc/ceph
ceph.client.cuiliang.keyring  ceph.conf
root@ceph-client:~# ceph --id cuiliang -s
  cluster:
    id:     402d9800-afef-11ef-92d7-9fbbd69ceccd
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum ceph-1,ceph-3,ceph-2 (age 11m)
    mgr: ceph-1.cuuabg(active, since 11m), standbys: ceph-3.uhtqme
    osd: 3 osds: 3 up (since 11m), 3 in (since 2h)

  data:
    pools:   3 pools, 65 pgs
    objects: 10 objects, 449 KiB
    usage:   135 MiB used, 150 GiB / 150 GiB avail
    pgs:     65 active+clean
```

## 映射rbd
使用普通用户映射

```bash
# 映射rbd
root@ceph-client:~# rbd --id cuiliang -p rbd-data map data-img1
/dev/rbd0
 
# 验证rbd
root@ceph-client:~# lsblk
NAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
loop0                       7:0    0 63.3M  1 loop /snap/core20/1828
loop1                       7:1    0 63.7M  1 loop /snap/core20/2434
loop2                       7:2    0 49.9M  1 loop /snap/snapd/18357
loop3                       7:3    0 91.9M  1 loop /snap/lxd/24061
loop4                       7:4    0 91.9M  1 loop /snap/lxd/29619
loop5                       7:5    0 44.3M  1 loop /snap/snapd/23258
sda                         8:0    0  100G  0 disk
├─sda1                      8:1    0    1M  0 part
├─sda2                      8:2    0    2G  0 part /boot
└─sda3                      8:3    0   98G  0 part
  └─ubuntu--vg-ubuntu--lv 253:0    0   49G  0 lvm  /
rbd0                      252:0    0    3G  0 disk
root@ceph-client:~# fdisk -l /dev/rbd0 
Disk /dev/rbd0: 3 GiB, 3221225472 bytes, 6291456 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 65536 bytes / 65536 bytes
```

## 格式化磁盘并挂载使用
```bash
# 格式化磁盘，xfs格式
root@ceph-client:~# mkfs.xfs /dev/rbd0
meta-data=/dev/rbd0              isize=512    agcount=8, agsize=98304 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=786432, imaxpct=25
         =                       sunit=16     swidth=16 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=16 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
 
# 新建/data目录并将rbd0挂载至该目录
root@ceph-client:~# mkdir /data
root@ceph-client:~# mount /dev/rbd0 /data/
# 向/data添加数据测试
root@ceph-client:~# cp -R /var/log/* /data/
root@ceph-client:~# df -h | grep rbd
/dev/rbd0                          3.0G  141M  2.9G   5% /data
```

## 管理端验证镜像状态
```bash
root@ceph-1:~# rbd ls -p rbd-data -l
NAME       SIZE   PARENT  FMT  PROT  LOCK
data-img1  3 GiB            2        excl # 施加锁文件，已被客户端映射
data-img2  4 GiB            2
```

## 验证ceph内核模块加载
挂载rbd之后系统内核会自动加载libceph模块

```bash
root@ceph-client:~# lsmod |grep ceph
libceph               327680  1 rbd
libcrc32c              16384  6 nf_conntrack,nf_nat,btrfs,xfs,raid456,libceph
```

## 开机自动挂载
```bash
# 设置开机挂载命令
root@ceph-client:~# cat /etc/ceph/mount.sh
#!/bin/bash
/usr/bin/rbd --id cuiliang -p rbd-data map data-img1
mount /dev/rbd0 /data/
root@ceph-client:~# chmod u+x /etc/ceph/mount.sh 
root@ceph-client:~# crontab -e
@reboot /etc/ceph/mount.sh
```

reboot重启验证

```bash
root@ceph-client:~# rbd showmapped
id  pool      namespace  image      snap  device   
0   rbd-data             data-img1  -     /dev/rbd0
root@ceph-client:~# df -h | grep rbd
/dev/rbd0                          3.0G  141M  2.9G   5% /data
```

# 客户端取消挂载
## 卸载rbd镜像
```bash
root@ceph-client:~# umount /data
root@ceph-client:~# rbd --id cuiliang -p rbd-data unmap data-img1
root@ceph-client:~# crontab -e
# @reboot /etc/ceph/mount.sh
```

## 删除rbd镜像
镜像删除后数据也会被删除而且是无法恢复，因此在执行删除操作的时候要慎重

```bash
# 删除存储池rbd-data中的data-img1镜像
root@ceph-1:/etc/ceph# rbd rm --pool rbd-data --image data-img1
Removing image: 100% complete...done.
# 验证镜像
root@ceph-1:/etc/ceph# rbd ls -p rbd-data -l
NAME       SIZE   PARENT  FMT  PROT  LOCK
data-img2  4 GiB            2
```

