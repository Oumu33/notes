# Ceph架构

> 来源: Ceph
> 创建时间: 2024-11-28T16:01:59+08:00
> 更新时间: 2026-01-11T09:43:24.291308+08:00
> 阅读量: 312 | 点赞: 0

---

# Ceph 简介
## 介绍
 Ceph 是一个开源的分布式存储系统，旨在提供高性能、高可扩展性和高可靠性的存储解决方案。它能够通过统一的存储平台同时支持块存储、对象存储和文件存储。Ceph 是一个去中心化的系统，能够在大规模分布式环境中工作，广泛应用于云计算和大数据处理领域。   

## 特点
+ 高可扩展性：Ceph 能够动态扩展，随着更多的存储节点的加入，集群的容量和性能也能够得到相应的扩展。
+ 高可靠性：Ceph 使用 CRUSH 算法来实现数据的分布式存储和副本管理，从而确保数据的高可靠性。当某个节点发生故障时，Ceph 可以自动恢复丢失的数据。
+ 自我修复：Ceph 会自动检测节点故障并进行数据恢复，确保数据的高可用性。
+ 灵活性：Ceph 可以同时提供块存储、对象存储和文件存储服务，适应多种存储需求。
+ 性能优化：Ceph 使用对象存储和 CRUSH 算法来优化存储性能，支持高吞吐量和低延迟的存储需求。

# Ceph 组件
## Monitor (MON)
Ceph Monitor 是 Ceph 集群的管理和协调组件，负责监控集群状态、管理集群配置、存储集群的映射数据等。集群的健康状态由 Monitor 节点监控，确保 Ceph 集群的正常运行。一个 Ceph 集群至少需要一个 MON 节点，但通常部署多个 MON 节点以实现高可用性。

## OSD (Object Storage Daemon)
Ceph OSD 是 Ceph 集群的核心存储组件，负责存储数据并处理数据的读写操作。每个 OSD 节点负责管理磁盘、进行数据的副本和恢复操作。OSD 节点的数量直接影响到 Ceph 集群的存储容量和性能。

## Manager (MGR)
Ceph Manager 负责提供集群的管理功能，包括集群的监控、统计、管理命令等。它提供了一个 RESTful API 接口，并且可以配合其他外部监控系统（如 Prometheus）进行集群监控。

## MDS (Metadata Server)
Ceph MDS 主要用于文件系统（CephFS）的元数据管理。它负责管理文件系统中的文件和目录结构信息，支持 POSIX 文件系统接口。当 Ceph 被用作文件存储时，MDS 是必须的组件。

# Ceph逻辑单元
+ pool（池）：pool是Ceph存储数据时的逻辑分区，它起到namespace的作用，在集群层面的逻辑切割。每个pool包含一定数量(可配置)的PG。
+ PG（Placement Group）：PG是一个逻辑概念，每个对象都会固定映射进一个PG中，所以当我们要寻找一个对象时，只需要先找到对象所属的PG，然后遍历这个PG就可以了，无需遍历所有对象。而且在数据迁移时，也是以PG作为基本单位进行迁移。PG的副本数量也可以看作数据在整个集群的副本数量。一个PG 包含多个 OSD 。引入 PG 这一层其实是为了更好的分配数据和定位数据。
+ OID：存储的数据都会被切分成对象（Objects）。每个对象都会有一个唯一的OID，由ino与ono生成，ino即是文件的File ID，用于在全局唯一标示每一个文件，而ono则是分片的编号，OID = ( ino + ono )= (File ID + File part number)，例如File Id = A，有两个分片，那么会产生两个OID，A01与A02。
+ PgID：首先使用静态hash函数对OID做hash取出特征码，用特征码与PG的数量去模，得到的序号则是PGID。
+ Object：Ceph最底层的存储单元是 Object对象，每个 Object 包含元数据和原始数据。

# pool、PG、OSD 关系
![](https://via.placeholder.com/800x600?text=Image+d9872341e54fcb8b)

+ 一个Pool里有很多PG，
+ 一个PG里包含一堆对象；一个对象只能属于一个PG；
+ PG有主从之分，一个PG分布在不同的OSD上（针对三副本类型）

# Ceph 设计
## 整体设计
![](https://via.placeholder.com/800x600?text=Image+94c5ff4d4056d366)

### 基础存储系统 RADOS
Reliable, Autonomic,Distributed Object Store，即可靠的、自动化的、分布式的对象存储

这就是一个完整的对象存储系统，所有存储在 Ceph 系统中的用户数据事实上最终都是由这一层来存储的。而 Ceph 的高可靠、高可扩展、高性能、高自动化等等特性本质上也是由这一层所提供的。

### 基础库 librados
这层的功能是对 RADOS 进行抽象和封装，并向上层提供 API，以便直接基于 RADOS（而不是整个 Ceph）进行应用开发。特别要注意的是，RADOS 是一个对象存储系统，因此，librados 实现的 API 也只是针对对象存储功能的。RADOS 采用 C++ 开发，所提供的原生 librados API 包括 C 和 C++ 两种。

### 高层应用接口
这层包括了三个部分：RADOS GW（RADOS Gateway）、 RBD（Reliable Block Device）和 Ceph FS（Ceph File System），其作用是在 librados 库的基础上提供抽象层次更高、更便于应用或客户端使用的上层接口。其中，RADOS GW 是一个提供与 Amazon S3 和 Swift 兼容的 RESTful API 的 gateway，以供相应的对象存储应用开发使用。RADOS GW 提供的 API 抽象层次更高，但功能则不如 librados 强大。

### 应用层
这层是不同场景下对于 Ceph 各个应用接口的各种应用方式，例如基于 librados 直接开发的对象存储应用，基于 RADOS GW 开发的对象存储应用，基于 RBD 实现的云硬盘等等。librados 和 RADOS GW 的区别在于，librados 提供的是本地 API，而 RADOS GW 提供的则是 RESTfulAPI。

由于 Swift 和 S3 支持的 API 功能近似，这里以 Swift 举例说明。Swift 提供的 API 功能主要包括：

+ 用户管理操作：用户认证、获取账户信息、列出容器列表等；
+ 容器管理操作：创建 / 删除容器、读取容器信息、列出容器内对象列表等；
+ 对象管理操作：对象的写入、读取、复制、更新、删除、访问许可设置、元数据读取或更新等。

## 逻辑架构
![](https://via.placeholder.com/800x600?text=Image+6e2025c939357e38)

## Ceph物理组件架构
RADOS是Ceph的核心，我们谈及的物理组件架构也只是RADOS的物理架构。

RADOS集群是由若干服务器组成，每一个服务器上都相应会运行RADOS的核心守护进程（OSD、MON、MDS）。具体守护进程的数量需要根据集群的规模和既定的规则来配置。

![](https://via.placeholder.com/800x600?text=Image+4f8aa2c2ed07528a)  


我们首先来看每一个集群节点上面的守护进程的主要作用：

OSD Daemon：两方面主要作用，一方面负责数据的处理操作，另外一方面负责监控本身以及其他OSD进程的健康状态并汇报给MON。OSD守护进程在每一个PG（Placement Group）当中，会有主次（Primary、Replication）之分，Primary主要负责数据的读写交互，Replication主要负责数据副本的复制。其故障处理机制主要靠集群的Crush算法来维持Primary和Replication之间的转化和工作接替。所有提供磁盘的节点上都要安装OSD 守护进程。

MON Daemon：三方面主要作用，首先是监控集群的全局状态（OSD Daemon Map、MON Map、PG Map、Crush Map），这里面包括了OSD和MON组成的集群配置信息，也包括了数据的映射关系。其次是管理集群内部状态，当OSD守护进程故障之后的系列恢复工作，包括数据的复制恢复。最后是与客户端的查询及授权工作，返回客户端查询的元数据信息以及授权信息。安装节点数目为2N+1，至少三个来保障集群算法的正常运行。

MDS Daemon：它是Ceph FS的元数据管理进程，主要是负责文件系统的元数据管理，它不需要运行在太多的服务器节点上。安装节点模式保持主备保护即可。

## Ceph数据对象组成
从客户端发出的一个文件请求，到Rados存储系统写入的过程当中会涉及到哪些逻辑对象，他们的关系又是如何的？首先，我们先来列出这些对象：

（1）文件（FILE）：用户需要存储或者访问的文件。对于一个基于Ceph开发的对象存储应用而言，这个文件也就对应于应用中的“对象”，也就是用户直接操作的“对象”。

（2）对象（Object）：RADOS所看到的“对象”。Object指的是最大size由RADOS限定（通常为2/4MB）之后RADOS直接进行管理的对象。因此，当上层应用向RADOS存入很大的file时，需要将file切分进行存储。

（3）PG（Placement Group）：PG是一个逻辑概念，阐述的是Object和OSD之间的地址映射关系，该集合里的所有对象都具有相同的映射策略；Object & PG，N：1的映射关系；PG & OSD，1：M的映射关系。一个Object只能映射到一个PG上，一个PG会被映射到多个OSD上。

（4）OSD（Object Storage Device）：存储对象的逻辑分区，它规定了数据冗余的类型和对应的副本分布策略；支持两种类型：副本和纠删码。

接下来，我们以更直观的方式来展现在Ceph当中数据是如何组织起来的：

![](https://via.placeholder.com/800x600?text=Image+46f2ab808bdf0523)

（1） File > Object

本次映射为首次映射，即将用户要操作的File，映射为RADOS能够处理的Object。

具体映射操作本质上就是按照Object的最大Size对File进行切分，每一个切分后产生的Object将获得唯一的对象标识Oid。Oid的唯一性必须得到保证，否则后续映射就会出现问题。

（2） Object > PG

完成从File到Object的映射之后， 就需要将每个 Object 独立地映射到 唯一的 PG 当中 去。

Hash（Oid）& Mask > PGid

根据以上算法， 首先是使用Ceph系统指定的一个静态哈希函数计算 Oid 的哈希值，将 Oid 映射成为一个近似均匀分布的伪随机值。然后，将这个伪随机值和 Mask 按位相与，得到最终的PG序号（ PG id）。根据RADOS的设计，给定PG的总数为 X（X= 2的整数幂）， Mask=X-1 。因此，哈希值计算和按位与操作的整体结果事实上是从所有 X 个PG中近似均匀地随机选择一个。基于这一机制，当有大量object和大量PG时，RADOS能够保证object和PG之间的近似均匀映射。

（3） PG > OSD

最后的 映射就是将PG映射到数据存储单元OSD。RADOS采用一个名为CRUSH的算法，将 PGid 代入其中，然后得到一组共 N 个OSD。这 N 个OSD即共同负责存储和维护一个PG中的所有 Object 。和“object -> PG”映射中采用的哈希算法不同，这个CRUSH算法的结果不是绝对不变的，而是受到其他因素的影响。

① 集群状态（Cluster Map）：系统中的OSD状态 。数量发生变化时， CLuster Map 可能发生变化，而这种变化将会影响到PG与OSD之间的映射。

② 存储策略配置。系统管理员可以指定承载同一个PG的3个OSD分别位于数据中心的不同服务器乃至机架上，从而进一步改善存储的可靠性。

到这里，可能大家又会有一个问题“为什么这里要用CRUSH算法，而不是HASH算法？”

这一次映射，我们对映射算法有两种要求：

一方面，算法必须能够随着系统的节点数量位置的变化，而具备动态调整特性，保障在变化的环境当中仍然可以保持数据分布的均匀性；另外一方面还要有相对的稳定性，也就是说大部分的映射关系不会因为集群的动态变化发生变化，保持一定的稳定性。

而CRUSH算法正是符合了以上的两点要求，所以最终成为Ceph的核心算法。


