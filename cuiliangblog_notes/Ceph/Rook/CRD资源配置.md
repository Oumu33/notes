# CRD资源配置

> 分类: Ceph > Rook
> 更新时间: 2026-01-10T23:35:20.054520+08:00

---

# 块存储
## 资源池pool
参考文档：[https://rook.io/docs/rook/latest-release/CRDs/Block-Storage/ceph-block-pool-crd/?h=cephblockpool](https://rook.io/docs/rook/latest-release/CRDs/Block-Storage/ceph-block-pool-crd/?h=cephblockpool)

### 副本型 pool
获得最佳性能，同时增加冗余，将 Ceph 配置为在多个节点上创建数据的三个完整副本。

```yaml
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3
  deviceClass: hdd
```

### 纠删码型 pool
降低总体存储容量要求，同时还会通过使用纠删码来增加冗余。

```yaml
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: ecpool
  namespace: rook-ceph
spec:
  failureDomain: osd
  erasureCoded:
    dataChunks: 2
    codingChunks: 1
  deviceClass: hdd
```

# 共享文件存储
## 文件存储
参考文档：[https://rook.io/docs/rook/latest-release/CRDs/Shared-Filesystem/ceph-filesystem-crd/?h=cephfilesystem](https://rook.io/docs/rook/latest-release/CRDs/Shared-Filesystem/ceph-filesystem-crd/?h=cephfilesystem)

```yaml
apiVersion: ceph.rook.io/v1 # 定义资源的 API 版本为 ceph.rook.io/v1
kind: CephFilesystem # 定义资源的类型为 CephFilesystem，用于创建和管理 Ceph 文件系统
metadata:
  name: myfs # 文件系统的名称，唯一标识该资源
  namespace: rook-ceph # 文件系统所在的命名空间
spec:
  metadataPool: # 定义元数据池的配置
    failureDomain: host # 设置数据副本分布在不同主机节点上
    replicated: # 使用数据复制模式存储元数据
      size: 3 # 元数据池的副本数量为 3
  dataPools: # 定义数据池的配置
    - name: replicated # 数据池的名称
      failureDomain: host # 设置数据副本分布在不同主机节点上
      replicated: # 使用数据复制模式存储数据
        size: 3 # 数据池的副本数量为 3
  preserveFilesystemOnDelete: true # 当删除 CephFilesystem 时保留其底层的数据池和元数据池
  metadataServer: # 配置 Ceph 文件系统的元数据服务器（MDS）
    activeCount: 1 # 活跃元数据服务器的数量
    activeStandby: true # 启用元数据服务器的活跃-备用模式
    # A key/value list of annotations
    annotations:
    #  key: value # 可选，添加自定义注解到元数据服务器 Pod
    placement: # 定义元数据服务器 Pod 的调度策略
    #  nodeAffinity: # 设置节点亲和性规则
    #    requiredDuringSchedulingIgnoredDuringExecution:
    #      nodeSelectorTerms:
    #      - matchExpressions:
    #        - key: role # 根据标签选择符合条件的节点
    #          operator: In
    #          values:
    #          - mds-node
    #  tolerations: # 设置容忍规则，使 Pod 可以调度到带有特定污点的节点
    #  - key: mds-node
    #    operator: Exists
    #  podAffinity: # 定义 Pod 亲和性规则
    #  podAntiAffinity: # 定义 Pod 反亲和性规则
    #  topologySpreadConstraints: # 设置拓扑分布约束
    resources: # 配置元数据服务器 Pod 的资源限制和请求
    #  limits: # 设置资源的最大限制
    #    memory: "1024Mi"
    #  requests: # 设置资源的最小请求
    #    cpu: "500m"
    #    memory: "1024Mi"
```

# 对象存储
## 对象存储ObjectStore
参考文档：[https://rook.io/docs/rook/latest-release/CRDs/Object-Storage/ceph-object-store-crd/?h=cephobjectstore](https://rook.io/docs/rook/latest-release/CRDs/Object-Storage/ceph-object-store-crd/?h=cephobjectstore)

```yaml
apiVersion: ceph.rook.io/v1 
kind: CephObjectStore 
metadata:
  name: my-store # CephObjectStore 的名称为 my-store
  namespace: rook-ceph # CephObjectStore 所属的命名空间为 rook-ceph
spec:
  metadataPool: # 配置元数据池
    failureDomain: host # 元数据副本分布的故障域为主机级别
    replicated: # 使用数据副本方式存储元数据
      size: 3 # 元数据池副本数量为 3
  dataPool: # 配置数据池
    failureDomain: host # 数据副本分布的故障域为主机级别
    erasureCoded: # 使用纠删码方式存储数据
      dataChunks: 2 # 数据块数量为 2
      codingChunks: 1 # 纠删码块数量为 1
  preservePoolsOnDelete: true # 删除 CephObjectStore 时是否保留池（true 表示保留）
  gateway: # 配置对象网关（RGW）
    # sslCertificateRef: # 注释掉的字段，用于指定 SSL 证书的引用名称
    # caBundleRef: # 注释掉的字段，用于指定 CA 证书的引用名称
    port: 80 # 对象网关的 HTTP 端口号
    # securePort: 443 # 注释掉的字段，用于设置 HTTPS 端口号
    instances: 1 # 网关实例数量为 1
    # A key/value list of annotations
    annotations: # 配置网关 Pod 的注解（当前为空）
    #  key: value # 示例注解（注释掉）
    placement: # 网关实例的调度策略（当前为空）
    #  nodeAffinity: # 注释掉的字段，用于定义节点亲和性
    #    requiredDuringSchedulingIgnoredDuringExecution: # 强制要求在调度时满足条件
    #      nodeSelectorTerms: # 节点选择条件
    #      - matchExpressions: # 匹配规则
    #        - key: role # 节点标签的键
    #          operator: In # 运算符为 In（表示值在列表中）
    #          values: # 匹配的值列表
    #          - rgw-node # 匹配值为 rgw-node
    #  tolerations: # 注释掉的字段，用于定义节点容忍度
    #  - key: rgw-node # 容忍的键为 rgw-node
    #    operator: Exists # 运算符为 Exists（表示键存在即可）
    #  podAffinity: # 注释掉的字段，用于定义 Pod 亲和性
    #  podAntiAffinity: # 注释掉的字段，用于定义 Pod 反亲和性
    #  topologySpreadConstraints: # 注释掉的字段，用于设置拓扑分布约束
    resources: # 配置网关实例的资源限制和请求（当前为空）
    #  limits: # 注释掉的字段，指定资源的上限
    #    memory: "1024Mi" # 内存上限为 1024Mi
    #  requests: # 注释掉的字段，指定资源的最低请求
    #    cpu: "500m" # CPU 请求为 500m
    #    memory: "1024Mi" # 内存请求为 1024Mi
  #zone: # 注释掉的字段，用于指定区域配置
    #name: zone-a # 区域名称为 zone-a
  #hosting: # 注释掉的字段，用于配置托管的端点
  #  advertiseEndpoint: # 配置对外公布的端点信息
  #    dnsName: "mystore.example.com" # 公布的 DNS 名称
  #    port: 80 # 公布的端点端口号
  #    useTls: false # 是否使用 TLS（false 表示不使用）
  #  dnsNames: # 注释掉的字段，用于指定额外的 DNS 名称
  #    - "mystore.example.org" # 额外的 DNS 名称
```

## 对象存储Bucket
参考文档：[https://rook.io/docs/rook/latest-release/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim/?h=objectbucketclaim#obc-custom-resource](https://rook.io/docs/rook/latest-release/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim/?h=objectbucketclaim#obc-custom-resource)

Rook提供了两种云原⽣的⽅式创建Bucket 

+ Object Bucket Claim 
+ Object Bucket 

要使⽤Object Bucket Claim，⾸先需要定义⼀个StorageClass，该StorageClass负责bucket的创建过程，创建的时候会通过configmap提供⼀个配置⽂件存储访问bucket的endpoint，同时还会通过Secret提供访问bucket的认证信息

+ StorageClass的定义 

```yaml
apiVersion: storage.k8s.io/v1 
kind: StorageClass # 定义资源的类型为 StorageClass
metadata:
  name: rook-ceph-bucket # StorageClass 的名称为 rook-ceph-bucket
  labels: # 定义 StorageClass 的标签
    aws-s3/object # 标签 key 为 aws-s3/object，值为空
provisioner: rook-ceph.ceph.rook.io/bucket # 指定使用的存储提供者（provisioner），这里是 rook-ceph 的 bucket 提供者
parameters: # 配置 StorageClass 的参数
  objectStoreName: my-store # 使用的 Ceph 对象存储名称为 my-store
  objectStoreNamespace: rook-ceph # Ceph 对象存储所在的命名空间为 rook-ceph
  region: us-west-1 # 配置对象存储所在的区域（region），这里是 us-west-1
  bucketName: ceph-bucket # 配置用于存储对象的桶（bucket）名称为 ceph-bucket
reclaimPolicy: Delete # 当 PersistentVolume（PV）释放时的回收策略，这里设置为删除（Delete）
```

+ Object Bucket Claim 的定义

```yaml
apiVersion: objectbucket.io/v1alpha1 
kind: ObjectBucketClaim # 定义资源的类型为 ObjectBucketClaim，用于请求对象存储桶
metadata:
  name: ceph-bucket # ObjectBucketClaim 的名称，表示该资源的唯一标识
  namespace: rook-ceph #  ObjectBucketClaim 所在的命名空间
spec:
  bucketName: #  指定存储桶的固定名称（该字段未填写具体值时，系统根据 generateBucketName 自动生成）
  generateBucketName: photo-booth # 指定存储桶的前缀名，系统根据此值生成唯一名称
  storageClassName: rook-ceph-bucket # 关联的 StorageClass 名称，定义存储桶的提供者和配置
  additionalConfig: #  存储桶的额外配置
    maxObjects: "1000" # 限制该存储桶中允许的最大对象数量
    maxSize: "2G" # 限制该存储桶的最大容量
    bucketMaxObjects: "3000" # 存储桶的最大对象数量限制（可能用于特定存储配置的额外参数）
    bucketMaxSize: "4G" # 存储桶的最大容量限制（可能用于特定存储配置的额外参数）

```

## 对象存储用户
参考文档：[https://rook.io/docs/rook/latest-release/CRDs/Object-Storage/ceph-object-store-user-crd/?h=cephobjectstoreuser](https://rook.io/docs/rook/latest-release/CRDs/Object-Storage/ceph-object-store-user-crd/?h=cephobjectstoreuser)

```yaml
apiVersion: ceph.rook.io/v1 # 定义资源的 API 版本为 ceph.rook.io/v1
kind: CephObjectStoreUser # 定义资源的类型为 CephObjectStoreUser，用于创建对象存储用户
metadata:
  name: my-user # CephObjectStoreUser 的名称为 my-user
  namespace: rook-ceph # CephObjectStoreUser 所属的命名空间为 rook-ceph
spec:
  store: my-store # 指定关联的 CephObjectStore 的名称为 my-store
  displayName: my-display-name # 为用户指定一个显示名称，便于识别
  quotas: # 配置用户的配额限制
    maxBuckets: 100 # 用户最多可以创建的存储桶数量为 100
    maxSize: 10G # 用户允许使用的最大存储容量为 10G
    maxObjects: 10000 # 用户允许存储的最大对象数量为 10,000
  capabilities: # 配置用户在对象存储中的权限
    user: "*" # 用户级别的权限，`*` 表示具有所有权限
    bucket: "*" # 存储桶级别的权限，`*` 表示具有所有权限
```

# 其他资源
## 主机存储集群
参考文档：[https://rook.io/docs/rook/latest-release/CRDs/Cluster/host-cluster/](https://rook.io/docs/rook/latest-release/CRDs/Cluster/host-cluster/)

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster # 定义资源类型为 CephCluster，用于创建和管理 Ceph 集群
metadata:
  name: rook-ceph # Ceph 集群的名称
  namespace: rook-ceph # 集群所在的命名空间
spec:
  cephVersion: # 指定使用的 Ceph 版本和镜像
    image: quay.io/ceph/ceph:v19.2.0 # Ceph 镜像的路径及版本
  dataDirHostPath: /var/lib/rook # 在主机上存储集群数据的路径
  mon: # 配置监视器（MON）服务
    count: 3 # MON 服务的副本数，建议为奇数以便实现仲裁
    allowMultiplePerNode: false # 是否允许一个节点运行多个 MON 服务，`false` 表示每个节点只运行一个 MON 服务
  dashboard: # 配置 Ceph 仪表盘
    enabled: true # 启用仪表盘以提供图形化管理界面
  storage: # 集群级别的存储配置
    useAllNodes: false # 是否使用所有节点作为存储节点，`false` 表示仅使用指定的节点
    useAllDevices: false # 是否使用所有存储设备，`false` 表示仅使用指定的设备
    deviceFilter: # 可选，根据设备名称筛选
    config: # 集群级别的存储配置
      metadataDevice: # 元数据设备，通常用于存储元数据
      databaseSizeMB: "1024" # OSD 的蓝店数据库大小（MB），建议在小硬盘环境下指定
    nodes: # 指定存储节点和设备配置
    - name: "172.17.4.201" # 存储节点的主机名或 IP 地址
      devices: # 指定节点上要使用的存储设备
      - name: "sdb" # 使用整个存储设备 `sdb`
      - name: "sdc1" # 使用设备的特定分区 `sdc1`（不应有文件系统）
      - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # 可以使用设备名称或显式的 UDEV 链接
      config: # 节点级别的配置，会覆盖集群级别的配置
    - name: "172.17.4.301" # 另一个存储节点
      deviceFilter: "^sd." # 使用正则表达式匹配设备名称，例如所有 `sd` 开头的设备
```

## ceph 客户端
参考文档：[https://rook.io/docs/rook/latest-release/CRDs/ceph-client-crd/](https://rook.io/docs/rook/latest-release/CRDs/ceph-client-crd/)

```yaml
apiVersion: ceph.rook.io/v1
kind: CephClient # 定义资源类型为 CephClient，用于管理 Ceph 客户端用户
metadata:
  name: example # Ceph 客户端的名称，用于标识客户端
  namespace: rook-ceph # 该资源所属的命名空间
spec:
  caps: # 指定客户端用户的权限配置
    mon: 'profile rbd, allow r' # 在 MON（监视器）组件上的权限，允许只读访问和 RBD（RADOS 块设备）相关操作
    osd: 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd-read-only pool=images' 
    # 在 OSD（对象存储守护进程）上的权限：
    # - `profile rbd pool=volumes`：允许对名为 `volumes` 的存储池进行 RBD 操作
    # - `profile rbd pool=vms`：允许对名为 `vms` 的存储池进行 RBD 操作
    # - `profile rbd-read-only pool=images`：允许对名为 `images` 的存储池进行只读 RBD 操作
```

## NFS 服务
参考文档：[https://rook.io/docs/rook/latest-release/CRDs/ceph-nfs-crd/](https://rook.io/docs/rook/latest-release/CRDs/ceph-nfs-crd/)

```yaml
apiVersion: ceph.rook.io/v1
kind: CephNFS # 定义资源类型为 CephNFS，用于创建 Ceph NFS 服务
metadata:
  name: my-nfs # NFS 服务的名称
  namespace: rook-ceph # 该资源所属的命名空间
spec:
  server: # NFS 服务的配置
    active: 1 # 启动一个 NFS 服务实例，值为 1 表示启用
    placement: # 配置 NFS Pod 的节点亲和性
      nodeAffinity: # 节点亲和性，指定 Pod 应该调度到哪些节点
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - nfs-node # 该 Pod 只能调度到具有 "role=nfs-node" 标签的节点
      topologySpreadConstraints: # 配置拓扑均衡，控制 Pod 的分布
      tolerations: # 容忍节点上存在的 taints
      - key: nfs-node
        operator: Exists # 容忍所有标有 "nfs-node" taint 的节点
      podAffinity: # Pod 亲和性，指定 Pod 应该和哪些 Pod 一起调度
      podAntiAffinity: # Pod 反亲和性，指定 Pod 应该避免与哪些 Pod 调度到一起

    annotations: # NFS 服务的注解
      my-annotation: something
    labels: # NFS 服务的标签
      my-label: something
    resources: # NFS 服务的资源请求和限制
      limits:
        memory: "8Gi" # 设置内存的限制为 8Gi
      requests:
        cpu: "3" # 请求的 CPU 核数为 3
        memory: "8Gi" # 请求的内存为 8Gi
    priorityClassName: "" # 设置 Pod 的优先级类为空
    logLevel: NIV_INFO # 设置日志级别为信息级别

  security: # 安全配置
    kerberos: # Kerberos 配置
      principalName: "nfs" # 设置 Kerberos 的主体名称为 nfs
      domainName: "DOMAIN1.EXAMPLE.COM" # 设置 Kerberos 的域名

      configFiles: # Kerberos 配置文件
        volumeSource:
          configMap:
            name: my-krb5-config-files # 配置文件的来源是一个名为 my-krb5-config-files 的 ConfigMap

      keytabFile: # Kerberos 密钥表文件
        volumeSource:
          secret:
            secretName: my-nfs-keytab # 密钥表文件来源于名为 my-nfs-keytab 的 Secret
            defaultMode: 0600 # 密钥表文件的权限模式设置为 0600

    sssd: # SSSD 配置
      sidecar: # SSSD sidecar 容器配置
        image: registry.access.redhat.com/rhel7/sssd:latest # 使用 RedHat 提供的 SSSD 容器镜像
        sssdConfigFile: # 配置 SSSD 的配置文件
          volumeSource:
            configMap:
              name: my-nfs-sssd-config # 配置文件的来源是一个名为 my-nfs-sssd-config 的 ConfigMap
              defaultMode: 0600 # 配置文件的权限模式设置为 0600
        debugLevel: 0 # 设置调试级别为 0（最低级别）
        resources: {} # SSSD sidecar 容器的资源配置，空表示没有指定特定的资源请求和限制
```

