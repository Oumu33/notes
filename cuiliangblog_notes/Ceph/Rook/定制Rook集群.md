# 定制Rook集群

> 分类: Ceph > Rook
> 更新时间: 2026-01-10T23:35:19.035765+08:00

---

# Operator 参数调整
## 调整osd发现参数  
默认配置中使⽤了所有的节点和所有的设备作为 osd 使用，rook会⾃动去扫描所有节点的设备，⼀旦有节点或者磁盘 添加进来，会⾃动将其添加到Ceph集群中，operator是通过启动rook-discover⼀个容器定期去扫描， 参数为ROOK_DISCOVER_DEVICES_INTERVA ， 间隔默认是60m  

```bash
vim operator.yaml
# rook 多长时间检测一次设备，检测设备的目的是为了发现新设备，并将符合条件的设备设置为 osd 加入 ceph
ROOK_DISCOVER_DEVICES_INTERVAL: "1m"
```

## 调整 kubelet 路径
如果自定义 k8s 节点 kubelet 数据目录路径到数据目录，需要修改 operator 配置。

```yaml
vim operator.yaml
# kubelet directory path, if kubelet configured to use other than /var/lib/kubelet path.
ROOK_CSI_KUBELET_DIR_PATH: "/data/kubelet"
```

# Cluster 参数调整
> 如果之前已经部署 rook 集群，如果涉及到 osd 调整，请先卸载后重新部署。
>

## 调度规则调整
默认情况下会扫描所有 k8s 节点，只要存在未被格式化的磁盘均会加入 ceph 集群 osd 资源，很多时候可能需要指定哪些节点的磁盘用于充当存储节点。

1. 节点添加标签

```bash
# kubectl label nodes master1 storage=true            
node/master1 labeled
# kubectl label nodes master2 storage=true
node/master2 labeled
# kubectl label nodes master3 storage=true
node/master3 labeled
```

2. 修改 cluster.yaml，新增节点调度规则。

```yaml
  storage: # 取消默认所有节点所有磁盘用于存储节点
    useAllNodes: false
    useAllDevices: false
  placement: # 指定存储节点调度规则
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: storage
              operator: In
              values:
              - "true"
  nodes: # 指定哪些节点的磁盘加入存储资源池
  - name: "master1"
    devices:
      - name: "nvme0n2"
  - name: "master2"
    devices:
      - name: "nvme0n2"
  - name: "master3"
    devices:
      - name: "nvme0n2"
```

3. 创建集群。

```yaml
# kubectl apply -f cluster.yaml 
```

4. 验证

```bash
# 查看pod资源分布、mon、osd、mgr资源只分布在了master节点
kubectl get pod -n rook-ceph -o wide
NAME                                                READY   STATUS      RESTARTS   AGE     IP               NODE      NOMINATED NODE   READINESS GATES
csi-cephfsplugin-6hvdv                              3/3     Running     0          3m19s   192.168.10.156   work3     <none>           <none>
csi-cephfsplugin-jv84p                              3/3     Running     0          3m19s   192.168.10.153   master3   <none>           <none>
csi-cephfsplugin-mrprf                              3/3     Running     0          3m19s   192.168.10.154   work1     <none>           <none>
csi-cephfsplugin-provisioner-6d866886bf-k9rk9       6/6     Running     0          3m19s   10.244.5.88      work3     <none>           <none>
csi-cephfsplugin-provisioner-6d866886bf-zcqhn       6/6     Running     0          3m19s   10.244.3.104     work1     <none>           <none>
csi-cephfsplugin-sgqsm                              3/3     Running     0          3m19s   192.168.10.155   work2     <none>           <none>
csi-cephfsplugin-wblsv                              3/3     Running     0          3m19s   192.168.10.152   master2   <none>           <none>
csi-cephfsplugin-xhc7v                              3/3     Running     0          3m19s   192.168.10.151   master1   <none>           <none>
csi-rbdplugin-29qcd                                 3/3     Running     0          3m19s   192.168.10.156   work3     <none>           <none>
csi-rbdplugin-6b46g                                 3/3     Running     0          3m19s   192.168.10.154   work1     <none>           <none>
csi-rbdplugin-75bnq                                 3/3     Running     0          3m19s   192.168.10.155   work2     <none>           <none>
csi-rbdplugin-j6qz6                                 3/3     Running     0          3m19s   192.168.10.151   master1   <none>           <none>
csi-rbdplugin-pdbcj                                 3/3     Running     0          3m19s   192.168.10.153   master3   <none>           <none>
csi-rbdplugin-provisioner-5f87858d77-894gz          6/6     Running     0          3m19s   10.244.4.118     work2     <none>           <none>
csi-rbdplugin-provisioner-5f87858d77-c2dgv          6/6     Running     0          3m19s   10.244.5.87      work3     <none>           <none>
csi-rbdplugin-t445x                                 3/3     Running     0          3m19s   192.168.10.152   master2   <none>           <none>
rook-ceph-crashcollector-master1-6d4fddb76b-tpl4t   1/1     Running     0          2m      10.244.0.54      master1   <none>           <none>
rook-ceph-crashcollector-master2-649885f4dc-h8dlr   1/1     Running     0          2m      10.244.1.46      master2   <none>           <none>
rook-ceph-crashcollector-master3-74c7d76644-m6lgf   1/1     Running     0          2m9s    10.244.2.46      master3   <none>           <none>
rook-ceph-exporter-master1-66657559dd-mmmsn         1/1     Running     0          117s    10.244.0.55      master1   <none>           <none>
rook-ceph-exporter-master2-75bbbf5dd5-rvf49         1/1     Running     0          117s    10.244.1.47      master2   <none>           <none>
rook-ceph-exporter-master3-559ff777f8-4859z         1/1     Running     0          2m9s    10.244.2.47      master3   <none>           <none>
rook-ceph-mgr-a-7964985548-z6k8g                    3/3     Running     0          2m29s   10.244.0.49      master1   <none>           <none>
rook-ceph-mgr-b-fd57d6489-7g8nx                     3/3     Running     0          2m28s   10.244.1.41      master2   <none>           <none>
rook-ceph-mon-a-6d8c94df78-wrq6r                    2/2     Running     0          3m11s   10.244.0.48      master1   <none>           <none>
rook-ceph-mon-b-778dcdb4dd-jcrwf                    2/2     Running     0          2m49s   10.244.1.40      master2   <none>           <none>
rook-ceph-mon-c-67b659ddf4-pgz9l                    2/2     Running     0          2m39s   10.244.2.45      master3   <none>           <none>
rook-ceph-operator-7dfdf865d5-ccntv                 1/1     Running     0          5m38s   10.244.5.86      work3     <none>           <none>
rook-ceph-osd-0-7df5496557-v7scw                    2/2     Running     0          2m      10.244.0.53      master1   <none>           <none>
rook-ceph-osd-1-b99ddc8ff-xpfnz                     2/2     Running     0          2m      10.244.1.45      master2   <none>           <none>
rook-ceph-osd-2-6ddbff86ff-pm25f                    2/2     Running     0          119s    10.244.2.49      master3   <none>           <none>
rook-ceph-osd-prepare-master1-h8psn                 0/1     Completed   0          2m6s    10.244.0.52      master1   <none>           <none>
rook-ceph-osd-prepare-master2-dqb6p                 0/1     Completed   0          2m6s    10.244.1.44      master2   <none>           <none>
rook-ceph-osd-prepare-master3-nqrhv                 0/1     Completed   0          2m5s    10.244.2.48      master3   <none>           <none>

# 查看osd信息，只使用了master节点的磁盘资源。
# kubectl exec -it -n rook-ceph rook-ceph-tools-699dcdd8bb-4ffhl -- bash
bash-5.1$ ceph -s
  cluster:
    id:     b9d61b9d-510d-4605-ba08-eb7774b21cb2
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum a,b,c (age 3m)
    mgr: a(active, since 2m), standbys: b
    osd: 3 osds: 3 up (since 2m), 3 in (since 2m)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 2 objects, 449 KiB
    usage:   81 MiB used, 300 GiB / 300 GiB avail
    pgs:     1 active+clean
 
bash-5.1$ ceph osd status
ID  HOST      USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE      
 0  master1  27.1M  99.9G      0        0       0        0   exists,up  
 1  master2  26.9M  99.9G      0        0       0        0   exists,up  
 2  master3  26.9M  99.9G      0        0       0        0   exists,up
```

## 调整网络模式
默认情况下 rook 创建的 pod 都是Bridge 网络模式，仅允许 k8s 集群内部资源连接访问，如果 k8s 集群外部的资源也需要访问 ceph 集群，就需要将网络模式修改为 host 模式。

1. 查看先前的 mon 地址，均为 pod 地址，集群外部无法访问。

```bash
# kubectl exec -it -n rook-ceph rook-ceph-tools-699dcdd8bb-4ffhl -- bash
bash-5.1$ ceph mon dump
epoch 3
fsid b9d61b9d-510d-4605-ba08-eb7774b21cb2
last_changed 2024-12-24T10:08:45.582011+0000
created 2024-12-24T10:08:12.001933+0000
min_mon_release 19 (squid)
election_strategy: 1
0: [v2:10.105.41.106:3300/0,v1:10.105.41.106:6789/0] mon.a
1: [v2:10.108.81.29:3300/0,v1:10.108.81.29:6789/0] mon.b
2: [v2:10.106.65.126:3300/0,v1:10.106.65.126:6789/0] mon.c
dumped monmap epoch 3
```

2. 修改网络模式

```bash
# vim cluster.yaml
    # enable host networking
    provider: host
# kubectl apply -f cluster.yaml
```

3. 查看新的 mon 地址，已经切换为节点所在的物理机 ip 地址。

```bash
bash-5.1$ ceph mon dump
epoch 9
fsid b9d61b9d-510d-4605-ba08-eb7774b21cb2
last_changed 2024-12-25T07:26:16.398700+0000
created 2024-12-24T10:08:12.001933+0000
min_mon_release 19 (squid)
election_strategy: 1
0: [v2:192.168.10.153:3300/0,v1:192.168.10.153:6789/0] mon.d
1: [v2:192.168.10.152:3300/0,v1:192.168.10.152:6789/0] mon.e
2: [v2:192.168.10.151:3300/0,v1:192.168.10.151:6789/0] mon.f
dumped monmap epoch 9
```

