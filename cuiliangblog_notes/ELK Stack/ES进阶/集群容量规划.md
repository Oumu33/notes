# 集群容量规划

> 分类: ELK Stack > ES进阶
> 更新时间: 2026-01-10T23:33:39.782831+08:00

---

# 数据量预估
第一，问自己几个问题：

+ 您每天将索引多少原始数据（GB）？
+ 您将保留数据多少天？
+ 每天增量数据是多少？
+ 您将强制执行多少个副本分片？
+ 您将为每个数据节点分配多少内存？
+ 您的内存：数据比率是多少？

第二，预留存储以备错误。(Elastic 官方推荐经验值）

+ 预留 15%警戒磁盘水位空间。
+ 为错误余量和后台活动预留+ 5％。
+ 保留等效的数据节点以处理故障。

第三，容量预估计算方法如下：

+ 总数据量（GB） = 原始数据量（GB） /每天 X 保留天数 X 净膨胀系数 X （副本 + 1）
+ 磁盘存储（GB） = 总数据量（GB）* ( 1 + 0.15 + 0.05)
+ 数据节点 = 向上取证（磁盘存储（GB）/ 每个数据节点的内存量 / 内存：数据比率）+ 1

Tips：腾讯云 在 2019 4 月的 meetup 分享中建议：磁盘容量大小 = 原始数据大小 * 3.38。

# 分片数预估
<font style="color:rgb(62, 62, 62);">当es集群的节点数大于索引的分片数时，集群将无法通过水平扩展提升集群的性能。而分片数过多，对于聚合查询以及集群的元数据管理也都有影响，分片数量较多时</font>

<font style="color:rgb(62, 62, 62);">优点：</font>

1. <font style="color:rgb(62, 62, 62);">新数据节点加入后，分片会自动分配扩展性强</font>
2. <font style="color:rgb(62, 62, 62);">数据写入分散到不同节点，减少每个节点的压力</font>

<font style="color:rgb(62, 62, 62);">缺点：</font>

1. <font style="color:rgb(62, 62, 62);">每个分片就是一个lucene索引，这个索引会占用较多的机器资源，造成额外的开销</font>
2. <font style="color:rgb(62, 62, 62);">搜索时，需要到每个分片获取数据</font>
3. <font style="color:rgb(62, 62, 62);">master节点维护分片、索引的元信息，造成master节点管理成本过高。</font>

<font style="color:rgb(62, 62, 62);">通常建议一个集群总分片数小于10w。</font>

<font style="color:rgb(62, 62, 62);">如何设计分片的数量呢？</font><font style="color:rgb(62, 62, 62);">一个分片保持多大的数据量比较合适呢？</font>

## 主分片
<font style="color:rgb(62, 62, 62);">我们需要根据使用场景来设置：</font>

+ <font style="color:rgb(62, 62, 62);">日志类，其特点为：</font><font style="color:rgb(62, 62, 62);">写入频繁，查询较少</font><font style="color:rgb(62, 62, 62);">单个分片不要大于50G</font>
+ <font style="color:rgb(62, 62, 62);">搜索类，其特点为：写入较少，查询频繁单个分片不超过20G</font>
+ 每 GB JVM 堆内存支持的分片数不超过 20 个。

<font style="color:rgb(62, 62, 62);">避免使用非常大的分片，因为这会对群集从故障中恢复的能力产生负面影响。</font><font style="color:rgb(62, 62, 62);">而每个分片也会消耗相应的文件句柄,内存和CPU资源，分片太多会互相竞争，影响性能。</font>

<font style="color:rgb(62, 62, 62);">主分片数一旦确定就无法更改，只能新建创建并对数据进行重新索引(reindex)，虽然reindex会比较耗时，但至少能保证你不会停机。</font><font style="color:rgb(62, 62, 62);">所以我们一定要科学的设计分片数。</font>

<font style="color:rgb(62, 62, 62);">这里摘录于官方关于分片大小的建议：</font>

小的分片会造成小的分段，从而会增加开销。我们的目的是将平均分片大小控制在几 GB 到几十 GB 之间。对于基于时间的数据的使用场景来说，通常将分片大小控制在 20GB 到 40GB 之间。

由于每个分片的开销取决于分段的数量和大小，因此通过 forcemerge 操作强制将较小的分段合并为较大的分段，这样可以减少开销并提高查询性能。 理想情况下，一旦不再向索引写入数据，就应该这样做。 请注意，这是一项比较耗费性能和开销的操作，因此应该在非高峰时段执行。

我们可以在节点上保留的分片数量与可用的堆内存成正比，但 <font style="color:rgb(154, 154, 154);">ElasticSearch</font>没有强制的固定限制。 一个好的经验法则是确保每个节点的分片数量低于每GB堆内存配置20到25个分片。 因此，具有30GB堆内存的节点应该具有最多600-750个分片，但是低于该限制可以使其保持更好。 这通常有助于集群保持健康。

如果担心数据的快速增长, 建议根据这条限制: ‍ElasticSearch推荐的最大JVM堆空间 是 30~32G, 所以把分片最大容量限制为 30GB, 然后再对分片数量做合理估算。例如, 如果的数据能达到 200GB, 则最多分配7到8个分片。

如果是基于日期的索引需求, 并且对索引数据的搜索场景非常少。

也许这些索引量将达到成百上千, 但每个索引的数据量只有1GB甚至更小对于这种类似场景, 建议是只需要为索引分配1个分片。如果使用ES7的默认配置(3个分片), 并且使用 Logstash 按天生成索引, 那么 6 个月下来, 拥有的分片数将达到 540个. 再多的话, 你的集群将难以工作--除非提供了更多(例如15个或更多)的节点。想一下, 大部分的 Logstash 用户并不会频繁的进行搜索, 甚至每分钟都不会有一次查询. 所以这种场景, 推荐更为经济使用的设置. 在这种场景下, 搜索性能并不是第一要素, 所以并不需要很多副本。

维护单个副本用于数据冗余已经足够。不过数据被不断载入到内存的比例相应也会变高。如果索引只需要一个分片, 那么使用 Logstash 的配置可以在 3 节点的集群中维持运行 6 个月。当然你至少需要使用 4GB 的内存, 不过建议使用 8GB, 因为在多数据云平台中使用 8GB 内存会有明显的网速以及更少的资源共享。

## 副本分片
<font style="color:rgb(62, 62, 62);">主分片与副本都能处理查询请求，它们的唯一区别在于只有主分片才能处理索引请求。副本对搜索性能非常重要，同时用户也可在任何时候添加或删除副本。额外的副本能给带来更大的容量，更高的呑吐能力及更强的故障恢复能力</font>

## 小结<font style="color:rgba(0, 0, 0, 0.9);"></font>
<font style="color:rgb(62, 62, 62);">根据实际经验我们稍微总结下：</font>

<font style="color:rgb(62, 62, 62);">对于数据量较小（100GB以下）的index</font>

+ <font style="color:rgb(62, 62, 62);">往往写入压力查询压力相对较低，一般设置3~5个shard，numberofreplicas设置为1即可（也就是一主一从，共两副本）。</font>

<font style="color:rgb(62, 62, 62);">对于数据量较大（100GB以上）的index：</font>

+ <font style="color:rgb(62, 62, 62);">一般把单个shard的数据量控制在（20GB~50GB）</font>
+ <font style="color:rgb(62, 62, 62);">让index压力分摊至多个节点：</font><font style="color:rgb(62, 62, 62);">可</font><font style="color:rgb(62, 62, 62);">通</font><font style="color:rgb(62, 62, 62);">过下面的参数来</font><font style="color:rgb(62, 62, 62);">强制限定一</font><font style="color:rgb(62, 62, 62);">个节点上该index的shard数量，让shard尽量分配到不同节点上。</font><font style="color:rgb(62, 62, 62);">参数：</font><font style="color:rgb(62, 62, 62);">index.routing.</font><font style="color:rgb(62, 62, 62);">allocation.totalshardsper_node</font>

<font style="color:rgb(62, 62, 62);">综合考虑整个index的shard数量，如果shard数量（不包括副本）超过50个，就很可能引发拒绝率上升的问题，此时可考虑把该index拆分为多个独立的index，分摊数据量，同时配合routing使用，降低每个查询需要访问的shard数量。</font>

Tips：

+ 将小的每日索引整合为每周或每月的索引，以减少分片数。
+ 将大型（> 50GB）每日索引分拆分成小时索引或增加主分片的数量。

分片预估方法如下：

1. 总分片数 = 索引个数 X 主分片数 * （副本分片数 +1）X 保留间隔
2. 总数据节点个数 = 向上取整（总分片数 / (20 X 每个节点内存大小））

# <font style="color:rgb(0, 0, 0);">搜索吞吐量预估</font>
搜索用例场景除了考虑搜索容量外，还要考虑如下目标：

+ 搜索响应时间；
+ 搜索吞吐量。

这些目标可能需要更多的内存和计算资源。

第一：问自己几个问题

+ 您期望每秒的峰值搜索量是多少？
+ 您期望平均搜索响应时间是多少毫秒？
+ 您期望的数据节点上几核 CPU，每核有多少个线程？

第二：方法论 与其确定资源将如何影响搜索速度，不如通过在计划的固定硬件上进行测量，可以将搜索速度作为一个常数，

然后确定集群中要处理峰值搜索吞吐量需要多少个核。

最终目标是防止线程池排队的增长速度超过了 CPU 的处理能力。

如果计算资源不足，搜索请求可能会被拒绝掉。

第三：吞吐量预估方法

+ 峰值线程数 = 向上取整（每秒峰值检索请求数 _ 每个请求的平均响应时间（毫秒）/1000）
+ 线程队列大小 = 向上取整（(每个节点的物理 cpu 核数 _ 每核的线程数 * 3 / 2）+ 1)
+ 总数据节点个数 = 向上取整（峰值线程数 / 线程队列大小）

# jvm配置 
## 参数配置建议
+ <font style="color:rgb(62, 62, 62);">xms和xmx设置成一样，避免heap resize时卡顿</font>
+ <font style="color:rgb(62, 62, 62);">xmx不要超过物理内存的50%因为es的lucene写入强依赖于操作系统缓存，需要预留加多的空间给操作系统</font>
+ <font style="color:rgb(62, 62, 62);">最大内存不超过32G，但是也不要太小</font><font style="color:rgb(62, 62, 62);">堆太小会导致频繁的小延迟峰值，并因不断的垃圾收集暂停而降低吞吐量；</font><font style="color:rgb(62, 62, 62);">如果堆太大，应用程序将容易出现来自全堆垃圾回收的罕见长延迟峰值；</font><font style="color:rgb(62, 62, 62);">将堆限制为略小于32GB可以使用jvm的指针压缩技术增强性能；</font>

## <font style="color:rgb(79, 79, 79);">堆内存优化建议</font>
方式一：最好的办法是在系统上完全禁用交。

这可以暂时完成：

```json
sudo swapoff -a
```

要永久禁用它，你可能需要编辑你的/ etc / fstab。

方式二：控制操作系统尝试交换内存的积极性。

如果完全禁用交换不是一种选择，您可以尝试降低swappiness。该值控制操作系统尝试交换内存的积极性。这可以防止在正常情况下交换，但仍然允许操作系统在紧急内存情况下进行交换。

对于大多数Linux系统，这是使用sysctl值配置的：

```json
vm.swappiness = 1
```

1的swappiness优于0，因为在某些内核版本上，swappiness为0可以调用OOM杀手。

方式三：mlockall允许JVM锁定其内存并防止其被操作系统交换。

最后，如果两种方法都不可行，则应启用mlockall。文件。这允许JVM锁定其内存并防止其被操作系统交换。在你的elasticsearch.yml中，设置这个：

```json
bootstrap.mlockall：true
```

## 大内存机器配置建议
如果您的机器具有128 GB的RAM，请运行两个节点，每个节点的容量低于32 GB。这意味着小于64 GB将用于堆，而Lucene将剩余64 GB以上。

如果您选择此选项，请在您的配置中设置cluster.routing.allocation.same_shard.host：true。这将阻止主副本分片共享同一台物理机（因为这会消除副本高可用性的好处）。

<font style="color:rgb(62, 62, 62);">这里是官方的jvm推荐配置链接：</font>

<font style="color:rgb(62, 62, 62);">https://www.elastic.co/cn/blog/a-heap-of-trouble</font>

# 集群节点数
<font style="color:rgb(62, 62, 62);">es的节点提供查询的时候使用较多的内存来存储查询缓存，es的lucene写入到磁盘也会先缓存在内存中，我们开启设计这个es节点时需要根据每个节点的存储数据量来进行判断。这里有一个流行的推荐比例配置：</font><font style="color:rgba(0, 0, 0, 0.9);"></font>

+ <font style="color:rgb(62, 62, 62);">搜索类比例：1:16(内存:节点要存储的数据)</font>
+ <font style="color:rgb(62, 62, 62);">日志类比例：1:48-1:96(内存:节点要存储的数据)</font><font style="color:rgb(51, 51, 51);"></font>

<font style="color:rgb(62, 62, 62);">示例：</font>

<font style="color:rgb(62, 62, 62);">有一个业务的数据量预估实际有1T，我们把副本设置1个，那么es中总数据量为2T。</font>

+ <font style="color:rgb(62, 62, 62);">如果业务偏向于搜索</font><font style="color:rgb(62, 62, 62);">每个节点31*16=496G。</font><font style="color:rgb(62, 62, 62);">在加上其它的预留空间，每个节点有400G的存储空间。</font><font style="color:rgb(62, 62, 62);">2T/400G，则需要5个es存储节点。</font>
+ <font style="color:rgb(62, 62, 62);">如果业务偏向于写入日志型</font><font style="color:rgb(62, 62, 62);">每个节点31*50=1550G，就只需要2个节点即可</font>

<font style="color:rgb(62, 62, 62);">这里31G表示的是jvm设置不超过32g否则不会使用java的指针压缩优化了。</font>

# <font style="color:rgb(0, 0, 0);">冷热集群架构</font>
Elasticsearch 可以使用分片分配感知（shard allocation awareness）在特定硬件上分配分片。

索引密集型业务场景通常使用它在热节点、暖节点和冷（Frozen）节点上存储索引，

然后根据业务需要进行数据迁移（热节点->暖节点->冷节点），以完成数据的删除和存档需要。

这是优化集群性能的最经济方法之一，在容量规划期间，先确定每一类节点的数据规模，然后进行组合。

冷热集群架构推荐：

| 节点类型 | 存储目标 | 建议磁盘类型 | 内存/磁盘比率 |
| --- | --- | --- | --- |
| 热节点 | 搜索优化 | SSD DAS / SAN（> 200Gb / s） | 1:30 |
| 暖节点 | 存储优化 | HDD DAS / SAN（〜100Gb / s） | 1:100 |
| 冷节点 | 归档优化 | 最便宜的 DAS / SAN（<100Gb / s） | 1:1000+ |


<font style="color:rgb(62, 62, 62);"></font>

