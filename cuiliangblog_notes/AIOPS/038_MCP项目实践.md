# MCPé¡¹ç›®å®è·µ
`LangChain`è°ƒç”¨`MCP`æ˜¯å¯ä»¥å°†`MCP`çš„å·¥å…·ç›´æ¥è½¬æ¢ä¸º`LangChain`çš„å·¥å…·ï¼Œç„¶åé€šè¿‡é¢„å®šä¹‰çš„`MCP_Client`å®ç°ä¸å¤–éƒ¨`MCP`çš„è¯»å†™æ“ä½œï¼Œæ¢è€Œè¨€ä¹‹å°±æ˜¯æˆ‘ä»¬éœ€è¦æ”¹å†™åŸå…ˆçš„clientï¼Œå°†åŸå…ˆçš„Function callingè°ƒç”¨é€»è¾‘ä¿®æ”¹ä¸ºLangChainè°ƒç”¨é€»è¾‘

# åˆ›å»º mcp server
```python
import json
import os
import httpx
import dotenv
from mcp.server.fastmcp import FastMCP
from loguru import logger

dotenv.load_dotenv()

# åˆ›å»ºFastMCPå®ä¾‹ï¼Œç”¨äºå¯åŠ¨å¤©æ°”æœåŠ¡å™¨SSEæœåŠ¡
mcp = FastMCP("WeatherServerSSE", host="0.0.0.0", port=8000)



@mcp.tool()
def get_weather(city: str) -> str:
    """
    æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å³æ—¶å¤©æ°”ä¿¡æ¯ã€‚
    å‚æ•° city: åŸå¸‚è‹±æ–‡åï¼Œå¦‚ Beijing
    è¿”å›: OpenWeather API çš„ JSON å­—ç¬¦ä¸²
    """
    url = "https://api.openweathermap.org/data/2.5/weather"
    params = {
        "q": city,
        "appid": os.getenv("OPENWEATHER_API_KEY"),
        "units": "metric",
        "lang": "zh_cn"
    }
    resp = httpx.get(url, params=params, timeout=10)
    data = resp.json()
    logger.info(f"æŸ¥è¯¢ {city} å¤©æ°”ç»“æœï¼š{data}")
    return json.dumps(data, ensure_ascii=False)


if __name__ == "__main__":
    logger.info("å¯åŠ¨ MCP SSE å¤©æ°”æœåŠ¡å™¨ï¼Œç›‘å¬ http://0.0.0.0:8000/sse")
    # è¿è¡ŒMCPå®¢æˆ·ç«¯ï¼Œä½¿ç”¨Server-Sent Events(SSE)ä½œä¸ºä¼ è¾“åè®®
    mcp.run(transport="sse")

```

è¿è¡Œ server

```python
# uv run server.py
2025-08-20 10:27:26.789 | INFO     | __main__:<module>:36 - å¯åŠ¨ MCP SSE å¤©æ°”æœåŠ¡å™¨ï¼Œç›‘å¬ http://0.0.0.0:8000/sse
```

# åˆ›å»º mcpé…ç½®æ–‡ä»¶
mcp.json æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š

```json
{
  "mcpServers": {
    "weather": {
      "url": "http://127.0.0.1:8000/sse",
      "transport": "sse"
    },
    "fetch": {
      "command": "uvx",
      "args": [
        "mcp-server-fetch"
      ],
      "transport": "stdio"
    }
  }
}
```

# Langchain å®¢æˆ·ç«¯
```python
import asyncio
import json
from typing import Any, Dict
from dotenv import load_dotenv
from langchain import hub
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_ollama import ChatOllama
from loguru import logger

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼Œoverride=True è¡¨ç¤ºè¦†ç›–å·²å­˜åœ¨çš„å˜é‡
load_dotenv(override=True)

def load_servers(file_path: str = "mcp.json") -> Dict[str, Any]:
    """
    ä»æŒ‡å®šçš„ JSON æ–‡ä»¶ä¸­åŠ è½½ MCP æœåŠ¡å™¨é…ç½®ã€‚

    å‚æ•°:
        file_path (str): é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º "mcp.json"

    è¿”å›:
        Dict[str, Any]: åŒ…å« MCP æœåŠ¡å™¨é…ç½®çš„å­—å…¸ï¼Œè‹¥æ–‡ä»¶ä¸­æ²¡æœ‰ "mcpServers" é”®åˆ™è¿”å›ç©ºå­—å…¸
    """
    with open(file_path, "r", encoding="utf-8") as file:
        data = json.load(file)
        return data.get("mcpServers", {})

async def run_chat_loop() -> None:
    """
    å¯åŠ¨å¹¶è¿è¡Œä¸€ä¸ªåŸºäº MCP å·¥å…·çš„èŠå¤©ä»£ç†å¾ªç¯ã€‚
    
    è¯¥å‡½æ•°ä¼šï¼š
    1. åŠ è½½ MCP æœåŠ¡å™¨é…ç½®ï¼›
    2. åˆå§‹åŒ– MCP å®¢æˆ·ç«¯å¹¶è·å–å·¥å…·ï¼›
    3. åˆ›å»ºåŸºäº Ollama çš„è¯­è¨€æ¨¡å‹å’Œä»£ç†ï¼›
    4. å¯åŠ¨å‘½ä»¤è¡ŒèŠå¤©å¾ªç¯ï¼›
    5. åœ¨é€€å‡ºæ—¶æ¸…ç†èµ„æºã€‚
    
    è¿”å›:
        None
    """
    # 1ï¸âƒ£ åŠ è½½æœåŠ¡å™¨é…ç½®
    servers_cfg = load_servers()
    
    # 2ï¸âƒ£ åˆå§‹åŒ– MCP å®¢æˆ·ç«¯å¹¶è·å–å·¥å…·
    mcp_client = MultiServerMCPClient(servers_cfg)
    tools = await mcp_client.get_tools()
    logger.info(f"âœ… å·²åŠ è½½ {len(tools)} ä¸ª MCP å·¥å…·ï¼š {[t.name for t in tools]}")
    
    # 3ï¸âƒ£ åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ã€æç¤ºæ¨¡æ¿å’Œä»£ç†æ‰§è¡Œå™¨
    llm = ChatOllama(model="qwen3:8b", reasoning=False)
    prompt = hub.pull("hwchase17/openai-tools-agent")
    agent = create_openai_tools_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

    # 4ï¸âƒ£ CLI èŠå¤©
    logger.info("\nğŸ¤– MCP Agent å·²å¯åŠ¨ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    while True:
        user_input = input("\nä½ : ").strip()
        if user_input.lower() == "quit":
            break
        try:
            result = await agent_executor.ainvoke({"input": user_input})
            print(f"\nAI: {result['output']}")
        except Exception as exc:
            logger.error(f"\nâš ï¸  å‡ºé”™: {exc}")

    # 5ï¸âƒ£ æ¸…ç†
    logger.info("ğŸ§¹ ä¼šè¯å·²ç»“æŸï¼ŒBye!")

if __name__ == "__main__":
    # å¯åŠ¨å¼‚æ­¥äº‹ä»¶å¾ªç¯å¹¶è¿è¡ŒèŠå¤©ä»£ç†
    asyncio.run(run_chat_loop())

```

# è®¿é—®éªŒè¯
```python
2025-08-20 10:42:03.213 | INFO     | __main__:run_chat_loop:21 - âœ… å·²åŠ è½½ 2 ä¸ª MCP å·¥å…·ï¼š ['get_weather', 'fetch']

ä½ : 2025-08-20 10:42:04.410 | INFO     | __main__:run_chat_loop:28 - 
ğŸ¤– MCP Agent å·²å¯åŠ¨ï¼Œè¾“å…¥ 'quit' é€€å‡º
ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·

> Entering new AgentExecutor chain...

Invoking: `get_weather` with `{'city': 'Shanghai'}`

{"coord": {"lon": 121.4581, "lat": 31.2222}, "weather": [{"id": 800, "main": "Clear", "description": "æ™´", "icon": "01d"}], "base": "stations", "main": {"temp": 35.36, "feels_like": 39.24, "temp_min": 35.36, "temp_max": 35.36, "pressure": 1007, "humidity": 44, "sea_level": 1007, "grnd_level": 1006}, "visibility": 10000, "wind": {"speed": 2.82, "deg": 125, "gust": 1.69}, "clouds": {"all": 1}, "dt": 1755657672, "sys": {"country": "CN", "sunrise": 1755638574, "sunset": 1755685962}, "timezone": 28800, "id": 1796236, "name": "Shanghai", "cod": 200}ä¸Šæµ·çš„å¤©æ°”æ™´æœ—ï¼Œå½“å‰æ¸©åº¦ä¸º35.36Â°Cï¼Œä½“æ„Ÿæ¸©åº¦ä¸º39.24Â°Cã€‚æ¹¿åº¦ä¸º44%ï¼Œé£é€Ÿä¸º2.82 m/sï¼Œé£å‘ä¸º125åº¦ã€‚å¤©æ°”æ¡ä»¶è‰¯å¥½ï¼Œé€‚åˆå¤–å‡ºæ´»åŠ¨ã€‚

> Finished chain.

AI: ä¸Šæµ·çš„å¤©æ°”æ™´æœ—ï¼Œå½“å‰æ¸©åº¦ä¸º35.36Â°Cï¼Œä½“æ„Ÿæ¸©åº¦ä¸º39.24Â°Cã€‚æ¹¿åº¦ä¸º44%ï¼Œé£é€Ÿä¸º2.82 m/sï¼Œé£å‘ä¸º125åº¦ã€‚å¤©æ°”æ¡ä»¶è‰¯å¥½ï¼Œé€‚åˆå¤–å‡ºæ´»åŠ¨ã€‚


    
ä½ : https://github.langchain.ac.cn/langgraph/reference/mcp/æ€»ç»“è¿™ç¯‡æ–‡æ¡£

> Entering new AgentExecutor chain...

Invoking: `fetch` with `{'max_length': 10000, 'raw': False, 'url': 'https://github.langchain.ac.cn/langgraph/reference/mcp/'}`

Contents of https://github.langchain.ac.cn/langgraph/reference/mcp/:
MCP é€‚é…å™¨ - LangChain æ¡†æ¶
â€¦â€¦â€¦â€¦
```


