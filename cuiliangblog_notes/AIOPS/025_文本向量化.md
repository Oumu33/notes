# 文本向量化

> 来源: AIOPS
> 创建时间: 2025-08-30T17:45:52+08:00
> 更新时间: 2026-01-11T09:44:52.464391+08:00
> 阅读量: 1120 | 点赞: 1

---

# Embedding 文本向量化
## 什么是Embedding
AI大模型相比于传统的数据检索，最大的区别在于能够“理解”人类的语言。比如你向ChatGPT问“我是谁”和“我叫什么名字”，ChatGPT都能够把他们正确理解成一个相似的问题。那么计算机是如何理解这些相似的话语之间的意思呢？

我们需要清楚，计算机并不能“理解”人类的语言，本质上，他只能进行各种各样的数据计算。所以要让计算机“理解”人类的语言，我们只能将语言拆成Token，再将Token数据化，转成一串串的数字，这样才能让计算机通过计算的方式去理解Token之间的相似性，从而进一步理解语言背后的语义信息。

接下来第一个问题就是，我们要如何用数字来表示语言，同时又要保留语言背后的语意呢？这中间经过了一系列的算法改进。目前比较主流的方法是对语言进行一些语法和词法层面的分析，将一个文本转换成多维的向量，然后通过向量之间的计算，来分析文本与文本之间的相似性。

向量代表一个有位置、有方向的变量。一个二维的向量可以理解为平面坐标轴上的一个坐标点(x,y)，他有x轴和y轴两个维度，在计算机中，就可以用一个二维的数组来表示[x,y]。类似的一个多维的数组就对应一个更多维度的向量。

而文本向量化，就是通过机器学习的方式将一个文本转化成一个多维向量，后续可以通过一些数学公式对多个向量进行计算。这既是为了更好的让计算机理解“语言”，同时也是AI大模型的基础。

## Embedding 模型简介
Embedding 是将文本字符串表示为向量（浮点数列表），通过计算向量之间的距离来衡量文本之间的相关性。向量距离越小，表示文本之间的相关性越高；距离越大，相关性越低。常见的 Embedding 应用包括：

+ 搜索：根据文本查询的相关性对结果进行排序
+ 聚类：根据文本相似性将其分组
+ 推荐：根据相关文本字符串推荐项目
+ 异常检测：识别与其他内容相关性较低的异常点
+ 多样性测量：分析相似性分布
+ 分类：将文本字符串根据其最相似的标签进行分类

## Embedding模型使用
实际上，很多AI大模型产品都提供了文本向量化功能。以国内的阿里云百炼平台为例，就推出了多个不同的向量化模型：

![](https://via.placeholder.com/800x600?text=Image+bbbf4430f23cf77a)

调用Embedding模型计算词向量结果

```python
import dashscope
import json
import os
from http import HTTPStatus
import dotenv

# 读取env配置
dotenv.load_dotenv()
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

# 准备输入文本数据
text = "通用多模态表征模型示例"
input = [{'text': text}]

# 调用多模态embedding模型接口进行向量编码
resp = dashscope.MultiModalEmbedding.call(
    model="multimodal-embedding-v1",
    input=input
)

# 处理模型返回结果，提取关键信息并格式化输出
if resp.status_code == HTTPStatus.OK:
    result = {
        "status_code": resp.status_code,
        "request_id": getattr(resp, "request_id", ""),
        "code": getattr(resp, "code", ""),
        "message": getattr(resp, "message", ""),
        "output": resp.output,
        "usage": resp.usage
    }
    print(json.dumps(result, ensure_ascii=False, indent=4))

```

运行结果如下：

```python
{
    "status_code": 200,
    "request_id": "539e9143-6360-914a-a4c8-a4a5a605618f",
    "code": "",
    "message": "",
    "output": {
        "embeddings": [
            {
                "index": 0,
                "embedding": [
                    -0.020747216418385506,
                    ……
                    -0.01585950329899788
                ],
                "type": "text"
            }
        ]
    },
    "usage": {
        "duration": 0,
        "image_count": 0,
        "input_tokens": 12
    }
}
```

## 通过向量计算语义相似度
把文本转换成向量有什么用呢？最核心的作用是可以通过向量之间的计算，来分析文本与文本之间的相似性。计算的方法有很多种，其中用得最多的是向量余弦相似度。Python语言中提供了一个库sklearn，可以很方便的计算向量之间的余弦相似度。

代码如下：

```python
import dashscope
import json
import os
from http import HTTPStatus
import dotenv
import numpy as np

# 读取env配置
dotenv.load_dotenv()
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

# 准备输入文本数据
texts = [
    '我喜欢吃苹果',
    '苹果是我最喜欢吃的水果',
    '我喜欢用苹果手机'
]

# 获取每个文本的embedding向量
embeddings = []

for text in texts:
    input_data = [{'text': text}]
    resp = dashscope.MultiModalEmbedding.call(
        model="multimodal-embedding-v1",
        input=input_data
    )

    if resp.status_code == HTTPStatus.OK:
        embedding = resp.output['embeddings'][0]['embedding']
        embeddings.append(embedding)

# 计算余弦相似度
def cosine_similarity(vec1, vec2):
    # 计算两个向量的余弦相似度
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    return dot_product / (norm_vec1 * norm_vec2)

# 比较所有文本之间的相似度
print("文本相似度比较结果:")
print("=" * 60)

for i in range(len(texts)):
    for j in range(i+1, len(texts)):
        similarity = cosine_similarity(embeddings[i], embeddings[j])
        print(f"文本{i+1} vs 文本{j+1}:")
        print(f"  文本{i+1}: {texts[i]}")
        print(f"  文本{j+1}: {texts[j]}")
        print(f"  余弦相似度: {similarity:.4f}")
        print("-" * 40)

```

执行结果如下：

```python
文本相似度比较结果:
============================================================
文本1 vs 文本2:
  文本1: 我喜欢吃苹果
  文本2: 苹果是我最喜欢吃的水果
  余弦相似度: 0.9064
----------------------------------------
文本1 vs 文本3:
  文本1: 我喜欢吃苹果
  文本3: 我喜欢用苹果手机
  余弦相似度: 0.7656
----------------------------------------
文本2 vs 文本3:
  文本2: 苹果是我最喜欢吃的水果
  文本3: 我喜欢用苹果手机
  余弦相似度: 0.7421
----------------------------------------
```

从这个案例看到，text1与text2的语义比较相近，他们计算出来的余弦相似度得分也就比较高。test1与text3的语义比较不相似，所以余弦相似度得分比较低。这种语义相近的计算，其实也是AI大模型理解“语言”的基础。

> 要注意，计算语义相似度的算法其实有很多种，比如有余弦相似度、欧氏距离、曼哈顿距离，等等。不同算法得到的分数含义也是不同的，例如，余弦相似度得分在-1到1之间，得分越高，语义越相似。而欧氏距离得分在0到正无穷之间，得分越低，表示语义越相似。
>

# 文档加载与切分
在实现了向量化之后，我们接下来需要编写一个文档加载与切分模块，用于处理不同格式的文档并将其切分为小片段。为什么要进行切分呢？这是为了确保每个文档片段都尽量保持简短且信息集中，以便于后续的向量化和检索。

## 文档加载
我们的目标是支持多种格式的文档，例如 PDF、Markdown、TXT 等，每种文件格式都有不同的读取方式。

## 文档切分
因为大语言模型的上下文窗口是有限的，如果在RAG检索完成之后，直接将检索到的长文档作为上下文传递给模型，可能会超出模型处理的上下文长度，导致信息丢失或回答质量下降，其中，进行文档分割的组件就是文本分割器。

文本分割器的主要作用有：

1. 控制上下文长度：把长文档分割成更小，缩小上下文长度
2. 提高检索准确性：小的文本片段能提升文档检索的精确度
3. 保持语义完整性：在分割过程中，能尽量保持文本的语义连贯性

常用文本分割器类型：

+ 按文本长度
+ 按 token 数量
+ 按特殊标记，如 html 代码、markdown 标题等。

这里我们考虑将文档按Token长度进行切分，设置一个最大的 Token 长度，然后按这个长度进行切分。在这个过程中，我们也会确保每个片段之间有一定的重叠，避免重要信息被切掉。

![](https://via.placeholder.com/800x600?text=Image+a271d3d9eadf7dec)

使用通义千问进行Token长度切分，具体参考文档：[https://huggingface.co/Qwen/Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B)

完整代码如下

```python
from pathlib import Path
from typing import List
from transformers import AutoTokenizer
from PyPDF2 import PdfReader


class DocumentLoader:
    """
    文档加载器类，用于加载不同格式的文档内容。
    支持的格式包括：txt、pdf、md。
    """

    @staticmethod
    def load_txt(file_path: str) -> str:
        """
        加载文本文件内容。

        参数:
            file_path (str): 文本文件的路径。

        返回:
            str: 文件中的文本内容。
        """
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()

    @staticmethod
    def load_pdf(file_path: str) -> str:
        """
        加载 PDF 文件内容。

        参数:
            file_path (str): PDF 文件的路径。

        返回:
            str: 提取的 PDF 文本内容，各页之间以换行符连接。
        """
        reader = PdfReader(file_path)
        text = []
        for page in reader.pages:
            text.append(page.extract_text() or "")
        return "\n".join(text)

    @staticmethod
    def load_md(file_path: str) -> str:
        """
        加载 Markdown 文件内容。当前实现与加载 txt 文件相同。

        参数:
            file_path (str): Markdown 文件的路径。

        返回:
            str: 文件中的文本内容。
        """
        return DocumentLoader.load_txt(file_path)

    @staticmethod
    def load_document(file_path: str) -> str:
        """
        根据文件扩展名自动选择加载方法，加载对应格式的文档内容。

        参数:
            file_path (str): 文档文件的路径。

        返回:
            str: 加载的文档文本内容。

        异常:
            ValueError: 当文件格式不被支持时抛出。
        """
        ext = Path(file_path).suffix.lower()
        if ext == ".txt":
            return DocumentLoader.load_txt(file_path)
        elif ext == ".pdf":
            return DocumentLoader.load_pdf(file_path)
        elif ext == ".md":
            return DocumentLoader.load_md(file_path)
        else:
            raise ValueError(f"不支持的文件格式: {ext}")


class QwenTextSplitter:
    """
    基于指定模型的 tokenizer 对文本进行切分的工具类。

    属性:
        tokenizer: 使用的分词器对象。
    """

    def __init__(self, model_name: str = "Qwen/Qwen2.5-7B"):
        """
        初始化分词器。

        参数:
            model_name (str): 用于加载 tokenizer 的模型名称，默认为 "Qwen/Qwen2.5-7B"。
        """
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

    def count_tokens(self, text: str) -> int:
        """
        计算文本的 token 数量。

        参数:
            text (str): 输入文本。

        返回:
            int: 文本的 token 数量。
        """
        return len(self.tokenizer.encode(text))

    def split_by_tokens(self, text: str, max_tokens: int = 500, overlap: int = 50) -> List[str]:
        """
        将文本按照最大 token 数量进行切分，并允许设置重叠 token 数量。

        参数:
            text (str): 待切分的文本。
            max_tokens (int): 每个片段的最大 token 数量，默认为 500。
            overlap (int): 片段之间的 token 重叠数，默认为 50。

        返回:
            List[str]: 切分后的文本片段列表。
        """
        tokens = self.tokenizer.encode(text)
        chunks = []
        start = 0
        while start < len(tokens):
            end = min(start + max_tokens, len(tokens))
            chunk_tokens = tokens[start:end]
            chunk_text = self.tokenizer.decode(chunk_tokens)
            chunks.append(chunk_text)
            start += max_tokens - overlap
        return chunks


if __name__ == "__main__":
    # 加载示例文档
    file_path = "example.md"
    text = DocumentLoader.load_document(file_path)

    # 初始化文本切分器并统计原始 token 数量
    splitter = QwenTextSplitter(model_name="Qwen/Qwen3-14B")  # 模型名要和 HF 对应
    print("原始 Token 数:", splitter.count_tokens(text))

    # 按 token 切分文本
    chunks = splitter.split_by_tokens(text, max_tokens=300, overlap=50)
    print("按 Token 切分:", len(chunks), "块")
    print("第一块示例:\n", chunks[0][:200], "...")

```

执行结果如下

```python
原始 Token 数: 5306
按 Token 切分: 22 块
第一块示例:
 # 什么是 RAG
RAG，Retrieval-Augmented Generation，也被称作检索增强生成技术，最早在 Facebook AI（Meta AI）在 2020 年发表的论文《Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》（ https://arxiv.org/abs/2005.11401 ）中正式 ...
```


